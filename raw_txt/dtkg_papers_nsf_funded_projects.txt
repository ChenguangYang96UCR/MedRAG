Methods for Dynamic Network Identification with Application to the Control of Smart Buildings
A dynamic network consists of interacting dynamic sub-systems. Such networks occur in many domains: living cells, financial markets, the Internet and the power grid are some examples. Heating, ventilation and air conditioning (HAVC) systems in buildings can also be modeled through dynamic networks since each room's climate depends on that of nearby spaces. Knowledge of such dynamic network models is essential to design and deploy control strategies devoted to the improvement of energy efficiency and occupant comfort. Yet, in practice the structure and dynamics of these networks are either unknown or imprecisely known. For instance, information on the thermal interaction among rooms is difficult to obtain from laws of physics due to the complexity of the physical processes involved. The goal of this project is to formulate algorithms for the identification of dynamic sparse network models from measured data. The research results will support the study of advanced controls for HVAC systems to reduce their energy use and to provide demand-side flexibility to the power grid. Since buildings consume 75% of the nation's electricity, improvement of energy efficiency through smart building control systems will contribute to the sustainability of the nation's energy system.

Although 'dynamic system identification' is a well-developed field, the field of identification of dynamic networks is not at all well-developed. Traditional dynamic system identification techniques cannot exploit the inherent sparseness of the network identification problem, while traditional machine learning techniques are mostly applicable to only static networks. In this project we combine ideas from traditional dynamic system identification, L1 optimization for sparse vector recovery (from compressed sensing), and graphical modeling from machine learning to address the challenges in dynamic network identification. If successful, the research will (1) provide fundamental contribution to the nascent field of dynamic network identification through new algorithms, and (2) enable speedy deployment of 'smart building' technologies in commercial buildings. In addition, the project will support a number of educational innovations for attracting students from under-represented groups to engineering and generating excitement about engineering.

EAGER/Cybermanufacturing: Architecture and Protocols for Scalable Cyber-Physical Manufacturing Systems
Cyber-physical manufacturing systems have potential to transform manufacturing industry in a significant fashion by sharing manufacturing resources, operating manufacturing machines, and connecting customers with manufacturing resources over the Internet. It integrates networking, embedded computing, control, manufacturing, service and cloud computing technologies to increase manufacturing efficiency and resource utilization. Despite its promising future, there are many significant and challenging issues in realizing the full potential of and wide adoption cyber-physical manufacturing in industry. One of them is the scalability of cyber-physical manufacturing systems. Cyber-physical manufacturing systems must be highly scalable to integrate the capabilities and services of a large number of manufacturing machines and other resources, operate and control them, and make them available to customers worldwide over the Internet. Another issue is network protocols for communicating manufacturing services and controlling manufacturing machines in factory floors over the Internet. In this EArly-concept Grant for Exploratory Research (EAGER) project, new architectures and protocols for scalable cyber-physical manufacturing systems will be explored. The expected innovations will improve connections between manufacturers and consumers, ease the exchange of manufacturing services across organizational boundaries, and increase utilization of manufacturing resources and reduce their idle time. The research will be integrated into the instruction of several classes on computer science and advanced manufacturing in two institutions. The cyber-enabled manufacturing research will expand the reach of manufacturing facilities and tools and help broaden participation of underrepresented groups in research.

Cyber-physical manufacturing systems are still in an early phase of their research and development. Several exploratory research tasks on the architecture and network protocols of scalable cyber-physical manufacturing systems will be carried out in this project. First, a service oriented architecture of scalable cyber-physical manufacturing systems, which links services of a large number of manufacturing machines from a variety of manufacturing service providers with customers over the Internet, will be investigated. Second, an Internet-scale manufacturing service protocol, which allows communication of manufacturing services across organizational boundaries over the Internet and enables plug-and-play of manufacturing machines in cyber-physical manufacturing systems, will be investigated. Third, a real-time Ethernet protocol, which enables real-time communication for monitoring and controlling manufacturing operations over the Internet, will be created. Fourth, a method of virtualization of manufacturing machines, which makes it easy to incorporate manufacturing resources in cyberspace for cyber-physical manufacturing systems, will be researched. Finally, a test bed will be developed for evaluation of the above techniques and methods.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The major goal of this project is to carry out exploratory research on architecture and network protocols of scalable cyber-physical manufacturing. In this project, we developed a service oriented architecture of scalable cyber-physical manufacturing clouds, an Internet-scale machining tool communication method, called MTComm,  a real-time Ethernet protocol RTEthernet,   and a testbed of cyber physical manufacturig cloud.
 -  MTComm has a significant improvement over the MTConnect, a widely used communication method of machine tools which allows only monitoring them over the Internet. It allows both operation of machining tools and monitoring them over the Internet. It is implemented and tested to analyze its performance in multiple manufacturing application scenarios. Several applications, including collaborative manufacturing, digital twin and fault diagnosis through active on-line testing, were developed to demonstrate its capabilities. The experiments show its excellent feasibility and performance of monitoring and performing remote manufacturing operations over the Internet.
- A comparison of the newly developed RTEthernet with the EtherCAT protocol in real-time performance has been conducted. The simulation results shows that the RTEthernet protocol has better performance than the EtherCAT in terms of data transmission latency.
-  The testbed of cyber physical manufacturing cloud, developed based on the methods and techniques developed in this project, allows monitoring and direct operations of machining tools from two sites over the Internet.

EAGER/Cybermanufacturing Systems: Fleet-Sourced Cyber Manufacturing Applications for Improved Transparency and Resilience of Manufacturing Assets and Systems
Internet-enabled services (such as cloud-based and mobile applications) have been influential through almost all economic sectors, such as retail, music, transportation, and healthcare, which have proven the benefit of performing analytics on historical data from a networked system. However, compared to existing Internet-enabled industries, manufacturing assets are less connected and less accessible in real-time. As a result, current manufacturing enterprises make decisions following a top-down approach: from overall equipment effectiveness to assignment of production requirements, without considering the condition of machines. This EArly-concept Grant for Exploratory Research (EAGER) award supports fundamental research to develop the concepts and theory for next-generation advanced cybermanufacturing systems that are networked and interoperable through analytics on the fleet-sourced data. Cybermanufacturing systems will enable a bottom-up real-time decision support manufacturing strategy by taking into account asset health conditions predicted based on historical asset data. Eventually, mobile applications will be developed for portable access of the actionable information. Since such analytics can be performed on data collected from existing asset condition monitoring systems with moderate levels of add-on sensor installment, manufacturing industries in almost every sector will benefit from the results of this research. Consequently, this research will inject speed into the development of U.S. economy and benefit the society by increasing the efficiency and productivity of manufacturing enterprises. This research requires knowledge and expertise from a variety of disciplines including manufacturing, mechanical engineering, computer science, and control theory. The interdisciplinary methodology will facilitate the creativity and healthy growth of the involved areas and draw interest from younger generation to impact science, technology, engineering and mathematics education.

The new cybermanufacturing methodology will deepen the research on fleet-sourced prognostics, which overcomes several drawbacks of conventional prognostics and health management approaches, including lack of generality and reconfigurability, lack of robustness against changing regimes, and sometimes insufficient accuracy. A fleet is referred to as a group of assets similar in working conditions (make and model, ambient conditions, and health status). Research gaps in conducting fleet-sourced prognostics exist in the quantification of asset similarity, clustering fleets, validation of such fleets, and dynamically changing the clustering scheme when regimes change. The research team will leverage existing fleet-level peer-to-peer prognostics approaches to develop a reconfigurable platform with capabilities to reduce the dimensionality from fleet-sourced data, devise a risk assessment methodology to provide real-time predictive actionable information, and eventually incorporate such functions into the developed mobile applications.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Goals and Challenges
The major goal of this project is to deepen the science of cyber manufacturing by sourcing information from a fleet of engineering assets with the purpose of determining the remaining functional life of these assets. Such an approach can result in more reconfigurable, accurate and flexible solutions that can understand and react to diverse operating regimes.This approach requires understanding and overcoming a number of gaps in existing methods for fleet-based predictive analysis, including determining the level of similarity among assets, accurate clustering and validation of the method for determining what should be considered a fleet.
Approach
The research team leverages existing fleet-level peer-to-peer prognostics methods to develop a reconfigurable platform to reduce the dimensionality (or the number of influential variables) from fleet-sourced data, devise a risk assessment methodology to provide real-time, predictive and actionable information. This actionable information should then be displayed in an easy-to-understand method that enables constant monitoring, such as through an application running on a mobile device. 
Results
This research project considered two specific areas to test and validate the proposed concept and focus efforts to overcome the various gaps in fleet-based predictive analytics for engineering assets. These areas were health monitoring for machine tool critical components and fluid flow control valve health modeling.
Ball Screw Cyber Health Twin: A cloud-based, fleet-ready mobile application for ball screw (a machine tool critical component) health monitoring was developed. This application has the ability to deliver the health status of individual ball screw components from a fleet of machines. The cloud-based architecture enables the data and health states of similar ball screws to be compared and to be considered in delivering the remaining useful life of a particular ball screw. At the same time, this creates a digital twin to which other similar ball screws can be compared. These predictive results can then be used to make maintenance decisions that can prevent unplanned downtime, saving manufacturers money from lost production. A user-friendly interface was designed to deliver instant access to actionable ball screw health information to support these maintenance decisions.
The results of this research were demonstrated at the 2016 International Manufacturing Technology Show in Chicago. The success in the IMTS 2016 validated the feasibility of the developed cyber physical ball screw system.
Fluid Control Valve Global Modeling: There exists many different types and models of fluid control valves being utilized in a wide range of industries, from oil and gas to water treatment and delivery. In addition, even valves of the same type and model, operating in the same application experience a wide variation of operating conditions. This variability makes the development of an accurate, robust health model a difficult task using traditional modeling and analysis methods. To fill this gap, a normalization-based global modeling approach was proposed, developed and has been validated for fluid control valves under this project. A test group of six control valves of three types were utilized in this development and validation efforts. The developed technique is a pre-processing method that minimizes the variation between different types of valves. This laid the foundation of developing more sophisticated clustering method to further unify individual models for more scalable predictive health monitoring solutions.

BIGDATA: Collaborative Research: F: From Data Geometries to Information Networks
Big Data often results from multiple sources, giving collections that contain multiple, often partial, "views" of the same object, space, or phenomenon from various observers. Extracting information robustly from such data sets calls for a joint analysis of a large collection of data sets. The project is developing a novel geometric framework for modeling, structure detection, and information extraction from a collection of large related data sets, with an emphasis on the relationships between data. While this approach clearly applies to data with a clear geometric character (e.g., objects in images), the work is also applied to datasets as diverse as computer networks (identifying common structure in subnets) and Massive Open Online Course homework data (automatically carrying grader annotations to similar problems in other students' homeworks).

The novel framework is based on the construction of maps between the objects under considerations (point clouds, graphs, images, etc...), and on the analysis of the networks of maps that result as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of map processing between data sets and require tool sets with new ideas from functional analysis, non-convex optimization, and homological algebra in mathematics, and geometric algorithms, machine learning, optimization, and approximation algorithms in computer science. Sophisticated algorithmic techniques for attacking the large-scale non-linear optimization problems that emerge within the framework will also be investigated.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The overall goal of this project has been to develop a novel geometric framework for model building, structure detection, and information extraction from a large collection of related data sets -- with a focus on the relationships between data, or between the variables describing the data. We have pursued an approach based on the construction of maps between the objects under considerations (point clouds, graphs, images, etc.), and on the analysis of the networks of maps that result, as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. The effort resulted in progress on under-determined systems and exact/approximate recovery conditions for spaces and maps, a new understanding regarding the geometry of convex and non-convex optimization problems related to matching geometric and information spaces, as well as novel algorithms for organizing geometric data and for the recovery of sparse structures based not only on distances/similarities but also on map consistency criteria.
To illustrate, one specific area where we have made major progress is joint data alignment or matching via map synchronization.  For example, imagine that we have to assemble information about an object from multiple images taken of that object, as in 3D reconstruction or cryoelectron microscopy.  To do so we need to jointly recover the camera orientations associated with each image. Typically, one obtains a noisy estimate of relative transformation between a pair of images using raw features. Transformation synchronization is then the task of making all these pairwise estimates globally consistent. This is a challenging optimization problem on which we have obtained both a deeper theoretical understanding as well as state-of-the-art practical algorithms. Our analysis is based on the notion of enforcing cycle consistency in the map network. We select cycles to optimize via a stability score based on a condition number of the Hessian matrix of the induced optimization problem, combining semidefinite programming and importance sampling.
We have also explored further applications of these ideas to problems in medical and biological data analysis. For example, using diffusion imaging data and tractography algorithms, we have analyzed the 3D shape variability of white matter fiber bundles in the brain aiming to differentiate pathologies from healthy forms across individuals, as well as assess disease progression within one individual. As another example, we have looked at the identification of a subset of relevant explanatory variables in joint data analysis, using a knockoff framework. In such a framework one constructs fake variables, knockoffs, which can then be used as controls for the true variables.  We have obtained a Metropolis-Hastings formulation of an exact knockoff sampler and used it to develop a novel Digital Twin Test that allows us to establish causality from trio data in genetic studies.
We expect that the techniques developed under this project can have many other applications in scientific, engineering, and medical data analysis.
CPS: Synergy: CNC Process Plan Simulation, Automation and Optimization
Machining is a fundamental manufacturing capability critical to the production of end-user goods and systems, as well as the tooling and equipment used in virtually every industrial process. Machine tool programming to support these processes is critical for both production and cost estimation. However, currently available automated process planning methods constrain the tool to follow geometrically simple paths to minimize computational requirements, limit the tool velocity and/or tool orientation during a simple path to constant values, also to save computational burden, and/or process one geometrical feature at a time without attempting to optimize the entire process. This award supports fundamental research to provide knowledge needed for development of a novel computer-aided process planning and control architecture for integrated complex tool path generation and optimization. The resulting new mathematical algorithms will drive effective real-time control and optimization of manufacturing processes and enable substantial increases in machine productivity. These capabilities will have potential for broad-ranging impact on the domestic economy, which uses machining in the vast majority of products, due to its flexibility, speed, cost, and accuracy advantages relative to other processes. The research integrates several complementary technical domains, including advanced manufacturing, geometric computing, and high performance parallel computing.

Conventional computer-aided manufacturing approaches for toolpath optimization are inherently post-hoc methods that are not well integrated for supporting simultaneous generation and optimization of the process plan. This research will investigate generic optimization and control architectures for automated toolpath optimization, which require global solutions to highly non-linear, constrained optimization problems in high-dimensional search spaces of tool motions with time-varying positions and orientations. To address this challenge, the research approach will utilize a bi-directional optimization scheme that consists of: (l) a top-down, multi-level decomposition of the problem into fundamental model motions (e.g., curved blocks, peel layers, tool swipes) and (2) a bottom-up optimization of these fundamental geometric model motions. The latter schema element will fully support the option of selecting cutting tools to maximize material removal rate and/or surface ﬁnish and will build upon theoretical formulations and efﬁcient computing implementations of volume preserving offsetting, steady motion interpolation, and ball morphing surface interpolation. The resulting fundamental motion models and associated fast, parallel computing algorithms will provide for rapid analysis of swept regions, collision avoidance and computing material removal rates that are critical for facilitating rapid toolpath optimization.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Most manufacturing processes involve a moving tool that cuts, deposits, or transforms materials. A CNC planner is a computer program that takes, as input, (1) a geometric model of the part to be produced, (2) the geometric and physical characterization and limitations of the manufacturing system (3) designer?s guidelines, and possibly (4) the model of the initial shape from which the part will be produced. The planner produces a sequence of commands (often G-codes) which are sent to the manufacturing system such as a 3D printer or a cutting machine. The project sought to reduce key factors of the total production cost pertaining to the time required to execute the resulting plan and the time required by the human to create the plan. Although the latter is amortized and clearly less important for mass production, its importance is growing with the deployment of cybermanufacturing applications focused on customizable parts and exploiting an ecosystem of diversified, cloud-accessible manufacturing resources.
This project established scientific methods for optimizing tool motion paths defined by a sequence of tool positions and orientations choreographed to a scheduled time for each pose in the sequence so as to minimize manufacturing time while preserving all other requirements of accuracy, correctness and safety. Toward this goal, the project: (1) predicted physics involved in a tool trajectory using high performance computing and voxel modeling, (2) applied constrained optimization techniques to minimize time to execute the trajectory while adhering to constraints, such as avoiding collisions with the final part, respecting the maximum rate of material removal, and not exceeding loads, velocity, acceleration, jerk, and snap limits, (3) applied techniques from the body of knowledge in computer animation to provide effective control over the interactive application of this optimization, (4) accommodated the limitations of the current generation of legacy CNC controllers and leverage new open source, open architecture solutions to define the next generation of controllers, and (5) collected and transmitted sensor information on the CNC systems with values that correlate to the actual physics encountered. Further, this project entailed a physical realization of the cyberphysical system. The scientific methods and new open architecture controller were implemented on two physical testbeds, one bench top scale and the other a full-size machine tool. The outcomes of these tests have facilitated tight integration of computer-aided manufacturing software planners with manufacturing execution platforms, with attendant advantages associated with manufacturing process time improvement.
These outcomes overcome disadvantages of conventional automatic process planners in that such planners are considerably sub-optimal because they suffer from one or more of the following shortcomings: (1) they constrain the tool path to follow simple paths that may be easily computed, (2) they restrict the tool velocity or orientation, or (3) they process one feature at a time without attempting to optimize the entire process.. The approach developed in this project involved bi-directional optimization: a top-down multi-level decomposition of the problem (into curved blocks, peel layers, and tool swipes) and a bottom-up optimization of the swipes, layers, and blocs. The team included both leading experts in manufacturing and geometric computing, as well as industrial firms interested in next-generation machine tools with close coordination between computer-aided manufacturing and process execution. The outcomes of this effort establish the basis for next-generation intelligent machine controllers.
CPS: TTP Option: Frontiers: Collaborative Research: Software Defined Control for Smart Manufacturing Systems
Software-Defined Control (SDC) is a revolutionary methodology for controlling manufacturing systems that uses a global view of the entire manufacturing system, including all of the physical components (machines, robots, and parts to be processed) as well as the cyber components (logic controllers, RFID readers, and networks). As manufacturing systems become more complex and more connected, they become more susceptible to small faults that could cascade into major failures or even cyber-attacks that enter the plant, such as, through the internet. In this project, models of both the cyber and physical components will be used to predict the expected behavior of the manufacturing system. Since the components of the manufacturing system are tightly coupled in both time and space, such a temporal-physical coupling, together with high-fidelity models of the system, allows any fault or attack that changes the behavior of the system to be detected and classified. Once detected and identified, the system will compute new routes for the physical parts through the plant, thus avoiding the affected locations. These new routes will be directly downloaded to the low-level controllers that communicate with the machines and robots, and will keep production operating (albeit at a reduced level), even in the face of an otherwise catastrophic fault. These algorithms will be inspired by the successful approach of Software-Defined Networking. Anomaly detection methods will be developed that can ascertain the difference between the expected (modeled) behavior of the system and the observed behavior (from sensors). Anomalies will be detected both at short time-scales, using high-fidelity models, and longer time-scales, using machine learning and statistical-based methods. The detection and classification of anomalies, whether they be random faults or cyber-attacks, will represent a significant contribution, and enable the re-programming of the control systems (through re-routing the parts) to continue production.

The manufacturing industry represents a significant fraction of the US GDP, and each manufacturing plant represents a large capital investment. The ability to keep these plants running in the face of inevitable faults and even malicious attacks can improve productivity -- keeping costs low for both manufacturers and consumers. Importantly, these same algorithms can be used to redefine the production routes (and machine programs) when a new part is introduced, or the desired production volume is changed, to maximize profitability for the manufacturing operation .
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
 
This project's aim was to reimagine the future and operations of manufacturing systems and their design, runtime monitoring and anomaly detection. As such, we envisioned the development of "software-defined control" (SDC), inspired by ideas in software-defined networking, that would provide a global view of the system to enable better monitoring and control of the manufacturing system. Coupled with ahead-of-time modeling and anomaly detection features, SDC can significantly improve the design, resiliency and security of manufacturing systems of the future. 
The high-level products of this project are:
1. SDCWorks: an open-source modeling and simulation framework for SDC systems. SDCWorks provides the semantics of such a manufacturing system in terms of a discrete transition system which sets up the platform for future research in a new class of problems in formal verification, synthesis, and monitoring. It can be used to evaluate relevant metrics such as throughput, latency and load, ahead of time, for manufacturing systems. This enables engineers to explore the implications of their design choices early on in the development process.
2. VetPLC: a temporal context-aware, program analysis- based approach to produce timed event sequences that can be used for automatic safety vetting.of manufacturing systems (both current as well as future ones). VETPLC outperforms state-of-the-art techniques and can generate event sequences that can be used to automatically detect hidden safety violations, thus enabling safer and more secure manufacturing systems.
3. Digital Twin (DT) Architectures: one of the important components of the SDC architecture and its proposed monitoring capabilities is the development of ?digital twins? ? essentially DTs are real-time digital images of physical systems, processes or products that help evaluate and improve business performance. DTs are developed in order to (a) accurately model the system and (b) monitor the system at runtime to detect deviations from the expected behavior. Since digital twins can model/capture different physical and cyber aspects of the system, there is a need for a framework to manage all the DTs. We developed a unified DT framework  to provide a real-time extensible global view of a manufacturing system by deploying multiple DTs at multiple levels of the automation pyramid of the International Society of Automation ISA?95. The DT framework is used within the Software-Defined Control, where it operates with a set of applications and a decision maker to monitor, control, predict, and re-configure (as necessary) complex production processes.
4. The CyPhyHouse and the Koord language were applied to the SDC framework to demonstrate that virtual commissioning can be facilitated using high-level domain specific languages (DSL) tailored for smart manufacturing.

CPS/Synergy/Collaborative Research: Safe and Efficient Cyber-Physical Operation System for Construction Equipment
Equipment operation represents one of the most dangerous tasks on a construction sites and accidents related to such operation often result in death and property damage on the construction site and the surrounding area. Such accidents can also cause considerable delays and disruption, and negatively impact the efficiency of operations. This award will conduct research to improve the safety and efficiency of cranes by integrating advances in robotics, computer vision, and construction management. It will create tools for quick and easy planning of crane operations and incorporate them into a safe and efficient system that can monitor a crane's environment and provide control feedback to the crane and the operator. Resulting gains in safety and efficiency wil reduce fatal and non-fatal crane accidents. Partnerships with industry will also ensure that these advances have a positive impact on construction practice, and can be extended broadly to smart infrastructure, intelligent manufacturing, surveillance, traffic monitoring, and other application areas. The research will involve undergraduates and includes outreach to K-12 students.

The work is driven by the hypothesis that the monitoring and control of cranes can be performed autonomously using robotics and computer vision algorithms, and that detailed and continuous monitoring and control feedback can lead to improved planning and simulation of equipment operations. It will particularly focus on developing methods for (a) planning construction operations while accounting for safety hazards through simulation; (b) estimating and providing analytics on the state of the equipment; (c) monitoring equipment surrounding the crane operating environment, including detection of safety hazards, and proximity analysis to dynamic resources including materials, equipment, and workers; (d) controlling crane stability in real-time; and (e) providing feedback to the user and equipment operators in a "transparent cockpit" using visual and haptic cues. It will address the underlying research challenges by improving the efficiency and reliability of planning through failure effects analysis and creating methods for contact state estimation and equilibrium analysis; improving monitoring through model-driven and real-time 3D reconstruction techniques, context-driven object recognition, and forecasting motion trajectories of objects; enhancing reliability of control through dynamic crane models, measures of instability, and algorithms for finding optimal controls; and, finally, improving efficiency of feedback loops through methods for providing visual and haptic cues.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The objective of this project was to improve the safety and efficiency of construction equipment operations through advances in robotics, computer vision, and construction management. Equipment operation is one of the most dangerous tasks on a construction site. Accidents related to equipment operation often result in death and property damage on the site and the surrounding area, as well as cause delay and disruption that negatively impact operational efficiency. This project focused on safety in construction operations involving equipment and workers by improving the frequency, detail, and applicability of safety planning, monitoring, and control, with four key components: 1) Improving safety via better planning by creating virtual models of the physical environment on construction sites and simulating the location and working condition of equipment and workers to make sure their operations are safe and efficient; 2) estimating and analyzing the state of equipment and workers as well as their surroundings through sensory data; 3) monitoring the work environment via detecting and tracking current and forecasted location of workers, equipment, materials, and other site objects (e.g., power lines) by analyzing site video feeds; and 4) offering control feedback to workers and equipment operators so that they are informed of potential incidents. The accomplished work contributes to science by addressing fundamental underlying research challenges in robotics, computer vision, and construction improvement: improving the efficiency and reliability of planning through new Failure Model Effects Analysis and Fault Tree Analysis; creating new methods for joint recognition and pose estimation, contact state estimation, and equilibrium analysis; improving monitoring through model-driven and real-time 3D reconstruction techniques, context-driven object recognition, forecasting motion trajectories of objects by leveraging appearance and geometry; enhancing efficiency and reliability of control through new dynamic crane models, novel measures of instability, and new algorithms for finding optimal controls; and finally new methods for providing visual and haptic cues to improve efficiency of feedback loops. Experimentation on several construction sites also provided unique insights into the practical considerations in the effective deployment of emerging technologies, such as cyber-physical systems (CPS), in industry practice.
 
The work also provided a unique opportunity for interdisciplinary research and teaching. Methods from construction management, computer vision, and robotics are being applied to problems of significant relevance to the construction industry. Graduate students were involved in mentoring undergraduate students. Undergraduate students and particularly those from underrepresented groups were involved in research activities. Research findings are disseminated to both academic researchers and industry practitioners. Material from this research is being introduced in classes that the investigators are teaching/contributing to. Beyond construction, the findings from this work have applications to traffic monitoring, smart infrastructure, intelligent manufacturing, and other areas that require continuous monitoring of operations, safety analysis, and hazard mitigation.
MRI - Development of Continuum: A Virtualized Attentive Environment for Amplified Collaboration
This project, developing The Continuum, a cyberinfrastructure instrument that unifies high-resolution computer-enhanced group collaboration workspaces with arrays of ambient sensors, aims to enable users to collaborate with local and remote colleagues and their data more effectively utilizing a War room to benefit the collaboration. The room itself is given the ability to anticipate needs, improve interaction, and focus the time on discovery rather than the technology. Continuum serves as a digital assistant providing a foundation for new computer science research, benefits global scientific collaboratories. War rooms benefit collaboration because they support continuous interaction and awareness of team members, enable persistent information sharing, and encourage mobility among team members and information. A computer enhanced war room, such as The Continuum, which 1. Connects distributed teams with their data (whether stored in the cloud or streamed from major computational resources over high-speed networks), 2. Supports persistent digital artifacts, 3. Enables ultra-high-resolution 2D and 3D stereoscopic digital media to be simultaneously viewed, and 4. Supports ubiquitous and intuitive interaction, creates a powerful and easy-to-use information-rich environment in support of scientific discovery. Just as NSF cyberinfrastructure produces greater volumes and varieties of data from data storage systems, online instrumentation and major computational resources like XSEDE and Blue Waters, advanced visualization instruments serve as the 'lenses' of these data generators (similar to the telescopes and microscopes of yore), enabling researchers to peer into cyberspace to manage, access and analyze big data.

Continuum is the culmination of a quarter century of experience at the institution developing interactive instruments, from the original CAVE in 1992, to the ultra-high-resolution LambdaVision tiled LCD wall in 2004, and the hybrid-reality CAVE2 in 2012. Each new generation of visualization instrumentation provided scientific communities with more advanced features. Continuum development focuses on three areas: 1. Transforming passive display walls that respond to user commands into attentive display walls that monitor the users in a room and anticipate their needs to improve their experience; 2. Providing an alternative approach to constructing high-resolution visualization display walls using thin-border 4K passive stereo Organic Light Emitting Diode (OLED) displays that have impressive brightness, contrast, uniformity, off-axis viewing, and low reflectivity in brightly lit rooms; and 4. Utilizing cloud computing to prototype future Continuum spaces that do not require local high-end compute and display resources. Twelve funded research projects from local institutions, in Astronomy, Astrophysics, Biology, Combustion, Geophysics, Neuroscience, Pathology, Physics, Psychiatry, and High Performance Computing, are poised to use Continuum for their data visualization needs. The Continuum will open up new opportunities in computer science research at the intersection of large-scale data visualization, human-computer interaction, and high-speed networking. Continuum will also directly support 10 classes taught in the UIC Computer Science, Art and Design, and Biomedical Science departments.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Continuum is an instrument that unifies high-resolution computer-enhanced group collaboration workspaces with arrays of ambient sensors, enabling users to collaborate with local and remote colleagues and their data more effectively by giving the room itself the ability to anticipate their needs, improve their interactions, and focus their time on discovery rather than on technology. Continuum is a digital assistant, benefiting global scientific collaboratories and providing a foundation for new computer science research.
Traditional war rooms benefit collaboration because they support continuous interaction and awareness of team members, enable persistent information sharing, and encourage mobility among team members and information. A computer-enhanced war room, such as Continuum, is designed to do exactly this. It benefits users by: (1) connecting distributed teams with their data, whether stored in the cloud or streamed from major computational resources over high-speed networks; (2) supporting persistent digital artifacts; (3) enabling simultaneous viewing of ultra-high-resolution 2D and 3D stereoscopic digital media; and (4) supporting ubiquitous and intuitive interaction, thereby creating a powerful and easy-to-use information-rich environment in support of scientific discovery.
Intellectual Merit. Research projects from local institutions, in Astronomy, Combustion, Geophysics, Neuroscience, Pharmacy, Physics, and High-Performance Computing, are poised to use the Continuum instrument for their data visualization needs. Also, Continuum opens up new opportunities in computer science research at the intersection of large-scale data visualization, human-computer interaction, and high-speed networking.
Broader Impact. This MRI provides the University of Illinois at Chicago, a Minority Serving Institution, a Hispanic Serving Institution and an Asian American and Native American Pacific Islander Serving Institution, with state-of-the-art equipment, opportunities, and faculty oversight to enhance research and research training meeting rooms and classrooms; to provide scientific communities with a highly integrated collaboration environment; to work with industry to integrate their products in new and novel ways; and, to continue ongoing partnerships between UIC and many of the world's best domain scientists and computer scientists in academia and industry, who readily become early adopters of new instrumentation and who provide UIC students with summer internships and jobs upon graduation. This enables the U.S. to maintain its leadership position in high-performance cyberinfrastructure instrumentation and to contribute to solutions of complex global issues, such as the environment, health, homeland security, and the economy, which, in turn, benefits society as a whole.

I-Corps: Bringing Digital Twin Technology to the Asset Management Community
The broader impact/commercial potential of this I-Corps project is based on the economic and safety benefits that arise from the improved inspection and maintenance of infrastructure assets. Assets in this context can refer to transportation systems such as bridges and roadways, energy sector systems such as offshore oil platforms and pipelines, or telecommunications towers, all of which must be routinely inspected for safety and functionality. The 'asset intelligence' platform will potentially benefit not only asset owner/operators, but infrastructure stakeholders and the general public by providing a more comprehensive record and understanding of infrastructure performance, improving strategic planning and maintenance processes, and optimizing total impact and cost of ownership. Beyond the economic benefits that improved decision-support software will provide to customers, improvements to inspection processes will also benefit the safety of the general public.

This I-Corps project will explore new solutions to the challenges of infrastructure inspection and management. During an inspection, a range of information is collected about an asset, including inspector observations, images, sensor readings, and nondestructive testing results. Challenges persist in the efficient and effective exploitation of this data, limiting its value and frequently resulting in overly conservative and economically sub-optimal decisions about an asset. This projects envisions a new process for integrating and visualizing inspection information by creating a 'digital twin' of an asset. Using a novel 3D modeling process, a virtual reality model of an asset can be generated. A broad range of inspection and environmental data is then overlaid on this model, serving as the data-of-record over the lifetime of the asset, and providing data continuity in support of real-time, synoptic, and forensic analyses. These digital twins can be accessed through conventional computers and subsequently through virtual and augmented reality platforms to improve field inspection processes.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The primary goal of this project was to explore commercialization of technology developed under a separate NSF research award, and was directed towards improving the monitoring and maintenance of civil infrastructure systems. The other goal was to provide entrepreneurial training to a doctoral student (EL) and the faculty advisor (PI). Major activities included completion of the NSF I-Corps cohort training program, customer discovery and business model validation, and fundraising through the Virginia Commonwealth Research Commercialization Fund.
Intellectual merit: this project supported efforts to identify the challenges facing asset owners and managers, and their needs with respect to data management and visualization. Knowledge gained through this program helped to identify critical outstanding research needs that must be addressed in order to support these communities, thereby impacting both commercialization and long-term research efforts.
Broader impacts: This project provided entrepreneurship and startup training for project participants. The findings of this work will also lead to new technologies that support improved urban planning and management, with direct economic and safety benefits for the general public.
 
Upon completion of the I-Corps cohort training program, customer discovery efforts were initially directed at US DoD facilities managers and engineers, primarily focusing on US Army Corps as an initial customer. The initial value proposition and business model formed through I-Corps continued to be validated and confirmed after completion of the cohort training program. However, additional discovery uncovered a key problem: the product offering most likely to gain market traction would require major investment. Based on research and conversations with investors in the Washington DC area, significant cash flow would be mandatory prior to successful investment. The team could not identify an initial product offering that could generate customers and cash flow in the DoD segment, particularly considering the extended sales cycle for government clients.
This led to a pivot. The team was approached by a security contracting company interested in partnering to provide surveillance and monitoring of oil and gas pipelines in high-risk international regions, in an effort to mitigate pipeline theft. While in contractual talks to partner, the team continued to pursue customer discovery in this segment (international pipeline operations and security managers). Team members attended a variety of expos and tradeshows (mostly geared towards oil and gas operations) to perform discovery. They also spoke with a variety of vendors, contractors, and potential customers in the pipeline monitoring segment. Discovery was also pursued through a commonwealth of Virginia market research program. The conclusion was that pivoting into pipeline security monitoring led to a poor value proposition relative to existing solutions, and was not a suitable fit for the core technology.
Through continued discovery, the EL and PI identified several potential initial product offerings in the infrastructure asset management market segment that could potentially generate cash flow and lead to investment. These analytical products would be directed towards unmanned aerial vehicle (UAV) operators who need to add value to their infrastructure inspection businesses. The team developed a series of prototypes and mockups of potential offerings and presented them to several potential partners. The feedback was mixed. While the UAV operators were interested in the analytical products, they did not feel that these products would expand their existing customer base. The EL and PI deemed this to be a negative response to business model validation and decided to cease discovery efforts.
Additionally, the team applied for the Virginia Commonwealth Research Commercialization Fund in January 2017, which resulted in a $50,000 award in June 2017 to pursue the original value proposition and business model formed during I-Corps. Unfortunately, the original partnership, which included the team mentor, dissolved prior to the announcement of the award. The award was therefore returned to the sponsor.
Recently, the EL for the team was admitted to the Cornell Tech post-doctoral “Runway” program. This program provides training and financial support for recent graduates interested in commercializing technology. The EL was accepted the program as a direct result of his I-Corps experience, and he plans to use it as an opportunity to explore other potential initial product offerings. The PI will serve as an advisor for these efforts moving forward.

I-Corps: A Platform for Matching Manufacturing Service Companies with Design Enterprises
The broader impact/commercial potential of this I-Corps project is to efficiently match, using intelligent multi-agent software, original equipment designers to manufacturers who have the necessary capabilities to fulfill those needed services. On one side, the solution directly empowers small and medium scale manufacturers to advertise unused machine capacity that can be rented out to clients who require small lot production of components. The platform may also provide them the ability to focus their business development efforts to obtaining short term and long term contracts through the broker system. On the other end of the solution, the platform directly impacts system integrators by providing them with a near real-time state of suppliers' performance and technical metrics. The solution architecture also allows enterprise customers to update their supplier databases in near real-time states without manual recurrent processing of supply chain information. The solution intends to make supplier sourcing, vetting and selection to be more efficient. Ultimately, the system intends to cut the time between product conception to production and delivery.

This I-Corps project is based on the functions and operations of a manufacturing service selection platform. Real-time streaming data from machines on a factory floor populates a virtual digital twin representation of the physical machine and its corresponding factory model in cyberspace. Utilizing a multi-agent simulation software algorithm, we match design and manufacturing features of a given product to manufacturers who have the capacity and capabilities to meet those requirements. The results of the simulation provide an intelligent decision making tool for original equipment designers to source suppliers who are best suited to meet both business and technical requirements. This digital connectivity of machine assets across job/contract shop factories provides unprecedented visibility into machine capabilities and capacities within the supply chain for real-time decision making in supplier sourcing and selection.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Small and medium scale manufacturers often resort to resort to continuous advertising at trade shows, online social media channels, trade magazines and past contracts to obtain new business. These methods rely on human networks and word-of-mouth recommendation on manufacturing services to the large system integrators. Our technology provides an intelligent method to find manufacturers who are best suited to complete a job based on the specific part order characteristics. While the proof-of-concept technology has been built, a major part of this NSF ICORPS grant was to explore the business plan needed to commercialize this technology.
Our team conducted more than 134 interviews with small job shop owners, young designers’ at large companies, supply chain professionals, procurement specialists and information technology personnel. We validated various components of the business plan through insights collected from our interviews. Following the interviews, we participated in business pitch presentations internally within competitions at NC State University. Feedback obtained from our presentations has helped refine our business model. The last year has also seen increased venture activity with a single company raising more than $50M in our targetted market segment. This has forced us to change our business model to accommodate the new dynamics within the market.
The original founders of the technology will continue to invest in technology development and differentiate ourselves from our competitors. The company Fabwave.io will be formed in August 2018 and are currently negotiating the license of the technology from NC State to the university based startup.
I-Corps: Robust Equation-oriented Chemical Process Optimizer
This I-Corps project focuses on applying rigorous optimization tools to solve process design optimization problems. For example, the broader impact/commercial potential of this I-Corps project can be associated with giving chemical process designers access to modern optimization techniques. While critical to all chemical companies, optimizing the design of a chemical process (in terms of choosing the process temperatures, pressures, flow rates, etc., that give the best economic performance) is still largely carried out empirically by design engineers. The workflow is typically iterative, labor-intensive and time-consuming, and the optimality of the outcome is not guaranteed. The proposed activity will place at the fingertips of engineers in the chemical, petrochemical, natural gas and other process industries a new computational framework that uses rigorous optimization tools to find the best process design. The commercial potential ranges from increasing the number of process alternatives that can be considered at the design stage, to supporting optimal investment decisions in new production facilities or upgrading existing ones. The proposed activity also has important potential societal impact: investment in US-based chemical production facilities (spurred by recent developments in accessing domestic shale gas and oil resources) will create new jobs and advance the training and development of the current and future workforce engaged in the design and operation of chemical plants.

Applying rigorous optimization tools to solve process design optimization problems has been hindered by numerical challenges posed by solving the process model - a set of very nonlinear equations corresponding to material and energy balances for reactors, distillation towers, heat exchangers, etc. found in a chemical and other process plant. The project relies on a recently-invented computational approach for solving and optimizing process flowsheet models. The approach involves a systematic mathematical reformulation of the model equations, resulting in equivalent expressions that can be solved reliably using common numerical methods. A library of unit operation models which can be linked together to create a mathematical model of an entire process is also available, and has been extensively validated on the design optimization of academic and industrial test problems. The commercialization of this work will bridge the large gap that currently exists between industrial practice and the numerous recent advances in optimization theory.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Summary
Chemical plants turn fresh or recycled raw materials into valuable products, using physical and/or chemical transformations. Process optimization aims to identify the operating conditions (temperatures, pressures, flow rates) under which these transformations occur in a chemical plant, that yield the best economic performance (highest profit/lowest cost). While critical to all chemical companies, improving process designs remains an empirical, labor-intensive “dark art,” typically carried out by engineering experts. They rely on their experience to pick a set of process conditions, then compute their impact on process economics by simulating them on a computer model of the process. This results are evaluated, a new set of process conditions is chosen, and the procedure is repeated until a satisfactory answer is obtained. In an effort to get to market as quickly as possible, this iterative search often falls well short of the best solution (the “optimal design”). A modern gradient-based computational optimization tool is required to this end. Current computer modeling and simulation software cannot be used in such optimization tools due to structure of the models and solvers, which is geared towards solving each unit operation individually.
The project team invented a new way of writing computer models of process flowsheets, for solving these models (process simulation) and for process optimization. It involves a systematic mathematical reformulation of the model equations using pseudo-transient continuation methods. In this way, they can be solved reliably using common numerical integration algorithms. This concept has been validated using numerous academic test problems as well as industrial case studies, producing significant capital and operating cost savings compared to empirical designs.
This I-Corps project aimed to evaluate the market size and opportunities for commercializing these tools to the US chemical and petrochemical industry. It involved an extensive survey, comprising over 100 in-person or web-based interviews of industry experts and managers. The survey revealed a clear picture of the current status of process optimization knowledge and practice.
The close interaction with practitioners and potential customers, and the immersion in the start-up environment provided by the I-Corps curriculum were an eye-opener in terms of conceiving, formulating, seeking funding for, and ultimately commercializing a research project. While this exploratory project indicated that our technology is not yet ready for commercialization, the team developed new ideas for research that will eventually lead to another commercialization attempt.
The project provided exceptional training and development opportunities for the entrepreneurial leads (ELs) Richard Pattison (at the time a post-doctoral researcher, now working at a startup specialty chemicals company) and Calvin Tsay (graduate researcher). The ELs interacted with a broad cohort of industry experts, while improving their interview, interaction and networking skills.
Intellectual Merit
The most significant technical commercialization barriers identified in the survey are summarized below and described extensively in the paper by C. Tsay et al. (Comput. Chem. Eng., 112, 180-189, 2018). They should provide direction for new research in the field.
1. Ensuring seamless integration of optimization capabilities and custom, detailed modeling in the process simulation software tools that are already familiar to industry practitioners. At the modeling level, the ability to deal with complex, physical models, as well as with subject matter expert knowledge (described in the form of, e.g., spreadsheets) will be very valuable. Furthermore, it is necessary to incorporate capabilities for model validation and data reconciliation, allowing engineers to minimize model uncertainty. The implementation of these capabilities focus on usability: an easy to use interface for problem definition, transparent troubleshooting, fast computation and a detailed presentation and interpretation of the results.
2. Alignment and information availability: sharing optimization-relevant information between the entities involved in the process design and operations ecosystem. Starting at the equipment manufacturer and technology development level, each process or process concept must be accompanied by a “digital twin,” a model that can be used throughout the lifecycle of a process to improve design, optimize operations and operational safety, and ultimately help with decommissioning.
Broader Impacts
We expect that our study will encourage industry-wide support and engagement in workforce development and advanced training programs. The undergraduate and graduate chemical engineering curricula provide limited exposure to modeling optimization concepts. Consequently, many engineers are not fully aware of the opportunities afforded by process optimization. Even if such awareness exists, engineers may not grasp the fundamentals of setting up and solving an optimization problem, recovering from solver failures and interpreting the results. Mandating that such concepts be taught at the undergraduate level is unrealistic given an already very full curriculum. A potential solution is the expansion of corporate training programs to a new model, whereby multiple companies would join forces and resources to develop the curriculum and support training of their employees. The course materials can be reasonably expected to belong to the “pre-competitive” domain, thereby ensuring that no trade secrets or valuable commercial information are disclosed.

CPS: Breakthrough: Solar-powered, Long-endurance UAV for Real-time Onboard Data Processing
In recent years, there has been a substantial uptrend in the popularity of unmanned aerial vehicles (UAVs). These aircraft find application in several areas such as precision farming, infrastructure and environment monitoring, surveillance, surveying and mapping, search and rescue missions, rapid assessment of emergency situations and natural disasters, next generation Internet connectivity, weather determination and more. Given the wide range of possibilities, UAVs represent a growing market in CPS and they are perceived as an "enabling technology" to re-consider the human involvement in many military and civil applications on a global scale. One of the major challenges in enabling this growth is UAV endurance. This is directly related to the amount of energy available to the UAV to perform its mission. This proposal looks to increase UAV endurance by trading off UAV performance with energy efficient computing. This requires mapping of mission and goals into energy needs and computational requirements. The goal of the project is to show that this trade can enable long-duration flight especially when solar energy is utilized as a primary energy source. The ambitious plan is to develop a light weight and efficient aircraft capable of maneuver-aware power adaptation and real-time video/sensor acquisition and processing for up to 12 hours of continuous flight (this limit being set by daylight hours).


This project aims to expanding the theoretical and practical foundations for the design and integration of UAVs capable of real-time sensing and processing from an array of visual, acoustic and other sensors. The traditional approach for small size UAVs is to capture data on the aircraft, stream it to the ground through a high power data-link, process it remotely, perform analysis, and then relay commands back to the aircraft as needed. Conversely, this research targets a solar-powered UAV with a zero-carbon footprint that carries a high performance embedded computer system payload capable of budgeting at run-time the available power between the propulsion/actuation subsystems and the computing and communication subsystems. First, a set of accurate power models for the considered UAV will be constructed to establish a mapping between different flight modes (aircraft maneuvers) and the corresponding power requirements at the propulsion/actuation subsystem. Second, software and hardware-level power adaptation mechanisms will be developed to devise a novel Power Adaptive Integrated Modular Avionic (PA-IMA) architecture suitable for UAVs. Safe temporal/spatial partitioning among applications and flexible scheduling to handle unpredictable power/load variations in flight represent key requirements. Once an accurate characterization is available for flight and computation modes, a higher-level supervisory logic will be developed to distribute the available power budget between the propulsion/actuation subsystem and the computation/communication subsystem. While precision farming and land/infrastructure monitoring will immediately benefit from such a technology, the long-term impact of this research is much broader since it explores the very foundations of environment-aware power and computation management. In general, the developed theory will be applicable to autonomous vehicles and robots whose power budget is limited and variable: these are common challenges faced when harvesting solar and wind energy.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The growing application space of unmanned aerial vehicles (UAVs) is creating the need for aircraft capable of autonomous, long-distance, and long-endurance flights for such applications as precision farming, environment monitoring, surveying, as well as search and rescue missions. The two main challenges are the limited power and data processing capacity of UAVs, as well as the adaptation to real-time detected stimuli, changing the course of the mission.
As outcomes of the project (titled: "Solar-powered, Long-endurance UAV for Real-time Onboard Data Processing"), a solar-powered unmanned sailplane with demonstrated all-day flight capabilities was developed. The UAV was developed from commercial-of-the-shelf components, making it affordable for a wide variety of applications. Development efforts have focused on the airframe layout, avionics, and flight software, and significant effort was allocated to the re-design of the propulsion and energy systems. The avionics and the open-source uavAP flight software were integrated into the aircraft, which was flight tested in a variety of conditions, confirming aircraft power consumption and production values. Long-endurance flight testing has occurred with the most challenging flight spanning 8 hours in cloudy, low-solar conditions with high winds of 10-20 mph and turbulence (where multi-rotor drones would be grounded). Being able to sustain long continuous flight decreases the need for takeoffs and landings, which constitute the riskiest portions of aircraft operation. With all of these ambitious goals achieved, the resulting unmanned aircraft will be more readily available to various communities such as research, industry, and emergency response, among others.
This project expanded the theoretical and practical foundations for the design and integration of solar-powered UAVs capable of real-time sensing and processing from an array of visual, acoustic and other sensors. Power awareness in solar-powered UAVs plays a key role to reduce aircraft size, weight, and ultimately production costs, thereby enabling more widespread use of UAVs for research and general society. Currently, design and testing of avionics for low-power unmanned aircraft is an iterative process that involves several test flights to gather power consumption data, interleaved with multiple revisions of the flight management software. All long endurance solar-powered aircraft to date have incorporated custom airframe designs and many custom components (e.g. single-application propellers or motors). To significantly reduce design and flight test time and software development costs, we have developed a power-aware UAV Emulation Environment (uavEE) that interfaces with high fidelity simulators to simulate the flight behavior of the aircraft in countless test scenarios and applications. Furthermore, we developed an accurate data driven approach for modeling of power consumption (propulsion, actuation, and computation), storage, and generation (solar) of fixed-wing UAVs. The uavEE and power model elements have been experimentally validated using a fixed-wing UAV testbed.
As mentioned earlier, limited on-board energy significantly limits flight time and usability of UAVs. The propulsion system plays a critical part in the overall energy consumption of the UAV; therefore, it is necessary to optimally select propulsion system components that are best suited for the desired mission profile, i.e. propellers, motors, and motor controllers (ESC), etc. A propulsion system optimization tool was developed that determines the optimal propeller and motor combination(s) for electric, fixed-wing unmanned aircraft, given desired mission requirements. Additionally, the tool considers aircraft safety in stall and upset recovery conditions. The tool has been experimentally validated by means of flight testing using a testbed aircraft, improving its efficiency by 20%. Simulation has shown significant energy saving potential as high as 50-75% depending on the mission profile. Specific implementation of the propulsion optimization tool towards the solar unmanned aircraft showed realizable savings up to 19% from the baseline configuration.
The main innovation consisted of a low-cost design using commercially available components, which are more accessible for research and general society. The intellectual merit of this work was the development and demonstration of a virtual and physical prototyping and testing framework (digital twin) that enables the rapid development of unmanned aircraft, the design of novel power optimization tools, and the implementation of a virtual emulation environment to evaluate the control algorithms in high fidelity simulations (uavEE). Additionally, a new autopilot framework was specifically designed to handle long-endurance, autonomous flight (uavAP).
As broader of impacts of this work, the designed solar-powered unmanned sailplane is capable of supporting computing and sensor payloads for a variety of civilian applications such as: precision agriculture, search and rescue, infrastructure inspection, environmental monitoring, and surveying and mapping. As future work, the unmanned sailplane will be tested for digital agriculture applications.
PPO: Platforms for Advanced Wireless Research (PAWR) Project Office
Innovations in wireless communication networks and applications relying on them have now become vital components driving the nation's economic growth and productivity. Sustaining the rapid growth in these technologies is essential to maintaining the nation's leadership and economic competitiveness. In July 2016, the National Science Foundation announced a multi-year effort, called Platforms for Advanced Wireless Research (PAWR), aimed at creating a set of city-scale testbeds to promote research in advanced wireless communication and networking technologies over the next decade. In this effort, the NSF was joined by a Consortium of companies with commercial interests in the wireless technologies resulting from this investment. PAWR will enable experimental exploration of robust new wireless devices, communication techniques, networks, systems, and services that will revolutionize the nation's wireless ecosystem, thereby enhancing broadband connectivity, leveraging the emerging Internet of Things (IoT), and sustaining US leadership and economic competitiveness for decades to come. PAWR will also enable rapid commercialization of promising technologies, bringing jobs and economic vitality. Researchers will have access to realistic, city-scale testbeds for testing new wireless theories and concepts, while a whole new generation of participating graduate students will emerge with hands-on practical training.

In order to support the design, development, deployment, and operations of the advanced wireless research platforms, the National Science Foundation's (NSF) Directorate for Computer and Information Science and Engineering (CISE) is funding this award, which will support the work of a PAWR Project Office (PPO). Working closely with the wireless research community, the PPO will assume responsibility for design, development, and deployment of a set of advanced wireless research platforms. Upon successful completion of the design of advanced wireless research platforms, the PPO will proceed to the development and deployment phases with funding provided by NSF as well as the Industry Consortium. The PPO will professionally establish and manage the advanced Research Platforms needed to unleash American innovation, drive economic development, and help extend US global leadership in the wireless industry. The PPO will work closely with the wireless research community in all aspects of the design, development, deployment, and operations of PAWR. A PAWR Steering Council (PSC), comprising research leaders in wireless networking technologies and a subset of the Industry Consortium, will represent the community's research interests in PAWR. The PSC will be chartered and supported by the PPO, with the goal of obtaining advice on all aspects of the deployment and operations of the advanced wireless research platforms.
Disclaimer
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The Platforms for Advanced Wireless Research (PAWR) program was designed to help ensure continued US leadership in advanced wireless networking technologies, with the PAWR Project Office (PPO) created to oversee the selection, deployment, and early operation of four large-scale wireless testbeds.
The success of the PAWR program is evidenced by the launch of POWDER, COSMOS, AERPAW, and ARA. These fully operational testbed sites – each with their own distinct area of technology focus – now serve as shared resources for the research and development community.
Regarding intellectual merit, these platforms enable advancements in the field of wireless networking by supporting reproducible, iterative, and collaborative research. Regarding their broader impact, these platforms not only drive innovations to benefit society, they also offer unique learning opportunities to an extended population by making complex, programmable systems widely available for use.
Among the research areas supported by the PAWR platforms are: network virtualization and optimization; AI-driven network functions; cybersecurity testing; open source hardware and software development; dynamic spectrum management; the communications impacts of unmanned aerial vehicles (UAV); and precision agriculture applications powered by advanced networks.
Research highlights include: testing of spectrum sharing technology on a live, open source 5G network; the development of a digital twin environment combining open source radio software with UAV control software; collaborative plugfests to demonstrate the capabilities of open radio access networks in areas such as increasing energy efficiency and orchestrating priority network access across multiple network systems; and the use of 5G technology to monitor crop fields and cattle from drone-mounted cameras.
In collaboration with the National Science Foundation, the PPO established several important elements in the PAWR program contributing to the successful launch of the platforms and their long-term sustainability. The PPO successfully implemented a broad-based industry consortium of more than 30 leading wireless companies and associations. These partners contributed equipment, services, and funding to support deployment of the PAWR testbeds.
The PPO oversaw the successful integration into the PAWR program of Colosseum, the world’s largest radiofrequency emulator. The inclusion of Colosseum in the PAWR program was the result of a collaboration with the Defense Advanced Research Projects Agency (DARPA). The RF emulator provides a valuable environment for running wireless experiments in emulation before moving them to live environments on the PAWR platforms.
The PPO also engaged with the Federal Communications Commission (FCC) to establish PAWR platforms as the first ever innovation zones for spectrum research, enabling more flexible access to spectrum for wireless research and experimentation.
There remain significant challenges in the increasingly complex field of wireless networking. The PAWR platforms collectively are a unique national resource for helping to address those challenges and are recognized as such in the research community, across government agencies, and throughout the wireless industry.

EAGER: Techniques for Deploying Mission Critical IoT Applications
The rapid deployment of sensing, computing and communication devices is resulting in significant growth in Internet of Things (IoT) infrastructure, and in increased use of this infrastructure for applications in many domains such as medical, agricultural, environmental, commercial, and homeland security. The area of sensor networks, and cloud and edge computing has contributed significantly to the development of IoT. However, IoT presents a more challenging environment as compared to edge computing and sensor networks: Specially challenging is the deployment of mission-critical applications - applications whose performance may deeply impact business/organization operations. The goal of this proposal is to develop techniques to deploy mission critical applications with stringent requirements in IoT environments.
This proposal will address the problem of deploying mission-critical systems by focusing on the following tasks: (a) The first thrust will be development of techniques to customize deployments to meet the needs of specific applications. One would like to design the IoT software base to be generic so that it can be used by a wide range of applications. However, generic software may not be suitable to to meet the requirements of mission critical applications. This may force the designers to manually develop services specialized for specific applications. The project will develop techniques that can automatically optimize generic software to obtain customized versions to meet the needs of specific applications. (b) The second thrust is the development of compositional techniques that will systematically exploit commonalities between the applications. As there may be multiple applications that share the underlying resources, exploiting such commonalities will be essential to meet stringent performance requirements. Extensive performance evaluation will be conducted to evaluate the performance of implementations generated by the proposed techniques with respect to manually developed implementations.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project has developed simulation software for the Internet-of-Things (IoT) infrastructure deployment. The simulation software has capabilities of simulating sensing and communication elements in smart entities such as smart streetlights, vehicles, traffic lights, and so on. Thus, the software can be used to make decisions on efficient and effective deployment of smart city applications.  
The results of this project are informing the development of infrastructure for IoT applications in communities and cities. We are collaborating with officials in the City of Syracuse in their development of smart cities infrastructure. The developed simulation software can serve as the digital twin of the infrastructure being deployed in the City of Syracuse. 
IoT applications are being increasingly used for applications that can have significant societal impact. The results of this project have the potential of impacting the development of these applications and their adoption in the communities.
This project has produced high quality papers. The dissemination of the results includes conference presentation, seminars, institutional research days, invited presentations and keynotes.
This project has involved two high school students in summer 2019, and one high school student in Spring 2021. The goal is to encourage students to pursue studies in engineering and computer science disciplines.
Understanding the Prime Factors Driving Distortion in Milled Aluminum Workpieces
Rework or rejection of metallic components due to component shape errors resulting from machining distortion reflect a significant economic loss. The cost of machining distortion is estimated to be billions of dollars annually, with a large part borne in the aerospace sector, so the control of machining distortion is a significant industrial challenge. Existing engineering approaches address the problem, but provide only point solutions to specific circumstances and rely on empirical trials at high cost. The collaborative research between the University of California, Davis (USA) and the University of Kaiserslautern (Germany) focuses on understanding the main factors and mechanisms that drive machining distortion, and subsequently explores the use of models for prediction and control. Distortion will be analyzed by analytical and experimental methods across variations of its main drivers: initial residual stress and geometry of the workpiece and parameters used in machining. The work will advance sustainable manufacturing in aerospace and other metal working industries by studying approaches that can lead to reduced losses from rework and rejection of parts. Along with graduate students directly involved, graduate and undergraduate students working in research laboratories at the two partner institutions will benefit from the international communication and exchange. Research results will be used to educate undergraduate and graduate students at both universities in existing courses in manufacturing and engineering (mechanical and aerospace), and the general public will learn about machining distortion and compensation through publications, conferences, and institutional websites. New knowledge will be gained and shared on how part geometry, symmetry, and stress levels affect machining distortion.

The main objective of this project is to understand how workpiece deformations can be forecast and controlled by predictive or compensative techniques for more economical and sustainable manufacturing. The research plan relies on unique expertise and facilities at two partner institutions, and synergizes metal-cutting experiments and residual stress measurements with analytical and numerical modeling. The deep understanding of residual stress at the University of California, Davis is complemented by the expertise in milling assessment at the University of Kaiserslautern. The hypothesis to be tested is: the effects of bulk residual stress (from material processing) and machining residual stress (from milling) each create distinct effects on workpiece deformation and can be separated. Therefore, bulk residual stress and machining residual stress are first analyzed separately. In work task 1 at UC Davis, different input models for bulk residual stress will be investigated, in particular process models from the University of Kaiserslautern and the eigenstrain model from UC Davis. Furthermore, different methods (simultaneous or incremental material removal) to simulate geometry changes in workpieces with bulk residual stress are analyzed. UC Davis is responsible for defining the bulk material stress levels and characterizing bulk residual stresses with established mechanical techniques. Complementary, work task 2 is performed at the University of Kaiserslautern and different input models and their quality for machining residual stress will be investigated. Comprehensive milling experiments will help to understand the distortion of thin-walled monolithic workpieces from machining residual stress and serve as a database for the combined bulk and machining residual stress distortion. At the University of Kaiserslautern, X-ray diffraction is used to characterize the surface residual stresses. Based on the new knowledge, in work task 3 both research partners will analyze how the machining residual stresses depend on the bulk residual stresses, for example by machining within different stress regimes. In addition, boundary conditions such as effects of workpiece setup and constraint and the geometric criteria for three regimes will be explored: where both types of residual stresses or one individually have major impact on machining distortion. Finally, in work task 4 compensation techniques will be investigated and summarized first in a best practice model. The possibility to compensate the bulk induced distortion via a deliberate machining induced distortion will be explored.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Distortion can arise during machining of metals and is a significant and expensive problem in the aerospace sector for milling of thin-walled monolithic aluminum workpieces. This project focused on understanding the main factors that drive this distortion and developed models for prediction and control. In particular, the two prime factors, release of bulk residual stress (BRS) from material processing and release of milling-induced residual stress (MIRS) from deformation and heat in the machining process were analyzed and their interaction. Machining experiments, residual stress measurements, and analytical and numerical analysis were performed on aluminum alloy AA7050, in quenched and artificially aged state (T74) with high residual stress, and in plastically stretched, stress-relieved state (T7451) with low residual stress. New knowledge was gained about the milling process of these materials and resulting residual stresses, as well as about different residual stress measurement methods for bulk stress (slitting and its variant cut mouth opening displacement method) and near surface stress (hole-drilling, slotting, cos(α) x-ray diffraction (XRD), sin2(Ψ) XRD, and wafer distortion). It was confirmed that bulk residual stress and near-surface residual stress did not obey the law of elastic superposition and preexisting BRS significantly changed the distribution of MIRS. A finite element model was carefully developed and validated that can implement complex part geometry, material properties, and residual stress measured after representative machining tests in workpieces of simple geometry. Different compensation techniques for machining distortion were highlighted including changing cutting parameters to change milling-induced residual stress, adjusting part design (wall thickness or rib design) to influence bulk residual stress, or change the milling path to influence both, MIRS and BRS. The distortion prediction model allows to reduce distortion or to mill an inverse shape onto the sample backside.
The results of this research can be used as a digital twin and help to increase the sustainability in manufacturing, in particular reduction of scrap rate and rework and increase of energy efficiency over the complete manufacturing process.

The project was synergistic research between the University of California, Davis and the University of Kaiserslautern in Germany. Metal-cutting experiments and residual stress measurements were collaboratively integrated with analytical and numerical modeling. The project involved 4 senior researchers, 7 graduate students, and 5 undergraduate students over the project period. All project partners benefitted from the international collaboration and communication, translation of different engineering approaches and methods, and international team work. The results of this project were documented through a public website, 3 journal papers, 5 peer-reviewed conference papers, 2 PhD and 2 MS theses, multiple conference presentations and posters, and several more papers in preparation.

II-NEW: Scalable Software Defined Radio Network Testbed for Hybrid Measurement and Emulation
Recent advancements in wireless network protocols offer opportunities in new research areas including algorithm design, spectrum optimization, throughput maximization, and wireless security. Each time wireless technologies evolve, additional testing and certification steps are required, and, in most cases, several rounds of such verification are needed. These steps are crucial to ensure the final product is indeed ready for public use. Wireless protocol and algorithm developers leverage highly-customizable software defined radio (SDR) nodes to create and improve their design. Although SDRs offer an ideal environment for testing using point-to-point wireless links, they incur an increased overhead to the user for when a large-scale network measurements involving multiple radios are required. At present, there does not exist, a remotely accessible testbed that would allow users to customize SDR applications and test their radios with controllable radio propagation (emulated, simulated, and over-the-air) channels. The testbed resulting from this project will achieve this goal, and in doing so, establish Drexel University as a leader in cybersecurity and telecommunication systems research. Upon completion of the project, the equipment will be locally and remotely shared with faculty, staff, and students in the research community for educational purposes.

The testbed will consist of Ettus USRP SDR nodes (i.e. N210, X310) connected to an RFNest 16-port Wireless Channel Emulator from Intelligent Automation, Inc., which offers capabilities of providing highly controlled wireless environments for repeatable experiments. A hardware infrastructure will be developed to distribute radio frequency and timing cabling as well as to install these nodes on a ceiling scaffolding grid structure in Drexel's new laboratory space. A software infrastructure will be developed to allow for researchers to utilize the testbed for customized experiments involving emulated, simulated, and over-the-air wireless channels, where this unique combination of features have never been offered before. This infrastructure will require custom application programming interfaces (for interfacing with the SDRs, RFNest, host computers, and switches) and remote access interfaces (web GUI, ssh) to allow users to upload their custom hardware and software designs. By leveraging existing software platforms, the project will offer sophisticated experiment setup, measurement and data management tools as well as emulated network options. To add to the unique features of the testbed, Drexel's software defined communications (SDC) testbed, which offers on-the-fly scalability, will be ported to X310s and made available as a part of the project.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The goals of this project were to develop the Drexel SDR Grid Testbed.  The relevant key features of the testbed are the ability to leverage: i.) software defined radio (SDR) for flexible, real-time prototyping of both the individual transceivers and the overlying network infrastructure, and ii.) a large scale (24x24 node) full-mesh wireless channel emulator to achieve fully customizable and repeatable wireless propagation channels representative of the dynamic urban environments in which the network will be deployed.  One of the unique aspects of the testbed is that it can prototype actual transceivers to be deployed in the field, but in the laboratory, with realistic propagation and network conditions provided by the emulator, prior to field deployment.  This experimental capability and approach is unique, and has led to substantial technical contributions in the deployment of future wireless technologies.  Our testbed is remotely accessible for use for research and education purposes.  Furthermore, a full-stack software defined radio developed for the testbed along with experimental datasets generated by the testbed, have been released.
The Drexel Grid SDR Testbed has enabled new research in offensive and defensive wireless network cybersecurity, development of new adaptive antenna technologies, machine learning for dynamic spectrum allocation, as well as field prototyping of new medical internet of things (IoT) and multi-target tracking radar technologies.  We have also leveraged the testbed for educational experiences impacting undergraduate and graduate students, notably through the development of a Software Defined Radio Laboratory course.  This class teaches the fundamentals of analog and digital communications, provides cross-stack wireless network training, and also includes a "Radio Wars" competition where students learn how to maximize the transfer of data while mitigating the effects of jamming.  This course is part of a new Master of Science program in the Internet of Things that was recently launched at Drexel University.

CAREER: Understanding Grain Level Residual Stresses Through Concurrent Modeling and Experiments
Polycrystalline materials such as metals, alloys and ceramics dominate the infrastructure of modern society in terms of both tons of raw material usage and in the breadth of applications, which span energy, transportation, defense, and other sectors. Stresses introduced during the processing of these materials, known as residual stresses, are ubiquitous in all materials and can have tremendous effects on performance. Despite their presence and critical role in component life, many analysis efforts cannot account for residual stresses. This Faculty Early Career Development (CAREER) award supports the method development to characterize residual stresses at the micron level within polycrystalline materials. The research will lead to the development of a computational framework to account for residual stresses, and faithfully predict their distributions. The research leverages recent advances in High Energy Diffraction Microscopy through collaboration with the Advanced Photon Source (APS) at Argonne National Laboratory. This award also supports an innovative educational program that is twofold. First, a series of continuing education courses will be developed focusing on the treatment of residual stress analyses produced in this research, specifically targeted towards the design systems, structural analysis, and manufacturing communities (i.e. non-materials) across professional societies. Second, hands-on learning activities will be introduced within Purdue's Space Day, an existing grade-school outreach program, which will focus on residual stresses in aerospace materials.

In order to establish a basic understanding of residual stresses, the research team will: (i) develop a modeling framework to initialize and evolve residual stresses based on a backstress formulation present on individual slip systems, (ii) measure residual stresses and their development at the grain and sub-grain length scales within polycrystalline aggregates via high energy x-ray diffraction microscopy, (iii) develop techniques to map dislocation arrangements within the bulk of structural alloys, and (iv) produce validation datasets for crystal plasticity models containing state dependent variables that were previously impossible to measure. The work is predicated on a synergy between state-of-the-art experiments and simulations at the same length scale. The results will unequivocally elucidate the role of residual stresses across length scales in polycrystalline materials, in order to develop more accurate lifetime predictions of the alloys and fabricate tailored components that offer either minimal or beneficial residual stresses and therefore are more resistant to failures.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Polycrystalline materials dominate the infrastructure of modern society in terms of both tons of raw material usage and in the breadth of applications, which span energy, transportation, defense, and other sectors.  Residual stresses are ubiquitous in all materials due to their process and in service use history.  Despite their presence and critical role in component lifing, many analysis efforts do not account for residual stresses.  This CAREER award supports the method development to characterize residual stresses at the micron level within polycrystalline materials and develop the computational framework to account for residuals stresses and faithfully predict their distributions.  The work is predicated on a synergy between state-of-the-art experiments and simulations at the same length scale.  The results have unequivocally elucidated the role of residual stresses across length scales in polycrystalline materials and can be directly used to develop more accurate lifetime predictions of the alloys.  
In order to establish a basic understanding of residual stresses, the PI's team has: (i) developed a modeling framework to initialize and evolve residual stresses based on a backstress formulation present on individual slip systems, (ii) measured residual stress evolution at the grain and sub-grain length scales within polycrystalline aggregates via high energy x-ray diffraction microscopy, (iii) developed techniques to map dislocation arrangements within the bulk of structural alloys connected to residual stresses, (iv) produced validation datasets for crystal plasticity models containing state dependent variables evolving with respect to applied loading that were previously impossible to measure, and (v) extended the ideas of residual stresses to examine polymeric composites, ceramic-metal (cermet) composites, and environmental barrier coatings on C-C composites.
Through experimental characterization, a general framework was established to systematically move up and down in length scale, from dislocation structures, geometrically necessary dislocations (GNDs), and associated residual stress at that same point (type III).  Further, the self-consistent framework can be applied (to scale up or scale down), thus moving in an equivalent manner from residual stresses of type III to type II (grain-average) to type I (across a component) or vice versa, as shown in Figs. 3 and 4.  This residual stress analysis fills a missing link in our engineering theory of materials behavior, as residual stresses are always present but not often accounted for in our engineering analysis.  The definitive means to measure residual stresses and systematically identify the corresponding GNDs connect engineering practice to fundamental materials science, therefore enabling the use of more physics-based tools to describe the mechanical behavior at the microstructural scales and lifing at the component scales.  
Further, the team has embraced and leveraged recent advancements in X-ray characterization tools, specifically high energy X-ray diffraction microscopy (HEDM) and dark field X-ray microscopy (DFXM).  HEDM allows for the characterization of millimeter sized specimens, including the morphology and grain averaged orientation and elastic strain tensor of each grain within the polycrystal.  Subsequently, DFXM enables the investigation of the intragranular misorientation and elastic strain of a set of lattice planes within a selected grain in the bulk of a polycrystal with increased resolution (~100 nm spatial resolution and 10^-4 angular resolution).  The team has demonstrated extreme sub-grain scale residual stress gradients present in polycrystalline microstructures, on the order of 400 MPa over a 30-micron span near a twin boundary, which is well captured via modeling approaches (Figs. 1 and 2).  Additionally, these results are extended to study the elastic strains and intragranular misorientations near triple junctions, compared to grain boundaries (Fig. 5).  Lastly, the bulk HEDM-based elastic strains are compared to high resolution surface strains, which is used to examine slip transmission across neighboring grains and discuss the degree of heterogeneity in the deformation response (Fig. 6), which increases with respect to the applied strain.  
This award also supports an innovative educational program that is twofold.  First, a series of continuing education courses haven been developed focusing on the treatment of residual stress analyses produced in this research, specifically targeted towards the design systems, structural analysis, and manufacturing communities (i.e. non-materials) across professional societies. The expansion of such cross-professional society activities has spanned many outcomes including (i) organization of verification and validation best practices for integrated computational materials engineering (ICME) approaches, (ii) opportunities for cross society integration of ICME within a digital engineering paradigm that has led to digital twin panels and AIAA society position papers, and (iii) a series of hands-on tutorials that the PI has organized at the 6th ICME world congress (2021).  Second, hands-on learning activities have been introduced within Purdue's Space Day (PSD), an existing grade-school outreach program, which the PI incorporated materials focused activities, including an activity to demonstrate residual stresses in aerospace materials.  PSD has grown each year to accommodate over 800 grade-school students per year and extended during this award to add outreach activities that has reached over 2500 students during this period.  
Bridging the Gap Between Education and Research through Pre-College Engineering Systems (PCES) Outreach Program
The purpose of the project is to develop a residential summer research and education Science Technology Engineering and Mathematics (STEM) outreach program targeted at 11th and 12th graders from across the country. The program will advance students' preparation for careers in engineering through integration of research and education. The students in the program will undertake lectures on topics such as advanced mathematics and physics, chemistry, preparatory scholastic aptitude testing, advanced placement courses, and will be exposed to fundamentals of engineering courses and special topics in Electrical and Computer Engineering such as artificial intelligence, communications, nanotechnology, photonics, energy systems and smart grid technologies. These courses will prepare and motivate students for research work in energy management, automation functions for secured critical infrastructures, sensor based systems and safe environment. The background achieved and exposure through hands-on activities will capacity for a future diverse workforce of underrepresented groups and women to pursue careers in engineering.

The research work in the pre-college engineering systems program will include various fields of engineering and science used to help students to appreciate the role of creativity, analytical and hands-on work in conducting engineering projects, processes and systems. The research work will involve needs assessment, constraints, problem formulation and design of algorithms, implementation, testing and validation under different scenarios. The major areas for project ideas include communication and signal processing, photonics/electronics, nanotechnology, materials and energy and power systems, with each area having its own set of specific projects. Under communication and signal processing, specific projects include the solar bag, smart phones, new integrated display board, and others. In photonics/electronics, the specific works include mobile diagnostics for power electronics and smart displays. Nanotechnology and materials projects include design of nano toothpaste, nano skating boots, and a nano tricycle for kids. In the area of energy and power systems, smart city design using renewable energy resources and electric vehicles with self driving properties are specific project areas. Furthermore, wireless sensors, a handmade wristwatch for health monitoring, and automatic fan control will be pursued within the computer engineering projects. Students will work in teams and final products will be presented at the end of the program each year to an audience of faculty, parents and a representative of the funding agency, the National Science Foundation (NSF). Mentoring during and after the program (during the school year) will be in place to track students' continued progress toward life-long pursuit of engineering as a carrier. Lessons learned will be shared with other engineering schools so that the country will benefit from the NSF investment in education and research activities for pre-college students at Howard University.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
 The  Howard university Precollege Summer program for bridging the gap between research and education was a 4 weeks program to enable 11th and 12th grade students to engage in theory and hands on experience of topics in engineering systems designed for electrical, electronics, computer science and engineering topics.
The program consisted of lectures in Calculus I & II, energy systems, Network theory, electronics, electromagnetism, AI and communication networks. Hands on experiments in ohm law, Kirchhoff's law, diodes, system operation amplifier concepts at introductory undergraduate level courses. Prep courses in SAT were also done.
Research projects such as smart homes, digital twin, load allocation under limited energy resources, and internet of things (IoT) were developed to hand shake between lecture topics that were taught.
The project was done in teams with support from senior undergraduate/ graduate students as mentors under the guidance of the PI and the faculty team.
Exposure for preparatory students to achieve high performance in SAT and promote admission to top Universities to study in STEM courses.
Based Impacts
The program has exposed a number of under privileged minority to gain firsthand knowledge of STEM education.
Students were able to take examinations and make presentations to faculty, family, NSF program Director and PI. Assesment.
Research work done by students and experiments from the program was published in IEEE transactions and presented at National and International Conferences.
Over 90% of the students from the program are now graduates from top Universities such as Georgia Tech, Howard University, Virginia tech and University of Virginia.    
Student skilled in problem solving and interest in Engineering systems has attracted under represented group participation

GOALI: Collaborative Research: Model-Predictive Safety Systems for Predictive Detection of Operation Hazards
Model predictive control is widely being implemented in many industries, such as chemical plants and oil refineries, leading to substantial improvement in operations. The use of process monitoring through model-based sensors has enabled industries to predict and improve processes. Prior research has introduced novel safety systems using models, which generate alarm signals that can provide warnings of pending problems. This research project involves developing a process improvement model will not only prove useful for the chemical and petrochemical industries, but will also benefit the food, nuclear, aircraft, and petroleum industries by identifying potential hazards. Deployment of this model would result in saving lives, reducing workplace injuries, and economic benefits. The researchers are collaborating with the Air Liquide Corporation, which will ensure the industrial relevance and practicality of the results of this research and will enhance the dissemination of research results. The data resulting from this research project will also provide improved security of industrial operations. Additionally, the researchers are developing educational modules and projects based on the outcomes of this research for use in graduate and undergraduate engineering courses at Drexel University and the University of Pennsylvania.

The objectives of this research project are to study: (1) robust large-scale state-estimate prediction (robust to process-model mismatch and unmeasured inputs), (2) offline optimization-based calculation of the worst-case combinations of process-model parameter values and the most extreme control actions, (3) efficient implementation of the model-predictive safety system for large-scale plants, and (4) implementation and testing of the model-predictive safety system first on the steam-drum system of an integrated steam-methane reformer/pressure-swing adsorber unit through simulations, and then on a steam-drum system in a real integrated steam-methane reformer/pressure-swing adsorber system in real time at Air Liquide. The research team also is developing industrial guidelines for adding and maintaining model-predictive safety systems as a complement for existing functional (safety-instrumented) systems. The involvement of the industrial collaborator enriches the training of graduate and undergraduate students involved in the project. The research project also is being integrated with the Drexel Co-op Program, and undergraduate students, preferably from underrepresented groups, are being recruited for six-month long research internships.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Process safety is the first objective in every process industry. Despite continuous efforts to improve the safety of processes, the level of human and financial losses due to incidents in the U.S. process industries is still significant. Furthermore, due to the reactive and generally univariate nature of existing functional safety systems (Figure 1), a more-intense operation may not be realistic without innovation in functional safety. Thus, further improvement in process functional safety methods and procedures is needed. Among these efforts are the development and deployment of software packages that predict frequencies and consequences of incidents based upon historical data. However, these packages are usually unable to predict the probabilities of incidents that have never happened before. This inability has motivated the study of methods capable of predicting hazardous conditions in processes.
We introduced and developed the concept of model-predictive safety (MPS), which represents a new paradigm in functional safety; that is, the use of process model (digital twin) predictions to detect operation hazards before they lead to safety risks. MPS uses a dynamic model of a process to predict future process behavior and forecast the potential consequences of future incidents with reasonable accuracy. Such forecasts are then used to determine and implement optimal proactive actions. MPS allows for a systematic utilization of dynamic process models to generate alarm signals (alerts) for the predictive detection and proactive prevention of operation hazards (OHs) in real time. It uses a process model (digital twin) to predict the process safety status over a moving prediction horizon and to generate alarm signal(s) indicating the presence of a present or future OH with reasonable accuracy (Figure 2); it generates alarm signals that alert the process personnel to imminent and potential future OHs before the actual OHs occur. This combined predictive and proactive (prescriptive), real-time use of process models in process safety is an attractive unique feature of MPS. Unlike conventional safety systems that are individually reactive to current conditions through specifically designed logic, MPS systematically accounts for process nonlinearities and interactions among process variables and generates predictive alarm signals. Therefore, this new paradigm in functional safety is analogous to the evolution in process control from only single-loop control (e.g., proportional-integral-derivative control) toward multivariable model-predictive control.
The implementation of an MPS system requires off-line calculations of (a) the most aggressive action that MPS should take to prevent each operation hazard when uncertain model parameters take their nominal values, and (b) the most aggressive action that MPS should take to prevent each operation hazard when uncertain model parameters take their worst-case values. We developed novel min-max optimization methods that can be used offline to perform the calculations systematically. The performance and ease-of-use of the methods were shown using simulated chemical process examples.
To enable the distributed implementation of MPS on a large-scale plant, we used the concept of community structure to optimally decompose large-scale systems into multiple sub-systems. We formulated community detection as a multi-objective optimization problem and employed the concept of weighted modularity to conduct partitioning and identify the subsystems. Based on the non-sorting genetic algorithm, we developed and deployed a computer algorithm that generates a list of non-dominated solutions. The algorithm was tested on the Tennessee Eastman process to show its application and performance. We also used the concept of community detection to decompose a large-scale system into a set of observable subsystems, allowing for the distributed implementation of state estimators and MPS on large-scale systems.
RAPID: Houston in Hurricane Harvey (H3): Establishing Disaster System-of-Systems Requirements for Network-Centric and Data-Enriched Preparedness and Response
The collected data and analysis conducted advances the knowledge of disaster informatics, intelligent disaster response, and community resilience in four important areas: (1) documentation of time bound social media and crowdsourcing data that capture the information dissemination patterns, infrastructure disruption, the operational coordination performance during Hurricane Harvey; (2) integrative frameworks for assessment of disaster management systems and processes and identification of methods and ideas to enable network-centric and data-enriched disaster management processes and systems; (3) novel and improved methodologies for social sensing of community disruptions and societal impacts during the unfolding of disasters; (3) enhanced fundamental understanding the underlying processes of self-organized information dissemination in online social media during disasters.
From a scientific perspective, first, the system-of-systems approach, meta-network analysis framework, and the Disaster City Digital Twin vision all contribute to an ongoing paradigm shift towards networked and intelligent disaster management and emergency response based on integration of human and machine intelligence and employment of information technology and social media for better information dissemination and situation awareness. Second, the multi-modal data analytics methods created and tested in this study offer considerable advancements to the existing approaches because of their enhanced reliability though identification of critical tweets, ability to fuse geo-textual-visual data for mapping of infrastructure disruptions, capability for timely and fine-grained classification of events, and ability to sense emotion signal related to different disaster impacts to better examine societal impacts. Third, the theoretical and empirical insights obtained from this study advances the understanding of the underlying mechanisms affecting information dissemination on online social media during disasters. As social media become an important infrastructure for communities in coping with disasters, fundamental understanding of information dissemination patterns may hold the key for communities to become more intelligent and resilient in responding to disruptions. The characterization of reticulation modalities, specification of emerging influential users and their attributes, and evaluation of information spread patterns based on empirical data from Harvey advance the understanding of behaviors and strategies to improve information dissemination in online social media.
From a practical perspective, the outcomes of this research provide new empirical insights, knowledge, and methods to decision makers, emergency managers, and public officials regarding ways to improve intelligence in disaster management and emergency response. With the rise of new technologies, such as social media and crowdsourcing technologies, the landscape of rescue and relief activities during disaster response has evolved into a human-information technology ecosystem. The outcomes of this study could help various organizational actors to improve their capabilities required for intelligent and networked disaster response operations through: (1) timely and accurate situational awareness including awareness about hazards, events, infrastructure conditions, and community needs; (2) effective information dissemination with network effects; and (3) enhanced, data-enriched capacities for responsive decision support.
Broader Impacts
Societal Impacts: The outcomes of this research have important societal benefits that could potentially enhance the public safety of residents. The research outcomes could improve the ability of decision-makers to tailor their strategies and procedures to enhance the intelligent emergency response and disaster management. Accordingly, the study will benefit society and public safety.
Capacity Building Impacts: The study also contributes to enhancing education and research infrastructure at the participating university through: (1) creation of new databases related to social media, crowdsourcing, and  community disruptions prior and during hurricane Harvey; (2) direct involvement of three graduate and one undergraduate students in research activities; (3) creation of resources (e.g., informative data analysis and analytical methods) to disseminate the findings and outcomes to various agencies in the study region. The project helped strengthen community engagement ties between the research team and the local stakeholders and communities in Houston and Harris County, TX. Furthermore, this project also provided various opportunities for students to involve in the interdisciplinary data collection and analysis. This is an invaluable experience for the students to work with a multidisciplinary team, engage with stakeholders, understand the interdisciplinary nature of the disaster informatics and resilience research, and encourage the use of an interdisciplinary lens in conducting their future research.
Conceptual Impacts: The study contributes to advancing knowledge related to intelligent disaster response and disaster informatics through presentations at scholarly conferences and peer-reviewed publications.
This RAPID study will collect and analyze time-sensitive data from Hurricane Harvey in Houston in order to document the needs, challenges, and required capabilities for enhanced management, decision-making, and situation awareness in disaster system-of-systems (interdependent processes, operations, and systems involved in disaster preparedness and response). The management success and efficiency of disaster system-of-systems depend on the ability to integrate individual systems and processes and communicate information in a network-centric manner. The outcomes of this study will include: (1) A research roadmap and ontology specifying the requirements and characteristics for disaster system-of-systems in order to improve situation awareness, risk reduction, and decision-making during preparedness and response to extreme weather events; (2) An automated multi-modal data analytics framework integrating multiple heterogeneous datasets for improved situation awareness of community risks, events, and activities; and (3) A network analysis providing a guide for where, to whom, and how to utilize smart technologies and systems to enable network-centric and data-enriched Disaster SoS. These outcomes will provide important understanding and guide for researchers and practitioners towards creation and implementation of smart technologies and data analytics systems that enhance the resilience of communities to extreme weather events.

Establishing robust and resilient disaster system-of-systems (SoS) is essential to protect communities in extreme weather events. However, the current knowledge lacks important elements related to the SoS requirements and characteristics (e.g., architecture, technologies, and integration) to augment human processes, interactions, and decision-making. To address this knowledge gap, this study aims to collect and analyze perishable and time-bound data from the 2017 Hurricane Harvey in Houston to: (1) Map Disaster SoS elements, relationships, needs, and challenges through in-depth interviews and participatory workshops with elected and appointed officials, infrastructure managers, emergency responders, and decision-makers, as well as household surveys; (2) Examine the utility of cyber informatics and social sensing technologies to better integrate social media infrastructure with Disaster SoS for improving the community situation awareness; and (3) Map and analyze community networks to model communities by their activities, events, and communication in order to determine opportunities for utilizing smart technologies for network-centric preparedness and response processes. The outcomes of this project will have significant societal benefits that will build new smart and connected communities that are more resilient to extreme weather events. To attain the societal benefits, the project outcomes will be disseminated through three avenues: community engagement workshops, scholarly publications and presentations, and a project webpage.
Collaborative Research: Assessment of Tornado Loading on Low-Rise Buildings Using Computational Modeling and Tornado Simulator Testing
Tornadoes are a type of severe windstorm that repeatedly have caused devastating damage to the built environment in the United States. Low-rise buildings, in particular, historically have been among the structures that are the most vulnerable to tornado damage. Failures of this type of structure are often responsible for the social and economic losses caused by tornadoes, including the loss of life and property, as well as the interruptions to many critical societal functions. The goal of this collaborative research at the University of Arkansas (UARK) and Texas Tech University (TTU) is to further advance the fundamental understanding of tornadic loading on low-rise buildings. The outcomes of this research can be used for the assessment of building performance in tornadoes, the design of new tornado-resistant buildings, and the development of retrofitting strategies to improve the performance of existing buildings in tornadoes. The research project will be integrated with educational and outreach activities at both UARK and TTU to raise public awareness of the impact of natural hazards, including tornadoes, on the built environment and society in general. At UARK, students will be involved in the research through the Freshman Engineering Program, and the George Washington Carver Research Program that provides research internships to students from Historically Black Colleges and Universities. At TTU, the project will provide research training for students in the Wind Science and Engineering Ph.D. program. The research will also be incorporated with programs such as the Severe Weather Awareness Day in Lubbock, TX, and through partnerships with local K-12 schools and national organizations, such as the 4-H Youth Development and Mentoring Program, to reach out to both local and national communities. These research, educational and outreach activities will help make U.S. communities more resilient to tornado hazards.

To achieve the research goal, this project will utilize a research approach that combines the complementary strengths of physical experiments in a tornado simulator at TTU and numerical modeling based on computational fluid dynamics (CFD) at UARK. Unlike previous experimental and numerical studies that were conducted independently, the experiments and computational modeling in this research will be systematically coordinated. Customized experiments in the tornado simulator will first serve as the basis to validate and improve the capability of the computational model in simulating near-ground tornado-like flows and in capturing the interaction between these flows and generic structural shapes. The validated computational model and additional experiments in the tornado simulator will then be used to evaluate tornado-induced forces and pressures on low-rise buildings of various envelope configurations. Tornadoes of various structures, in terms of the swirl ratios and the maximum mean tangential velocity to translation velocity ratio, will be considered. The primary outcomes of this study will be in the form of pressure and force coefficients acting on different building shapes. These coefficients, as well as the raw data and the metadata, will be archived at both UARK and TTU and made available to the natural hazards engineering community through the NSF-supported Natural Hazards Engineering Research Infrastructure Data Depot (https://www.designsafe-ci.org/).
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project used experiments in a large tornado simulator to understand vortices that resemble tornadoes of various types in characteristics and the loading by these vortices on low-rise buildings. Rectangular shapes of various geometries were tested in the experiments to provide fundamental understanding of the interaction between bluff bodies and tornado-like winds. A model of a low-rise building is also tested in the experiments to quantify tornado loading on low-rise buildings. The data from the experiments revealed some major characteristics of the loading on the rectangular shapes as well as the characteristics of the loading on the low-rise building model. These characteristics were correlated with the characteristics of the velocity and pressure fields of the tornado-like vortices. The effects of the major factors that affect the characteristics of the loading were quantified based on the data from the experiments. 
In addition to being used in the study of the interaction between the tornado-like vortices and bluff bodies, including low-rise buildings. The data from the experiments were also used to validate and calibrate numerical simulations of tornado-like winds and the interaction between tornado-like winds and low-rise buildings, which were performed by the collaborating research team at University of Arkansas, as well as a digital twin of the tornado simulator, which was developed at Texas Tech University. The numerical methods developed in in collaboration with the partners at University of Arkansas complement the capability of the physical simulation in the tornado simulator. An integration of the two types of simulation can overcome the respective limitations of each approach, which can help accelerate the understanding of tornado effects on structures. 
The outcomes of the research contribute to the improvements of building design and construction against tornado loading.  These improvements can help significantly reduce the impact of tornado hazards to society.
CAREER: Rational Design and Manufacturing of Nanostructured Surfaces and Interfaces in Lightweight Materials
This Faculty Early Career Development Program (CAREER) award supports fundamental research on scalable manufacturing of lightweight structural materials including polymer-matrix composites and lightweight magnesium alloys. The grant looks to develop new surface modification techniques, relying on atomic layer deposition (ALD) which can deposit sub-monolayer thick layers which serve as seeds to subsequently grow new structures three-dimensional structures at the nanoscale. These designed nanostructures are remarkably versatile. They can serve to improve the strength of polymer composites allowing for light-weight materials as well as impede corrosion on metal surfaces. This addresses critical needs, for example, in the transportation sector to improve fuel efficiency and vehicle performance, while maintaining safety. Beyond the direct impact on lightweighting, the scientific knowledge generated will provide an alternative design paradigm to the bottom-up design and manufacturing at the nanoscale. The educational goal is to promote public awareness of the importance of nanomaterials for the future of domestic manufacturing. The educational plan integrates concepts of design and manufacturing, nanotechnology, and surface science across multiple levels, including a new 6th grade materials curriculum, integrating materials education into undergraduate student team mentorship, and promoting a more diverse STEM workforce through research demonstrations to encourage underrepresented students to apply to graduate school and participate in research.

Current manufacturing processes for controlling surface and interfacial structure in bulk structural materials suffer from poor control of geometric parameters such as feature size, shape, geometric orientation, which is particularly challenging within hierarchical assemblies of nanomaterials on non-planar surfaces. This lack of deterministic control of structure limits our ability to fundamentally understand how the nanostructuring produces specific material properties and a route to rationally design optimized structures to achieve application-specific properties. This project's effort addresses this limitation by quantitatively identifying the process-structure relationships needed to achieve deterministic control of interfacial geometry and composition at the nanoscale. The approach uses the concept of spatial ALD to generate sub-monolayer seeding areas for the growth of nanostructures through solution-based processes. The process can be repeated to develop three dimensional nanostructures on surfaces to precisely tune the interfacial geometry, composition, and microstructure of lightweight structural materials, and quantify the impact on their mechanical properties and corrosion resistance. An improved understanding of these process-structure relationships will provide the fundamental knowledge needed to ?encode? the manufacturing instructions for the controlled hierarchical assembly of nanoscale building blocks into surface architectures. This approach will be used to address two critical challenges in lightweight structural materials: 1) rational design of nanostructured interphases in polymer matrix composites to tune their mechanical properties, and 2) improving corrosion-resistance of magnesium alloys.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The project developed a fundamental scientific understanding of surface-directed assembly as a method to precisely tune the interfacial geometry, composition, and microstructure of lightweight structural materials. The work quantified the impact of these hierarchical structures at the nanoscale on multifunctional performance, including mechanical properties and corrosion resistance. Through the sequential deposition of tailored coatings using Atomic Layer Deposition (ALD), and its atmospheric pressure analog, spatial ALD (S-ALD), atomically-precise control of surface composition and defects was enabled on bulk materials, which was used as “seed layers” to drive programmable solidification of nanoscale constituents from fluid sources onto the surface. Understanding the fundamental mechanisms that guide this surface-directed assembly process was the emphasis of the research effort. These process-structure relationships provide the fundamental knowledge needed to “encode” the manufacturing instructions for assembly of nanoscale building blocks into hierarchical surface structures with programmable control of feature size, shape, and orientation.
A major outcome was the development of new ALD-seeded surface directed assembly processes. In particular, we introduced a suite of new processes that enable rational control of the resulting nanostructure size, shape, position, and geometric orientation. We then applied this methodology to fabricate complex, multi-component nanomaterial architectures, including hierarchical structures that span multiple length scales. Examples include core-shell-shell nanowires, nanosheets with tunable geometries, and compositionally graded interfaces. We utilized high-resolution microscopy and spectroscopy methods to reveal the mechanisms that guide this control of material geometry and structure, illustrating the dynamic nucleation, growth, and diffusion processes that occur. This mechanistic understanding was supported by multi-scale reaction-diffusion models. These findings represent a new “toolbox” for materials engineers to integrate nanomaterial architectures into complex, non-planar 3-D substrates without the need for traditional lithography or cleanroom processes.
The research approach spanned multiple disciplines, ranging from atomistic understanding of chemical reaction mechanisms to machine design and scalable nanomanufacturing. For example, we designed and fabricated a new mechatronic S-ALD system that allows for tunable control of process parameters such as dynamic control of alignment and gap size with closed-loop feedback control. A a 3-D transport-reaction model was developed to study how variations in SALD process parameters affect the resulting deposited materials. This direct hand-off between experiments and modeling enables us to create a digital twin of the manufacturing platform, to perform rapid screening and predictive modeling. In another example, we developed an automated hydrothermal growth system with digital control of key flow, temperature, and chemical parameters. This allowed for a study of the science of scaling up the surface directed assembly process onto large, non-planar surfaces. Computational modeling was used to describe the fluid and chemical interactions with the substrate, and to rationalize the observed process-structure relationships. To compliment these multi-scale manufacturing platforms and modeling packages, we also integrated novel metrology systems to provide in situ process sensing and real-time feedback.
Equipped with these new material fabrication processes, manufacturing platforms, and process models, we then applied these methodologies to explore the application of this new “toolbox” towards multiple real-world applications. Examples include multi-functional polymer-matrix composites with integrated electrical conductivity, corrosion-resistant Mg alloys, and silica aerogel monoliths with improved thermal and mechanical properties. A common theme of all of these applications is structural lightweighting, with the goal of overcoming key limitations of low-density materials for engineering applications. Again, these applications were supported by the rational control of material structure and geometry afforded by ALD and surface-directed assembly, as well as the computational models that allow for predictive control of the resulting surface nanostructures.
Throughout the project, we have engaged in a number of education and outreach activities, including: training of undergraduate and graduate students; enrichment of curriculum; dissemination to researchers in interdisciplinary fields, practicing engineers, and common public; publication in journals; and presentation at national and international conferences. This project supported the recruitment, retention, and graduation of multiple researchers that identify as members of underrepresented groups in STEM. We built a long-term relationship with multiple K-12 educational institutions and programs, integrated research outcomes into a new graduate course, and promoted awareness of these technologies through involvement and mentorship of student teams.
Targeted Infusion Project: Infuse Cybermanufacturing Concepts to Manufacturing Processes and Automation Courses
The Historically Black Colleges and Universities Undergraduate Program (HBCU-UP) through Targeted Infusion Projects supports the development, implementation, and study of evidence-based innovative models and approaches for improving the preparation and success of HBCU undergraduate students so that they may pursue science, technology, engineering or mathematics (STEM) graduate programs and/or careers. The project at Virginia State University seeks to strengthen the university's ability to recruit, retain, and graduate underrepresented students in STEM by enhancing the manufacturing engineering curriculum. Undergraduate students are involved in the project as researchers.

This project has the objectives to: enhance the cybermanufacturing engineering curriculum; increase the number of students enrolled in and graduating from the cybermanufacturing program; and improve the placement rates of graduating seniors into STEM graduate programs or the STEM workforce. The evidence-based strategies and activities to achieve these objectives are to: infuse concepts such as digital thread, sensing and acquisition and data analytics into the targeted courses under the framework of cognitive apprenticeship; develop three new lab courses based on digital design and manufacturing of centrifugal pumps for the targeted courses; enable digital thread and digital twin cyberlearning environments at the institution; and establish joint undergraduate research opportunities with Virginia Tech. The project will be guided by an external evaluator, as well as internal and external advisory committees.

This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Through this NSF grant, the research team at Virginia State University (VSU) accomplished the following tasks.
1) Infused the cybermanufacturing (CM) concepts including digital thread, digital twin, sensing and control, and data analytics into the targeted Manufacturing Processes and Automation courses, also enabled the learning environment for CM at VSU.
2) Offered three laboratory courses in MANE 201-Manufacturing Processes Lab, MANE 301-CAD/CAM Lab, MANE 302- Automation Lab. In addition, the PI developed a MANE 499-Manufacturing Data Analytics course.
3) Developed a digital twin for CNC machining using virtual reality, also developed a platform of sensing, monitoring, and analytics for CNC machining.
4) Involved 21 undergraduate students in the research mentored by faculty members. Established a joint research program with Virginia Tech.
Intellectual Merit.
The activities in this project push the boundary of the current manufacturing education knowledge model- ?Four Pillar of Manufacturing? for CM. The research team published two (2) journal papers and four (4) conference papers. The faculty delivered seven (7) oral presentations and four (4) poster presentations on conferences and workshops, student trainees delivered two (2) oral presentations on ERN Conferences. Student trainees in the project won NSF travel awards twice to attend the ERN. Some undergraduate students (J. Diaz, C. Trott, and F. Iskandar) published their first academic conference paper, some (J. Diaz and C. Trott) even won a Fourth Place in AI Tracks at Sea Naval Challenge (https://www.ccals.com/2021/03/16/vsu-team-takes-fourth-place-in-ai-tracks-at-sea-naval-challenge/ ) . Dr. Wu presented "developing virtual CNC milling using Unity software" at the MANEUVER workshop organized by Tennessee Tech. This workshop taught 30 faculty members from 2-year colleges on manufacturing education using virtual resources. Also, we invited Dr. Sheng-Jen Hsieh from Texas A&M University to VSU for a seminar on ?Cyber Physical Systems and Industry 4.0: Perspectives and Future Directions? in Spring 2019. This seminar was open to faculty and students in all STEM majors at VSU. Over 50 attendees participated in the seminar.
The team offered three laboratory courses in MANE 201-Manufacturing Processes Lab, MANE 301-CAD/CAM Lab, MANE 302- Automation Lab, and a lecture course MANE 499-Manufacturing Data Analytics. Twenty-four (24) students were trained in these courses: three (3) in MANE 201, three (3) in MANE 301, four (4) in MANE 302, and fourteen (14) in MANE 499.
The laboratory at VSU has been enhanced greatly for cybermanufacturing. A set of new equipment including sensing and acquisition devices (dynamometer, accelerometers, acoustic emission sensor, and NI DAQ), virtual reality equipment, and a 5-axis machine, has been procured and put in use for different research and education activities. Digital twin tools for CNC machining have been developed for: 1) sensing and monitoring of the machining processes, and 2) cyberlearning tool of CNC machine.
The project expanded the collaboration with Virginia Tech (VT) under the umbrella of "4+1" pathway program, which builds the pathway for VSU engineering bachelor graduates spending one extra year at VT to earn the Master?s degree in Industrial Engineering (IE). The PI secured another two federal grants with the VT collaborator in the area of Smart Manufacturing: one is for the 5-axis CNC machine, the other is IDREAM-4D consortium (https://www.idream4d.org/about-us/team/index.htm ). These grants built new platforms for our collaboration and brought new opportunities for our students.
Broader Impact.
This project identified the educational approach to prepare students from a HBCU Engineering School for advanced manufacturing. Twenty-one (21) students were involved undergraduate research training. Among them, seventeen (17) graduated, three (3) are working towards their degrees. As far as the PI know, thirteen (13) students are working in the STEM industry or enrolled in graduate school. The representing industries employing our students are Aerospace Cooperation, NAVSEA, Boeing, Newport News Shipbuilding, etc. This project increased the number and diversity of STEM workforce by facilitating the underrepresented students into the STEM career.
The grant greatly relieved our students from the impact caused by the COVID-19 pandemic. During the pandemic years of 2020-2022, this project involved 12 VSU students in the research. Students were working in different project activities such as 1) developing digital twin tools, 2) helping laboratory enhancement, and 3) joint design project with VT and the University of Texas at Rio Grande Valley, etc. The students were paid stipends for the work that they contributed in the project. These activities greatly relieved student from financial stress and social isolation caused by the pandemic.
The project also engaged with local communities. Dr. Wu helped local high schools in their robotics teams in 2021 and 2022. We also outreached to Richard Bland College of William and Mary in the 2022 Manufacturing Day event. These activities have enhanced the younger generations? awareness towards STEM career and refurbished their images on advanced manufacturing.
Collaborative Research: Multiple Scale Biomechanics of Tissue Damage in the White Matter of the Human Central Nervous System
The debilitating long-term consequences of traumatic brain injury (TBI) caused by head impacts in sports or blast effects in war are now generally recognized as dangerous. Even milder head impacts require patient monitoring, though for how long is not completely known. To care for people with less obvious injuries and to measure whether a patient has healed, new approaches to brain imaging are needed. This research is targeted at improving the prediction and monitoring of brain injuries. A loss of white matter is one of the changes in brain tissue after injury. How the head impact is distributed as stress in the brain is not known, nor do we know the tolerance of brain cells to stress. This research project will use fast confocal microscopy and Magnetic Resonance Imaging (MRI) techniques to develop and validate a multiscale mechanical model for brain white matter that can predict cell injury. This validated model will be used to estimate axonal injury in the brain using impact models of the whole brain. The research will be used in the education of undergraduate and graduate students. The PI instituted the GirlsConnect club in Mechanical and Aerospace Engineering to increase the participation and retention of women in this field.

This project will focus on identifying the injury thresholds for individual axons within white matter when the tissue is exposed to dynamic, mechanical stretch. This interdisciplinary research builds on the innovative experimental approaches to understand axon kinematics and novel multi-scale computational models of the collaborative team to introduce: (i) Analysis of axon-level displacements and sub-failure axonal damage following clinically-relevant, dynamic loading of white matter; (ii) Computational incorporation of the composite mechanics of axon and glial matrix interaction, including damage, into the global (macro) response of the white matter following tissue-scale loading; (iii) Quantitative prediction of stress concentrations at the axon-level from the computational models; and (iv) Integration of clinical imaging (MRI and Magnetic Resonance Elastography) and subsequent histopathology and correlation of mechanical properties to axonal damage. The multi-scale model will enable the prediction of changes in bulk mechanical properties from damage to the axonal microstructure and from myelin degeneration. It is anticipated that this model can be easily integrated into advanced imaging modalities to predict and monitor brain injury and disease.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
In the US, 95% of the 7 million head injuries reported annually are mild, but mild traumatic brain injury (mTBI) is difficult to diagnose. Diffuse axonal injury is the most significant cause of morbidity following brain trauma. The overarching aim of this research project is to quantitatively understand the multi-scale mechanics associated with traumatic injury to central nervous system (CNS) white matter. This includes evaluating how mechanical forces are transferred to individual cells of neural tissue (axons and glia), and how these components respond separately. This information is vital for localizing the extent and severity of damage, designing appropriate in vitro and vivo models to study mTBI, and rationally developing safety means and measures to prevent injuries. The PI integrated the materials and methods developed during the duration of this project into sophisticated digital twin models used in teaching biomechanics to undergraduates and for popularizing brain biomechanics.
At least two challenges were addressed in the path towards understanding the dynamics of CNS traumatic injury. The first has to do with the disparity of length scales: blunt impact exposes the whole tissue to mechanical excitation but damage occurs at the cellular level. For example, brain imaging using magnetic resonance imaging gives millimeter resolution, but studying individual neuron damage requires resolution at the micrometer length scale. We bridged this gap by incorporating sub-voxel computational models for white matter, consisting of bundles on neurons surrounded by the glial matrix. These models accounts for the effective mechanical anisotropy introduced by the individual axon bundles embedded in the glial matrix, the 3-D morphology and curvature of the axonal bundles, the intrinsic mechanical properties of each phase (axon and glial matrix). Machine learning algorithms were developed to predict the response of multi-axon assemblies on a larger scale, using information obtained by histology or confocal microscopy. The scaled-up models connect the effective (volume averaged) properties of the tissue, to axon volume fraction and to the intrinsic properties of axons and glia, and were used to extract these properties from whole brain experiments. By simulating harmonic shear loading, we demonstrated that the effective transverse shear moduli are higher than the axial moduli, and this was verified by our human brain brain elastography data. The degree of myelination is an important contributor to overall tissue stiffness, since myelin is stiff and it forms the boundary between the axon and the glial phase. We also found that the axon/glia composites are more sensitive to shear loading than tensile loading, and that axon curvature plays a very important role in the overall dynamic behavior of brain tissue. This is corroborated by clinical data found in the literature indicating that neural damage is frequently found at the grey-white matter junction, where projections from cortical neurons often make sharp turns into the sub-cortical white matter, and decussations where axons cross, such as in the brain stem, corpus callosum, corona radiata, etc. 
The second challenge involves studying the dynamic nature of the mechanical response of the tissue. Brain trauma involves rapid acceleration/deceleration of the tissue, but measurements of mechanical properties are typically obtained under static or harmonic loading. TBI inherently involves the mechanical response of brain white matter across multiple temporal scales, and the quantification of this response is inherently difficult because the tissue constituents do not respond in the same manner. On the other hand, the viscoelastic characterization of axon properties that are based on interpretation of results from in vivo brain Magnetic Resonance Elastography (MRE). These results indicate that the shear moduli depend on the actuation frequency used to generate shear waves with which measurements are made. A fractional viscoelastic model of the axons and glial matrix was developed in the time domain to address impulse loading. Given the paucity of human experimental data, this model was applied to interpret shear wave propagation experiments following focused ultrasound pulse excitation of excised porcine brains. A 3-D stereotactic system was built that allows both ultrasound imaging of the whole brain and accurate localization of tissue samples to be excised for further study. The ultrasound component allows both anatomical, and shear wave elastography imaging following a focused ultrasound “push”. The spatial registration allows the extraction of thin tissue slices for oscillating disk rheometry from areas identified by ultrasound imaging, and of sections selected for histological examination (using immunofluorescence microscopy). We are building a database of material parameters for different load cases, axon volume fractions, and incorporate the properties into tissue-scale models, which can be directly compared to further experiments.
SaTC: CORE: Medium: Collaborative: REVELARE: A Hardware-Supported Dynamic Information Flow Tracking Framework for IoT Security and Forensics
Smart and connected devices, also known as Internet of Things (IoT) devices, are now an integral part of our daily lives. These devices are found in cars, phones, watches, appliances, home security systems, and in critical applications, such as utilities and in the biomedical industry. The convenience provided by IoT devices comes with unique security and privacy concerns. Because of the shortened time-to-market and the fierce competition among companies, security has not been treated as a priority in these devices. Very importantly, IoT security challenges are different from those present in conventional devices because IoT devices (i) are heterogeneous, (ii) have limited computational resources, and (iii) can be prevalent in very large numbers. Thus, there is an urgent need to develop standardized, efficient, and embedded security modules to protect such devices from cyber attacks. The goal of this project is to design, implement, and fabricate REVELARE, a security solution for IoT devices, which protects IoT devices in two ways. The first is through a hardware module embedded in the device, which can analyze and filter low-level events based on predefined security policies. The second component resides on a cloud environment and performs forensic analyses on a large set of events continuously recorded from the IoT device. This project has the potential to immensely improve IoT security. Manufacturers will be able to ship IoT devices with built-in protection against cyber attacks. The principal investigators, with complementary expertises in the Computer Science and Engineering fields, have a strong record of advancement of female and minority students, as well as involvement of undergraduate students in research projects. Further, this project opens up new avenues for future work in hardware-for-software security, an area which, while still in its infancy, has the potential for breakthroughs in cyber security.

REVELARE is a hardware-supported dynamic information flow tracking (DIFT) framework to enhance IoT security and forensics. It consists of the following components: (i) a DIFT-enabling core for the ARM and the RISC-V architectures, which complements the main processor with DIFT capabilities, (ii) two DIFT-based security policies (prevention of memory corruption and in-memory-only attacks) enforced by hardware, whose accuracy is enhanced by the capture of DIFT indirect flows, and (iii) a mechanism for IoT virtualization-based security analysis and forensics, with the implementation of two types of security/forensics analyses: causality graphs and personalized (per-device) anomaly detection. REVELARE realizes the potential of DIFT capabilities for the needs of IoT security and forensics, transforming the state-of-the-art for how researchers in academia and industry have been addressing IoT security. Our efficient (architecture-supported) and effective (addressing indirect flows) DIFT framework can also inform future research on architecture-supported DIFT for other architectures (e.g., Intel x86) leveraged in traditional devices. Our combination of in-device built-in protection with cloud heavy-weight analysis and forensics has the potential to ignite the new field of IoT virtualization, in which IoT device management and security are outsourced to the cloud via virtualized devices.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The main research goal of this project was to implement REVELARE - a hardware-supported dynamic information flow tracking (DIFT) framework to enhance IoT security and forensics. REVELARE consists of the following components: (i) a DIFT-enabling IP core for the ARM and the RISC-V architectures, which will complement the main processor with DIFT capabilities, (ii) two DIFT-based security policies (prevention of memory corruption and in-memory-only attacks) enforced by hardware whose accuracy is enhanced by the capture of DIFT indirect flows, and (iii) a mechanism for IoT virtualization-based security analysis and forensics, with the implementation of two types of security/forensics analysis: causality graphs, and personalized (per-device) anomaly detection using deep learning-based approach on both the device level and the IoT-virtualization platform level.We found that DIFT systems can vary many key attributes to implement taint tracking functionality, such as tag size, format, and granularity. The greatest advantage of DIFT implemented in software (SW-DIFT) is the flexibility in implementation. Implementation is accomplished by either specialized compilers or via binary instrumentation. The software and hardware collaboration to support DIFT has been accomplished in two ways: hardware-supported software and software-supported hardware.For dynamic information flow tracking, our implementation was designed on implementation on a RISC-V core. We used the real-world application and customized vulnerable application to demonstrate the effectiveness of the proposed framework in protecting computing systems.Our implementation of the anomaly detection models was designed to be applicable for both the IoT architecture limitations and the digital twin platform. The lite-weight architecture allows to accelerate the anomaly detection time, which is a crucial component in guarding any computing system, including but not limited to the IoT. The models have a significant reduction in trainable budget components compared to the existing state-of-the-art deep learning models that target anomaly detection for the IoT.
Integrated Power Grid and Microgrids with Massively Distributed Intelligent Sensors
Improving the reliability of the electric power grid is one of the critical challenges facing the nation. To meet this challenge, we believe that the Intelligent Sensors and Analyzers advanced technology proposed here can begin to transform our power grid and its affiliated microgrids to a Next-Generation-Smart-Grid. It could also enhance our power and energy sustainability and restorability in severely adverse weather conditions. It has the potential of starting a new revolution in creating intelligent Power Grid and Microgrids by extending todays microchip technology to integrated multi-sensing and information processing in a single compact device -- specialized for Power and Energy arena. Advanced sensor models integrated with Power Grid and Microgrids models will be developed for achieving the true potential of the intelligent sensors and analyzers (proposed herein) that would be deployable on a massive scale. The project will also promote teaching, training and learning in this key technology area. Specifically, the sensors, process-flow, and specialized microchip design activity of the project will be imported into the undergraduate senior-design projects. The project will also significantly benefit several graduate courses including a new course on Smart Power Grids, and existing courses on Micro-Electro-Mechanical-Systems II, and System on a Chip. The advances will be shared with the power and energy industry, government, and academia through a dedicated web site and, of course, through scientific publications.

As stated above, the goal of this project is to improve the reliability and restoration capability of wide area Power Grid as well as its affiliated Microgrids thereby reducing power outages and their cascade effects. Our solution is to embed novel intelligent sensors with distributed processing tools to provide real time monitoring and control of "Power Grid as well as its affiliated Microgrids" from transmission and distribution levels. The problems associated with the present lack of a real time intelligent infrastructure to monitor and control Power Grids -- on a massively distributed basis, have resulted in numerous blackouts and other serious inefficiencies. Our sensors/analyzers will provide accurate information to protective relays, enable intelligent islanding, a means by which the modeling of loads can be based upon accurate measured data rather than estimated data, for load shedding plans. The unique sensor architecture, fabrication, and signal processing issues will be addressed through an advanced Heterogeneous Sensor System on a Chip. As just stated, included will be Microgrids, in which the focus will be on islanding and restoration, sensors, estimation of key variables (frequency, phase and amplitude -- for harmonics as well when desired), defect and fault tolerance, thereby reliability improvement. Microgrids incorporate renewable energy sources, among others. System on a Chip based sensors can provide effective, real-time monitoring and control that is self-healing, for such sources. In short, the main objectives are the following: (1) Multi-Sensor (including Micro-Electro-Mechanical-Systems) design, fab, and test, (2) Intelligence: analog and digital sections design, fab, test (for "Power Grid and Microgrids"), (3) Power Grid and Microgrids objectives: Fault detection, islanding, active & reactive power estimation, theoretical models & simulations, (4) Final Heterogeneous System on a Chip design, fabrication, tests; Also, tests in our Smart Grid Lab.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The goal of this project is to improve the reliability of power grids through novel intelligent sensors and system-on-a-chip (SoC)-based real-time monitoring and control devices. Our sensors and SoC will provide accurate information, enable real-time system condition estimation, and enable the digital twin technology to monitor and predict power grid stability at real time. Furthermore, real-time mitigation to enhance grid stability has also been developed.  The project has two distinct outcomes in designing applications by use of smart sensors and real-time data stream and sensor fabrication. 
First, a power plant digital twin has been developed by use of software-defined reconfigurable SoC. The state-of-the-art power system operation is limited to steady-state operation, e.g., generator dispatch, due to the low sampling rate of its data acquisition systems. To be able to predict dynamic stability upon a contingency, it is necessary to have a very different set of tools with the capability of fast and accurate sensing, real-time data streaming, and providing digital replica for dynamic stability evaluation. To this end, this project develops a digital twin for an inverter-based resource power plant that can acquire real-time measurements, calibrate a digital model's parameter based on real-time data streams, and predict stability based on such a model. The digital twin has been developed in NI cRIO-9063 FPGA/DSP chip and its functionalities of real-time monitoring and stability prediction have been fully tested.
The second distinct outcome is smart miniaturized sensors (MEMS), which can tell if there's a problem in the big power lines without needing people to report a power cut. Such sensors do not require contact to the electrical lines and hence require no current transformer or voltage transformer. So far, we've sketched out a bunch of different blueprints for our new sensor gadget. This trick might let us use the gadget in two ways: as a diving board for electricity (that's the two-port resonator) and as a detector for the magnetic 'wind' (the Lorentz force magnetometer). We're also working on making special models for different kinds of tests. Wrapping up, we've been like gadget inventors, tinkering with different designs for tiny devices that can feel the 'push' caused by magnetic 'winds' – these are our special tiny sensors. We made these sensors in a super clean room equipped with semiconductor foundry fabrication tools at the Nanotechnology Research and Education Center at USF. When we tested our tiny creations with some special equipment, they showed us they could do the job pretty well with performance on par with those of prior works. But we're not stopping here. We've got a bunch of designs and tests lined up to see just how good we can make these sensors. That's what we're doing, but with sensors for our smart power grids.

Planning Grant: Engineering Research Center for Computing Yourself to be Better - Engineering for Revolutionizing Medical Decision-making (CYBER-MD)
The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program. Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.

Improving the quality and efficiency of healthcare - at home, in long-term care facilities, and in hospitals - is becoming ever more important for patients and the society at large. Although the two current trends in healthcare - telemedicine and digital medicine - are making headway, they only scratch the surface of the problem as opposed to dealing with the root cause. To revolutionize healthcare delivery, we need a comprehensive biophysical patient model, namely a "digital twin", that fuses data streams, doctor notes/assessments, and medical histories with a rigorous computational engine that can make probabilistic predictions of future patient health/well-being. Healthcare providers will then be able to integrate traditional knowledge/training with the "data-to-decision" posture of the digital twin, while working with the patient, to best assess their health, evaluate options, predict future health, and optimize patient outcomes and prognosis. The aim of this Planning Grant is to engage and converge a broad stakeholder community of engineers, data scientists, educators, artists, medical specialists, hospitals, consulting firms, digital health firms, and health and government agencies to openly discuss and identify the most urgent engineering research, education, and outreach gaps that need to be addressed for revolutionizing medical decision-making and personalized medicine. Once the ERC is established, the team will partner closely with stakeholders to identify current issues in medical care, design and build engineering solutions, and deploy these solutions in phases after careful assessment of their effectiveness. It is through these activities that truly personalized medicine will be achieved, where cyber-physical healthcare monitoring can be tailored to specific patients as they are identified early at risk versus when severe complications unfold. Furthermore, the education and outreach initiatives of the CYBER-MD ERC will actively recruit women, underrepresented, and economically disadvantaged individuals for research and training future generations of multidisciplinary, convergent scientists and engineers.

Healthcare practice today only provides clinicians with limited amounts of historic patient data, and doctors tend to rely on outdated training and anecdotal experiences to best care for their patients. However, given the broad overlap in symptomatology across various diseases, missed diagnosis and misdiagnosis remains a major problem. The vision of the CYBRE-MD ERC is to establish a cyber-physical innovation hub, where engineering breakthroughs in bio-physical modeling, imaging, sensing, data science, data visualization, and probabilistic risk assessment methodologies collectively revolutionize the medical diagnostic/treatment process for improving patient outcomes. The cornerstone of this concept is a "digital twin" that is born and then evolves with the patient while constantly being updated over time with new data streams ranging from biometrics to illness/injury assessments to diagnostic images/results to subsequent treatments. The digital twin not only presents a new data architecture for storing patient health informatics, but it also provides transformative health assessment and predictive analytic capabilities that enable doctors and patients to work together to improve patient health, well-being, and outcomes. CYBER-MD is inherently transdisciplinary and spans the entire spectrum of science, technology, engineering, arts, and mathematics (STEAM) as well as all branches of medicine. This Planning Grant will convene diverse experts to converge on a final strategic implementation, define focused research grand challenges, identify relevant thematic areas, communicate with stakeholders, educate, devise innovative and broadly implementable education/outreach efforts, and assemble the most competent and diverse team that underpins this transdisciplinary NSF ERC. The activities planned directly support convergent research, which is of utmost necessity for tackling this overarching grand challenge that spans the fields of STEAM, medicine, healthcare, and public health/well-being.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The medical field asks a risk assessment “triplet” of questions in patient care: (1) What can go wrong? (2) What are the likelihoods that these things can go wrong? (3) What are the consequences and associated time scales? Healthcare practice today only provides clinicians with limited amounts of historic patient data, and doctors use their training and past experiences to best care for their patients. However, many symptoms are shared among various diseases, and misdiagnosis (or even not catching a serious illness in time) remains a major problem for society. The vision of this proposed Engineering Research Center (ERC) is to revolutionize patient care and patient outcomes by equipping doctors and patients with a Digital Twin and decision framework that can respond to the triplet of questions and convert relevant data to actionable information. Thus, the goal of this Planning Grant was to convene experts and to articulate the research grand challenges and workforce development hurdles that need to be overcome for realizing this Center vision and how to derive such a personalized Digital Twin for individuals and patients. The cornerstone of this concept is that the Digital Twin is born and then evolves with the patient while constantly being updated over time with new data streams. The Digital Twin is not only a new data architecture for storing patient health informatics, but it also provides transformative health assessment and predictive analytic capabilities that enable doctors and patients to work together to improve patient health, well-being, and outcomes. This Planning Grant directly supported an innovative, transdisciplinary, and community-driven workshop (and meetings) to form the core Center team and to sculpt the vision, main thematic research domains, education and outreach initiatives, and overall structure of the proposed ERC.
Collaborative Research: Learning and Optimizing Power Systems: A Geometric Approach
The transformations of the electrical grid present a plethora of challenges to system operators and utilities. They must adapt to manage a set of highly uncertain and distributed resources such as electric vehicles and solar PVs, while at the same time operating a grid infrastructure that was designed decades ago. These challenges are particularly acute in the distribution system, where the networks are traditionally not monitored closely, and operators lack the essential information to obtain an accurate real-time operational state of the system. At the same time, the number of outages in distribution systems has started to increase as the system ages, and the loads become more dynamic. The goal of this proposal is to overcome these challenges by developing novel algorithms and new insights that increase the efficiency and resilience of the distribution systems. Educational activities would be developed around these research thrusts to ensure diverse student participation and outreach to the broader community.

The project focuses on three thrusts: i) system topology estimation using the wealth of data made available by smart meters and other sensors, where the network may contain loops and the data may be highly heterogeneous; ii) characterization of the feasibility of operating points using a new geometric understanding of power flow that leads to provably efficient and optimal algorithms; and iii) restoration of service right after outages through line switching by using the results from the first two thrusts. These investigations bring in tools from power system analysis, optimization, and statistical learning to enable fundamental advances in the distribution system operations. In particular, these thrusts allow us to leverage recent advances in both technology and theory to develop timely and rigorous algorithms that solve some pressing engineering problems for the power grids. Successful application of our proposed project will allow distribution system operators to answer various "what now" and "what if" questions deriving from those highly volatile grids with large amounts of distributed resources.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The project was dedicated to the development of algorithms aimed at acquiring the system states of distribution systems. These algorithms play a crucial role in enhancing our understanding of how geometric analysis contributes to the feature engineering process within power system utilities. Additionally, they illustrate the construction of a digital twin for power systems, utilizing both physical system information and discriminative learning, leading to better system knowledge extraction. This integration facilitates the creation of grid analytical tools akin to those utilized in the transmission grid. The implicit inclusion of system information in historical data enables the feasibility of grid utility, thereby significantly enhancing monitoring and control capabilities at a more refined data resolution. In response to the growing presence of distributed renewable energy, our proposed algorithm offers timely and scalable tools for sustainable operation. The project has resulted in numerous publications in the field of data-driven analysis and theoretically robust machine learning methods for power systems. The funding obtained from this project has played a pivotal role in training multiple Ph.D. students who are emerging as next-generation power engineers. Moreover, these students have actively contributed to the implementation of the algorithms in the private sector with system operators, extending the project's impact beyond academia.

Cyberlearning: Sensei: High-Fidelity, Non-Invasive Classroom Sensing for Professional Development
For years, research has shown that moving away from large lectures and increasing student engagement and participation in classrooms significantly improves learning. Unfortunately, professors lack quality professional development opportunities to improve their instruction, and typically receive no training on how to teach. This project is addressing the issue of college professors' professional development through a cyberlearning innovation called Sensei. Sensei has novel capabilities using sensors to capture, isolate, and analyze voice and video that will provide near real time data on classroom interactions such as the percent time students talk vs professors, the percent time students talk to students, student engagement through facial expression analysis, turn taking between students and professors, etc all of which involve multimodal analysis of voice and video. The second component of this research is the development of suggested actions to improve the professor's performance as a teacher.

More precisely, Sensei draws on technical and socio-technical advances in sensing arrays, computer vision, intelligent environments, and personal informatics, as well as frameworks of professional development in higher education. In this project the researchers will 1) develop the technologies needed to automatically sense and display feedback to instructors, 2) deploy this system in-vivo to college instructors over semesters of use in a series of design-based research studies, and interpret the results to 3) iterate on our framework for the routine incorporation of classroom data into professional development. This research is enabled by a cyber innovation in which computing is expanded by the capabilities of state of the art multimodal sensing approaches to achieve non-invasive sensing at classroom-scale. This cyber innovation drives a learning innovation of delivering near-real-time data on teaching practices in a combined reflection and training system by delivering rapid and frequent feedback and instruction on good strategies in manageable instructional units, that support a focus on student-centered beliefs. In turn, the learning innovation advances understanding of how instructors learn in technology-rich learning environments by exploring mechanisms in a framework of professional development that would not be possible without this new cyberlearning genre. In particular, through a series of design-based research studies with instructors teaching STEM college courses, the researchers explore ways in which Sensei a) can trigger critical self reflection, b) how this self-reflection changes based on the features of the data viewed, c) how datadriven goal-setting can foster self-efficacy in teaching, and d) how these effects vary over time. All of the code will be developed as open source and, if successful, Sensei could be generalized to include K-12 teachers.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
For years, research has shown that moving away from traditional lecture format to methods like active learning that increase student engagement and participation in classrooms significantly improves learning and broader learning outcomes, such as increasing the likelihood of choosing a technical major. However, most colleges still rely on lectures where students passively receive information from a professor. Students thus gain access to critical domain knowledge, but are often disengaged from the process of learning, leading to lower rates of success and lower long-term retention in STEM fields. On the other side, professors lack quality professional development opportunities to improve their instruction, which contributes to this major societal problem. Professors are hired and promoted for their domain expertise, and typically view themselves as domain experts and not teaching experts. Unlike K-12 teachers, college faculty typically receive no training on how to teach and instead “learn” how to teach while pursuing an advanced degree, often developing inaccurate views of teaching and learning and continuing the lecture model practiced by their own professors.
In this project we developed a new cyberlearning innovation, EduSense (Ahuja and Kim et al, 2019), supported by novel technical capabilities. Our system addresses the above challenges by automatically collecting, processing and displaying classroom data to provide feedback to college instructors that helps shift behaviors and related beliefs in a distributed and time-efficient way. It consists of a connected set of socio-technical systems that support instructors in acquiring scientifically-supported, student-centered teaching practices through reflection on feedback using near-real-time classroom sensed data.
Almost all prior classroom sensing systems have limited practical real-world utility since they are either are single-classroom scale (e.g., requiring a server per classroom), are not comprehensive in terms of the sensing facets they can provide, or do not present a scalable system architecture that could approach campus-scale deployments – a key goal of this grant. The value in our system is in putting together disparate advances from several fields into a novel, comprehensive and scalable classroom-focused system, paired with a holistic evaluation combining both controlled studies and months-long, real-world deployments. 
Our work was cited in a recent review paper on classroom sensing (Foster et al, 2024) as a prominent classroom sensing system in the discipline, noting that “To our knowledge, Edusense is the first sensing tool used in real-life classroom environments to unify the detection of all these features into a single system.” To do so, we equipped multiple classrooms (38) at our institution with an infrastructure-level set of sensors (cameras for video and audio recording) connected to EduSense. This provides our university an unprecedented testbed to study classroom sensing technologies and their impact.  
EduSense collects behavioral data using this dual camera setup, and utilizes computer vision and machine learning based methods to inform classroom behaviors at a fine granularity, consisting of information such as instructor and student location, gaze, and ambient audio. Our second system, ClassroomDigitalTwins (Ahuja et al, 2021), introduces 3D gaze estimation for better estimation of attention from instructors and students. It further utilizes this to build a digital twin of processed classroom sessions, which can be used by instructors to revisit moments from their teaching, and use it for further improvements. 
While Edusense and ClassroomDigitalTwins provides a way to capture data on teacher-student behaviors and interactions at a scale, the output is very high fidelity, and is too complex to be used directly by instructors or pedagogy researchers. We observed that other classroom sensing systems also face similar challenges, and translating sensing data into meaningful insights often requires custom analyses, thus making them non-scalable. To address these issues, we built Edulyze, an analytics engine that processes complex, multi-modal sensor data and translates it into a set of analytics that can provide meaningful insights to different stakeholders. Edulyze can combine sensing data from a variety of classroom sensing systems, and translates it into a unified schema regardless of the underlying sensing system or classroom configuration. Analytics from Edulyze can train machine learning models to predict activities happening in the classroom, based on standardized classroom observation protocols that can support the professional development of instructors to help them improve their teaching. 
As classroom sensing systems impact both instructors and students, the student perspective is also critical for the development and implementation of these systems. We conducted interviews with undergraduate students and presented them with possible scenarios of classroom sensing systems to understand their perceptions and views. We found that students had nuanced views of classroom sensing technologies related to how they impacted student autonomy, connections with instructors, and their thoughts towards the role of technology in the classroom. Our publication of these findings won a paper award at ACM Designing Interactive Systems Conference.
RI: Small: Computational and Physiological Studies of Complex Neural Codes in the Early Visual Cortex
In this interdisciplinary project, machine learning approaches are coupled with neurophysiological studies of primate early visual cortex to investigate the functional, coding and computational benefits of the observed neural representation and computing architecture. Neural models, with recurrent connections and the proposed dual-code strategy, will be developed to solve multiple vision problems simultaneously and to fit neurophysiological data. The representations will be studied from both coding perspectives and computational perspectives, based on scene statistics and their relevance for solving vision problems. The research program will be facilitated by international collaboration and tightly integrated with undergraduate and graduate education in neural computation. The proposed project wide provide new insights to the computations and functions of the biological visual system, as well as new ideas and inspirations for developing machine learning systems that can learn from limited data and function robustly and flexibly in novel complex situations, potentially with broad societal and technological impact.

Current deep learning neural networks utilize tens or hundreds of layers to learn solutions for specific computer vision problems. The mammalian visual system has much fewer layers, and yet can solve many tasks in a variety of novel and complex situations. The nervous system might achieve this feat by having neuronal circuits with loops and recurrent connections, and with order of magnitude more neurons in each "layer." Recent neurophysiological findings suggest that neurons in the primary visual cortex (V1) of primates are not simply oriented edge and bar detectors as described in textbooks, but respond strongly to highly specific complex local patterns, although they also respond to many other patterns with much weaker responses. The PI proposed that the individual neurons are not amorphous entities, functioning facelessly in a large population, but are distinct and unique individuals that serve as specialists for some specific tasks and as generalists in other tasks. They participate in population encoding of information with strong sparse codes or weak distributed codes respectively, depending on the functional roles they serve.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
In this project, we studied how visual concepts are represented in primate brains using deep learning techniques and explored the computational advantages of these representational strategies in computer vision. In neurophysiological studies based on single unit recording and 2-photon calcium imaging in the early visual areas (V1, V2, and V4) of non-human primates, we found that individual neurons use strong responses to represent specific visual prototype patterns, such as curves, corners, and rings, that are more complex than simply the oriented edges as commonly assumed. We used deep neural networks to model and create a digital twin for each neuron. Our investigation of these digital twins revealed that a significant proportion of the complex pattern selectivity in V1 arises from local recurrent neural circuits. We studied the neural representation of visual prototype concepts rendered realistically or in abstraction, specifically in photographs, cartoons, and line drawings. We found that visual concepts rendered in different cues share similar geometric structures in neural population activities, and these geometric structures are stable across different populations of neurons and across different visual areas. As information processing progresses downstream to V4. The visual concepts represented become increasingly elaborated. Neurons representing a variety of concepts are organized in a topological map with distinct domains tuned to a variety of natural scene features, some tuned for surface properties of objects, while others tuned for object shapes. These neurophysiological results suggest that neurons in the early visual cortex can encode prototype concepts with strong responses and serve as a distributed code with weaker responses. We investigated the implications of these neuroscience findings in machine learning deep neural network systems by dissecting the information encoded in their units. We found that the strong responses in the units tend to encode foreground structural shape information, while weak responses tend to encode texture and background information. By preserving the strong responses and suppressing the weak responses during neural network training, we can induce the neurons to explicitly encode local visual concepts of shapes.  We explored the use of these prototype codes in deep learning networks for image generation and image analysis. We found that neural networks trained with prototype codes for object recognition exhibit stronger shape bias, a human perceptual trait lacking in convolutional neural networks. In generative AI systems, these prototype codes enable neural networks to learn with fewer examples to synthesize images with more structurally correct objects with coherent parts and overall shapes.  This project integrated neuroscience and artificial intelligence with broad implications in both fields. It leveraged machine learning techniques to understand neural codes for visual representation, addressing fundamental problems in neuroscience and psychology. The insights from the neuroscience findings, in turn, help advance deep learning approaches.  The observation that the neurons might serve as prototype codes challenged provided impetus for developing a new generation of deep networks that can reason more symbolically and conceptually like humans. The findings of this project were disseminated in competitive peer-reviewed conferences and journals. They are also incorporated in courses taught at Carnegie Mellon, lectures delivered in conference workshops, and national summer programs for college and high school students. These efforts promoted public awareness of computational neuroscience and artificial intelligence and the dissemination of scientific knowledge in industry and in the scientific community. 
Smart and Connected Communities- Perspectives for Border Communities
The U.S.-Mexico border region is home to more than 80 million people. Binational cooperation, especially on issues of shared importance, such as transportation, commerce, and the environment, is vital to ensuring economic prosperity and environmental sustainability in both countries. Emerging technologies and other innovations can offer smart solutions that have the potential to address many of the challenges in border communities including infrastructure resilience, and food, energy, and water security. Never has the need been greater for academic institutions, border communities, private industry, non-governmental, and government agencies to work together to identify common technology, water, energy, environmental, and security challenges, and to explore and test smart utility infrastructure solutions along both sides of the border. This project brings together four public universities in California, Arizona, New Mexico, and Texas in collaboration with four universities in Mexico to conduct a series of three workshops to foster collaboration amongst academic institutions, government agencies, nonprofit organizations, and industry partners on both sides of the U.S.-Mexico border to pursue fundamental research, the results of which can benefit communities and economies in both countries.

The workshops will catalyze the ideas, partnerships, and resources needed to foster collaborative, binational research, and open access to data that will inform solutions to regional U.S. and Mexico challenges that take advantage of smart technology. Specifically, the proposed workshops will support the initiation and advancement of convergent, interdisciplinary research relevant to the development of innovative solutions to the most pressing border-region challenges: water-energy security, economic opportunity, education, security, immigration, and crime. Additionally, the workshops and the research activities that flow from them will promote Science, Technology, Engineering and Mathematics (STEM) workforce development on both sides of the border. Ultimately, the research will contribute to improving quality of life for those who live in the border region by potentially lowering energy costs, reducing food scarcity, improving border safety and commerce, and offering better healthcare access to underserved communities. Application of the data and research results of anticipated Alliance projects is anticipated to extend far beyond border communities to other regions with similar infrastructural, environmental, and social concerns.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Home to more than 80 million people in four USA states and five states of Mexico, and with population as large as 13 million in the immediate vicinity of the border, the US-Mexico border region serves as a conduit and source of commerce, tourism, and student exchange that is vital to both countries. Challenges in transportation, infrastructure, and the environment make the border region a living laboratory and call for research advancements using innovative tools from computer science and engineering to social sciences.?
University of California San Diego, in partnership with University of Arizona, University of Texas El Paso, New Mexico State University, and University of Texas San Antonio, along with leading Mexican academic institutions just south of the border and government agencies, nonprofit organizations, and industry partners on both sides of the U.S.-Mexican border, have conducted a series of three workshops to identify fundamental research needs and opportunities, the results of which are intended to benefit communities and economies in both countries.?
The first two workshops, held in June 2019, to formulate the research themes, consisted of two days of activities in San Diego, California, one day in Tijuana, three days of activities in El Paso, Texas, and one day in Las Cruces, New Mexico. The workshops included presentations, panels, round tables, and working groups in breakout sessions, whose recommendations were presented in a plenary format on the last day of each of the two workshops.?
The third and final workshop was held in February 2020, just before the outbreak of COVID-19, at the National Academies in Washington, DC. This workshop included addresses from the National Science Foundation CISE directorate, three "fireside chat" panels (on the topics of production, well-being of people, and health of the planet), a keynote on the future of work at the US-Mexico border, a panel of government and private partners (which included representatives from the NIH and DHS, as well as from Mexico's NSF equivalent, CONACYT), and a closing roundtable of the vice presidents for research from the Border Solutions Alliance universities.?
Over 200 people took part in the June 2019 workshops and at least another 100 people at the convocation workshop in Washington in February 2020. A large share of the participants were members of underrepresented minority groups and junior researchers.?
The discussions at the workshops crystalized the research needs and opportunities into six themes: 1. intelligent border mobility systems; 2. advanced manufacturing and cross-border supply chains; 3. water resources (watersheds like Rio Grande/Bravo/Conchos, water decontamination and desalinization, transboundary aquifers, urban green infrastructure networks); 4. urban and rural resilience; 5. emergency and disaster management; and 6. health (drug resistance as a phenomenon amplified by health system asymmetry and the population-borne enhanced circulation of pathogens, chronic disease in border regions).?
A cross-cutting idea, which emerged as the unifier of all of the themes, was the idea of a ?digital twin,? namely, the idea that data-based quantitative modeling of the socio-technical-physical systems would be of value to the researcher, to the government policy maker, and to the commercial decision maker.?
The information gathered in the workshop reports are anticipated to be of benefit to the CISE directorate, and to the NSF as a whole, in shaping future initiatives around the border research questions and research initiatives in the realm of data science. A version of a digital twin is WIFIRE, an evolving platform providing fire commanders with the ability to better understand and examine the behavior of a wildfire in real time to better allocate response resources and save lives.??Built with an array of networks, cameras and sensors on a foundation of fire science and modeling, plus rich data on fire behavior, topology, and fuel load, it also utilizes real-time input from sensor-carrying aircraft overflying the fires to accurately model and predict fire behavior.??
Societal impact and the betterment of the border communities, through science and technology, is at the heart of the Border Solutions Alliance. The communities to derive potential benefits are extremely broad and diverse, including academic institutions on both sides of the border, members of the municipal governments and services, representatives of the private sector and industry, and, besides the NSF, other federal agencies and foundations in the United States. The advancement of the research relating to border challenges and opportunities will lead to accelerated economic development and improve the quality of life of people in the border communities.?
CPS: Medium: Collaborative Research: Building Information, Inhabitant, Interaction and Intelligent Integrated Modeling (BI5M)
Each year the nation spends over $400 billion to power, heat and cool its buildings. Moreover, buildings are a major source of environmental emissions. As a result, even a modest improvement in energy efficiency of the nation's building stock would result in substantial economic and environmental benefits. In this project, the focus is on improving energy efficiency in commercial buildings because this sector represents a substantial portion of the energy usage and costs within the overall building sector. Enhancing the energy efficiency of commercial buildings is a challenging problem, due to the fact that centralized building systems -- such as heating, ventilation and air conditioning (HVAC), or lighting -- must be synthesized and integrated with individual inhabitant behavior and energy consumption patterns. This project aims to design, analyze, and test a cyber-physical and human-in-the-loop enabled control system that can drive sustained energy savings in commercial buildings. It brings together expertise in computational building science, eco-feedback, network theory, data science, and control systems to integrate physical building information and inhabitants with cyber (building-human) interaction models to enable intelligent control of commercial building systems. Specifically, this project will: 1) design an integrated cyber-physical system (CPS), called Building Information, Inhabitant, Interaction, Intelligent Integrated Modeling (BI5M), aimed at reducing energy usage in buildings; 2) assess the complex inter-relationships between and across physical building and inhabitant models, cyber building-human interaction and intelligent control models related to energy conservation behavior; and 3) empirically test and validate modules and the overall BI5M system at test-bed buildings on Stanford's campus and Google's office park.

This research incorporates measurement (geospatial building data, energy use data), dynamics (inhabitant social networks), and control (enhanced user control of: plug-load devices, HVAC, lighting) into the BI5M system. The BI5M system is centered on a cyber Building Information Management (BIM) model of the building, and will encompass rigorous systems engineering that will explore relationships across the cyber-physical domains and develop new insights for how the scientific principles of cyber-physical systems can be used to influence the energy efficiency of commercial buildings through both occupant behavior and intelligent control. By integrating physical building information and inhabitants with cyber interaction modeling, the research aims to introduce an integrated human-in-the-loop control paradigm for commercial buildings. In addition to a testbed and validated CPS system for commercial buildings (BI5M), this project targets fundamental knowledge on: ontological components required to integrate dynamic data streams and control information into static building models; complex socio-spatial structures of inhabitants; insights into how building-human and human-human interactions impact inhabitant consumption behavior; and new control models that leverage input on the energy usage, spatial, social and behavior dynamics of inhabitants. The educational impacts of this project will extend to participants (students, faculty, Google employees in the test-bed buildings), as well as a broader student population through the integration of key insights from this work into courses/projects at all three collaborating universities (Stanford, Georgia Tech, and Columbia). The project team will also disseminate results to practitioners/policy-makers working in the building management space through an Outreach Workshop. Additionally, this project will broaden participation in computing fields through a diverse team and by partnering with the Girls Who Code nonprofit to integrate project data sets and tools into their activities.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The Building Information, Inhabitant, Interaction and Intelligent Integrated Modeling (BI5M) project was a multi-year project supported by the Cyber Physicals Systems (CPS) program.  Over the course of the project, the team:
1.Established experimental test-beds at each of the project sites, including Stanford University, Columbia University, and Georgia Tech. These test-beds facilitated the collection of high-fidelity plug-load energy usage data.
2.Developed novel integrated cyber-physical frameworks, models, and methods to structure and analyze spatial, energy, social, and behavioral data.
3.Conducted experiments to investigate and comprehend the influence of various feedback dimensions, such as energy, spatial, and social factors, on occupant behavior, interactions with building systems, and subsequent energy usage.
4.Created and tested a human-in-the-loop smart building system that incorporates decision-making utilizing spatial, energy, social, and behavioral data.
5.Formed a collaborative partnership with IIT-Bombay to explore smart buildings in tropical environments as part of the Indo-US DCL supplement.
Key outcomes at the Columbia University site include the deployment of a living testbed across two levels of Columbia University's Northwest Corner Building that tracks HVAC and lighting consumption, ambient conditions, air quality, and resident thermal comfort. A personal comfort-level estimation subsystem, leveraging thermal cameras, analyzes facial temperatures to gauge comfort levels significantly outperforming standard methods. Two months of historical data were used to develop a simulation environment of the building, which assisted in training of a novel deep reinforcement learning network to learn energy, comfort, and air quality effects of different actions in the building. Results of this work were published in ACM TOSN ‘21, CPHS ‘21, and ACM/IEEE IPSN ‘21. 
The Columbia team developed a data-driven system for estimating personal energy footprints at the city-scale. Leveraging urban sensing infrastructure and data sources, it offers building-level energy and population estimates. The system encompasses an energy and occupancy digital twin, a city-scale energy footprinting system ensuring complete coverage, and a deployment in New York City that uses local data for model development and evaluation. It also includes tools and applications for both citizens and city planners. This work represents a significant step towards comprehensive city-wide energy footprinting. Results were published at ACM BuildSys '19 and received Best Paper Runner-Up Award. 
At the Manhattan site, a realistic EV-driving-charging dataset was created, integrating real-world data on EV driving behaviors and EV chargers. Data from three EV drivers was collected over three months, alongside information from public sources, location-based data companies, and government authorities. A data augmentation scheme was developed to expand this collection, which has been made publicly available for research. Additionally, a Deep-Q-Network-based deep reinforcement learning framework for EV charging recommendations was developed, treating EV chargers as agents and EV behaviors as the environment, aiming to improve both driver experience and grid capacity. This approach marks a significant advancement in the co-optimization of EV driver welfare and grid efficiency.
We spend over $400 billion annually to power, heat and cool our buildings making buildings the largest producer of environmental emissions in the United States. Despite this massive environmental and financial expenditure, our buildings still fail to deliver adequate levels of energy performance (i.e., meeting occupant needs while consuming minimal energy), with even so called “high- performance” buildings receiving low scores for comfort and satisfaction.  The new integrated cyber-physical and human-in-the-loop building management system (BI5M) developed as part of this project will have a significant impact on society by enhancing occupant comfort/productivity, reducing energy costs and minimizing our environmental impact. 
Smart City Digital Twin Convergence Conference
Cities around the world are increasingly integrating socio-technical advancements to transform their urban systems toward smart cities. As the world continues to urbanize, convergent science, engineering and policy for urban systems is needed to ensure such transformative developments are, in fact, creating sustainable urban systems that benefit the citizens and societies at large. This requires both: (1) convergence of a variety of involved academic disciplines and practitioner expertise from city stakeholders; and (2) convergence of cyber-enabled technology systems at the human-infrastructure interface, creating a new system that can be referred to as a Smart City Digital Twin. With a vision of advancing understanding, development, and application of Smart City Digital Twins, this Smart City Digital Twin Convergence Conference aims to bring together experts from academia, industry, municipalities, and nonprofit organizations from large metropolitan areas in the United States to develop a convergent socio-technological framework for delivering smarter services through Smart City Digital Twins. The proposed Convergence Conference on Smart City Digital Twins also intends to create a community of thought leaders in this emerging area of inquiry.

This convergence conference will explore basic research occurring at the intersection of infrastructure systems, human systems, and technology systems. Smart City Digital Twins represent a new critical infrastructure transforming city operations and management. Existing knowledge on the requirements for single infrastructure Smart City Digital Twins will be shared as emerging testbeds in the areas of energy, built environment, water, and transportation are under development at Georgia Tech, Stanford University, Columbia University and the University of Illinois (Chicago). This sharing of knowledge will provide fundamental insights on multi-infrastructure interdependencies, as well as how human- infrastructure interactions can be sensed, analyzed, controlled, and visualized using cyber-infrastructure enabled technology such as Internet of Things (IoT). New knowledge will be created by engagement of interdisciplinary academic experts, industry practitioners, and government officials in workshop-style facilitated discussions to (1) develop a framework for comparing Smart City Digital Twin efforts and their stages of evolution and (2) chart a roadmap for future Smart City Digital Twin efforts that advance urban sustainability, resilience, and social well-being. Taken together, these convergence conference activities will provide a critical coalescing force as the new discipline of Smart City Digital Twins emerges. Smart City Digital Twinning efforts have the potential to transform the livability, sustainability, and resilience of cities, creating new business opportunities for companies of all sizes, new forms of citizen engagement by communities, creative forms of pedagogical practices, and new approaches to city operations and management by governments.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Cities globally are undergoing a profound transformation, integrating socio-technical advancements to evolve into smart cities. As urbanization accelerates, the demand for convergent science, engineering, and policy becomes imperative to ensure the creation of sustainable urban systems. The Smart City Digital Twin (SCDT) Convergence Conference emerged as a response to this need, aiming to bring together experts from academia, industry, municipalities, and nonprofit organizations across major U.S. metropolitan areas. The primary objective was to create a convergent socio-technological framework for the implementation of Smart City Digital Twins, thus advancing the understanding, development, and application of this transformative concept. The conference sought to bring together diverse disciplines and cyber-enabled technologies to foster a community of thought leaders in this emerging field.
To achieve the project's major goals, two integral workshops facilitated primary activities, where participants engaged in discussions and collaborative sessions, contributing to the project's objectives.
Workshop I (September 16-17, 2019, Georgia Tech) showcased a comprehensive overview of ongoing single civil infrastructure Smart City Digital Twin efforts, encompassing mobility, water, and energy. Featured 15 conference-style presentations, which provided insights into cutting-edge developments, while seven Smart City Digital Twin technology demonstrations highlighted the current state of the field. The workshop involved inclusive panel discussions that explored the barriers, challenges, and opportunities associated with Smart City Digital Twinning. Perspectives were sought from diverse disciplines, ensuring a holistic understanding that spanned academia, industry, and government boundaries.
Workshop II (October 26, 2022, IEEE DTPI 2022) held in conjunction with the IEEE Digital Twins and Parallel Intelligence international conference in Boston, served as both a follow-up and a shift in focus to current Smart City Digital Twin efforts, revisiting the progression of the field and evolution of digital twin technology since the initial workshop in 2019. Featuring 12 insightful conference-style presentations. Topics ranged from innovative community resilience hubs to virtual reality-based building emergency training, environmental justice, and predictive maintenance automation. The workshop was divided into two sessions: Advancements in SCDT Methods & Applications and Mapping the Future Directions & Opportunities of SCDT. The former explored the latest trends, challenges, and opportunities in Smart City Digital Twins, showcasing practical tools and techniques. The latter session was designed to generate speculative discussions, exploring possibilities for new research, technology, or policy. The workshop succeeded in capturing perspectives across disciplines, providing a platform for in-depth exploration and fostering collaboration in the dynamic landscape of Smart City Digital Twins.
The intellectual merit of the project lies in advancing understanding at the intersection of infrastructure systems, human systems, and technology systems. The sharing of existing knowledge on single infrastructure Smart City Digital Twins contributed fundamental insights into multi-infrastructure interdependencies. New knowledge was generated through the engagement of interdisciplinary academic experts, industry practitioners, and government officials in workshop-style facilitated discussions. The convergence workshops facilitated the development of a framework for comparing Smart City Digital Twin efforts and charting a roadmap for future endeavors. The project marked the emergence of Smart City Digital Twins as a transformative discipline with the potential to shape the livability, sustainability, and resilience of cities.
The broader impacts of the project extends beyond academic boundaries, fostering convergence among various disciplines and technology systems and successfully encouraged collaboration among academia, industry, government, and communities. The project recognized the imperative of community engagement and the involvement of citizens in the development of Smart City Digital Twins. It highlighted the importance of addressing fundamental questions surrounding the scope and application of Smart City Digital Twins, laying the groundwork for a more comprehensive and equitable approach. The impact goes beyond theoretical considerations, with practical recommendations emerging from discussions, including empowering interdisciplinary collaboration, investing in real-world infrastructure Digital Twin testbeds, improving data quality and accessibility for more reliable Digital Twin models, and engaging with stakeholders and the broader community to incorporate their needs and priorities into modeling and decision-making processes.
Overall, the workshops provided a valuable forum for sharing insights, experiences, and recommendations. They highlighted the potential for continued growth and innovation in the area of Smart City Digital Twins. The outcomes and reports from the workshops are accessible via the project website (https://smartcitydigitaltwins.gatech.edu/NSF-SCDTConvergence). Additionally, the invited contributions have been published in the proceedings of the IEEE Digital Twins and Parallel Intelligence (DTPI 2022) conference and accessible via the IEEE Xplore (https://ieeexplore.ieee.org/xpl/conhome/9998874/proceeding).

SBIR Phase II: Reconstructing Consistently Detailed City-Scale Environments From Incomplete 2D and 3D Data
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is the ability to more completely understand the real world through a perfect virtual copy. This SBIR Phase II project makes it possible to create virtual reproductions of real cities that not only show how the world looks, but reproduce every object and every detail perfectly, allowing users to interact with it. These virtual copies make it possible to quickly and effectively train soldiers and first responders, train autonomous ground and flying vehicles, place new construction into a dense city, simulate the effects of catastrophic weather, and explore imaginary scenarios through games. Billions of dollars are already spent mapping and 3D modeling the real world for these and many other applications, but these virtual cities are created through painstaking manual methods that can take months to years, or lack necessary information about what is in the world. This SBIR Phase II project will make it possible to rapidly and automatically create detailed 3D copies of any area of the real world.

This Small Business Innovation Research (SBIR) Phase II project will advance the state of the art in reconstructing highly detailed 3D models of the world for diverse commercial applications. This project introduces new methods for turning raw multimodal sensor data into semantic information describing the world and immersive, interactive-ready 3D models. It will remove the time, money, and manual effort necessary to create accurate 3D models of real world areas today, by using computer intelligence instead of human effort to parse sensor data like photographs and laser scans. The resulting 3D models and underlying semantic information describing the world will be used directly in game engines and simulation software, in analysis tools, in rendering software, and beyond. This research will build on the associated Phase I project, first improving the detail that can be identified and reproduced in virtual copies of the real world, then showing readiness for commercialization with paying customers. The result of the project will be a market-ready product for an initial market segment, capable of accurately reproducing real cities in 3D models that customers can readily use, as well as traction that demonstrates customer need for the product.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Digital twins of the real world are a vital component of business in many fields: billions of dollars are spent creating models of real environments for games and simulations, as well as architecture, urban design, and urban planning. These virtual copies of reality allow humans to be trained safely and efficiently, entertainment to be set in immersive environments players and viewers know and love, the world of tomorrow planned in painstaking detail, and new technology tested without endangering lives or property. Today, these models are often built with weeks, months, or years of painstaking and expensive manual modeling, or via inaccurate, ugly automated techniques like photogrammetry that don't know what they're reproducing.

Geopipe is building the authoritative digital twin of the world, using AI/ML techniques to understand what's in the world, and then rapidly creating metadata-rich digital twins ready to use in customers' existing workflows. Founded by two CS PhDs who have collaborated for two decades to solve hard technical problems, Geopipe has used NSF and investor funding to build unique technology to understand reality from raw sensor data, and deliver digital twins to accelerate simulating, visualizing, and interacting with the real world in virtual space across many verticals.

The NSF SBIR Phase I project associated with this Phase II/B allowed Geopipe to prototype a pipeline that could take multimodal sensor data, identify objects and their properties, then reconstruct the resulting rich 3D metadata into accurate 3D cities. This Phase II/B project encompassed creating the production-ready, scalable ML techniques and supporting pipeline components to segment the objects in the world, identify their properties, then reproduce each object at the level of detail necessary for gaming, simulation, and architecture customers. It also included substantial commecialization and supporting activities, including enterprise sales, major product releases, hiring/team growth, closing institutional investments, building relationships with other goverment groups (e.g. Army and Air Force funding), marketing, and partnerships.

Geopipe's AI reality-modeling capbility built and refined during this Phase II project has begun making an impact on the crowd and training simulation, gaming, architecture, urban planning, urban design, real estate, and other fields, an impact that will increase as we can provide higher-detail models covering more geographic data. Specifically, it is and will be come much cheaper and easier to analyze and visualize the current world or a modified world of tomorrow without expensive, slow manual reconstruction, and more detailed visualizations and planning can be added to typical projects where it is currently infeasible. The availability of Geopipe’s models via Geopipe’s API and Unity and Unreal SDKs also makes possible interactive and simulation applications in game engines at an unprecedented scale, and we have already seen adoption that proves developers are thinking larger-scale with what they can do with Geopipe. Finally, the time, money, and resources freed from manually repeatedly reconstructing and detailing virtual models of the real world is likely to free those human resources for the technical and creative tasks where humans are uniquely suited to excel.

Collaborative Research: Adaptive Gaussian Markov Random Fields for Large-scale Discrete Optimization via Simulation
Major federal agencies, including the Department of Veterans Affairs, Department of Defense, Department of Homeland Security, Federal Aviation Administration, Department of the Treasury, Internal Revenue Service, Centers for Medicare and Medicaid Services, Department of Health and Human Services, and others, seek government and non-government assistance with the application of scientific, data-driven methods to help them execute effectively on their critical missions. Because their mandate is typically large-scale, complex, and involves inherent uncertainty, computer simulation is often the only tool for representing their problems in a comprehensive way. Similar problems occur in the private sector, especially in health care delivery, computer networks, warehousing and distribution, and transportation systems. Unfortunately, "large-scale, complex, and involving inherent uncertainty" are the features that make "optimizing" a simulated system hard, particularly when the decisions are how to allocate discrete units of resources such as personnel, vehicles and facilities. The proposed research marries high-performance computing, smart numerical methods, and state-of-the-art statistical methodology to significantly increase the size and complexity of simulated systems that can be optimized. As a result, agencies such as those listed above will be able to more fully solve their "system of systems" resource-allocation problems using computer simulation.

The proposed research tackles statistical and computational challenges that arise in solving large-scale stochastic optimization problems when the objective function may only be evaluated by executing a stochastic simulation. Such optimization problems are often with respect to a high-dimensional, discrete-valued decision variable in a large solution space. The modeling flexibility of simulation comes at a cost: arbitrarily complex stochastic simulations may not be optimized using tools from mathematical programming. As a result, the scale of problems that can currently be solved by simulation with an optimality gap guarantee is limited. The investigators propose to create theory, algorithms and software for large-scale discrete-decision-variable simulation optimization that converge to the global optimum asymptotically, and provide optimality-gap inference when terminated. The proposed methods are based on inferential optimization, which models the unknown objective function by a Gaussian Markov Random Field (GMRF), a type of Gaussian Process defined by a graph on the discrete solution space; the investigators have shown that GMRFs provide better inference for a discrete problems than Gaussian processes defined on a continuous domain. The conditional distribution of a GMRF provides inference for selecting solutions to simulate and for search termination when the inferred optimality gap is small. However, the computational cost of numerical linear algebra increases faster than the number of feasible solutions. To facilitate the solution of large-scale problems, three core topics are proposed: exploiting high-performance computing; creating a restricted search scheme and tailored computational linear algebra that significantly reduces the computations in GMRF updates; and attacking limits on dimensionality via an adaptive multi-resolution GMRF and projections to lower dimensions. This award will provide support of graduate student training through research.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Stochastic simulation is a tool for the design, analysis and improvement of large-scale systems that are subject to risk and uncertainty. Application areas are broad and diverse, including manufacturing, supply chains, healthcare delivery, transportation, digital twin technology and service systems. Simulation optimization is the key technology employed by industrial engineering, operations research and management science to attain the best possible system performance using stochastic simulation. In many simulation optimization problems the underlying decision variables over which the system can be optimized are discrete valued; examples include the number of units of each product to stock in a sports equipment supply chain, the number of beds to allocate to mental health patients in a hospital, and the number of vans to include in a dial-a-ride fleet. However, solution technology for discrete-decision-variable problems has been greatly limited in the size (number of feasible solutions) and dimension (number of quantities to be set) of the decision variables of problems that are practically solvable. This research project pushed these limits back substantially, thereby allowing richer real-world problems to be solved. The intellectual merit of this project was extending Bayesian optimization to discrete-decision-variable problems by employing an innovative prior distribution---discrete Gaussian Markov Random Fields---smart computational linear algebra for posterior updating, projection and decomposition techniques to reduce dimension, a new Bayesian acquisition function for guiding the search, innovations in hyperparameter estimation, and parallel computing to greatly extend the size and dimension of practical problems that can be solved. Problem classes addressed include simulation optimization over integer-ordered decision variables, over integer-ordered decision variables with apriori objective function information, over categorical decision variables, and over categorical decision variables in the presence of real-time covariate information. Specific applications included future mobility simulations for General Motors, air traffic recovery from weather and equipment disruptions, and real-time assortment optimization using publically available data from Yahoo! In addition, the grant supported related work in simulation analytics (i.e., deeper analysis of stochastic simulation outputs). The research outcomes, including publically available, open-source software, have broad impacts across the application domains noted above. Further, the methodological breakthroughs have value in Bayesian optimization more broadly, including the design and analysis of deterministic computer experiments. Two Ph.D. students at Northwestern University were supported, and publication and presentation of the work was disseminated widely via journals, conference proceedings and highly visible workshops.
Student Support: 4th North American Industrial Engineering and Operations Management Conference, Toronto, Canada; October 23-25, 2019
This award supports student participation in the 2019 Fourth North American International Conference on Industrial Engineering and Operations Management in Toronto, Canada. It is organized by the Industrial Engineering and Operations Management (IE0M) Society International. The conference provides a forum for academia, researchers and practitioners to exchange ideas and recent developments in the fields of industrial fourth revolution / cyber physical systems, also referred to as Industry 4.0. Smart technologies are shaping our industries and societies. US industries are competing in the global footprint to lead in smart intelligent technologies along the entire supply chain. The new workforce is expected to have new job types that currently do not exist. Student participants will have opportunities to learn from industry practitioners and academia from conference technical and keynote sessions related on Internet of Things (IoT), artificial intelligence (AI), big data, iCloud, cybersecurity and related fields. Most important to student participants is the opportunity to gain a world-wide perspective from representatives from over 50 countries about research, development, design and manufacturing of global supply chain networks, vertical integration, and the use of cyber-human systems comprising sensors, IoT, and understood through advanced analytics and computational modeling. The investigators will enable students to improve their knowledge of disruptive technologies of Industry 4.0 and contribute to the competitiveness of the 21st Century U.S. workforce.

This award supports conference registration, lodging and a portion of the travel expenses of approximately 51 graduate and undergraduate students who should be enrolled full-time at a United States colleges or universities. Students at colleges and universities will be made aware of the opportunity through a posting on the conference website, the conference committee network, and from students who participate in various competitions, posters and technical papers. Students will be selected for the award by the conference committee. The criteria for granting a student award includes the following: enrolled in a U.S. college or university as a full-time student, desire to present a paper or poster, and willingness to attend the Industry 4.0 related sessions. Preference will be given to students from historically underrepresented communities including African Americans, Native Americans and Hispanics. After the conference, the list of student awardees will be announced on the main conference website, in the IEOM post-conference report, and in the IEOM Society Awards website.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content. This award supported the student participation in the 2019 Fourth North American International Conference on Industrial Engineering and Operations Management in Toronto, Canada. It was organized by the Industrial Engineering and Operations Management (IE0M) Society International. The conference provided a forum for academia, researchers and practitioners to exchange ideas and recent developments in the fields of industrial fourth revolution / cyber physical systems, also referred to as Industry 4.0. Student participants had opportunities to learn from industry practitioners and academia from conference technical and keynote sessions related to Internet of Things (IoT), artificial intelligence (AI), big data, iCloud, cybersecurity and related fields. Most important to student participants was the opportunity to gain a world-wide perspective from representatives from over 50 countries with more than 300 participants. This award enables students to improve their knowledge of disruptive technologies of Industry 4.0, as well as to help them become engineers and researchers who will lead the United States into the next generation and be prepared for the smart disruptive technology challenges. This award supports conference registration, lodging and a portion of the travel expenses of 36 graduate and undergraduate students who are enrolled full-time at a United States college or university. Out of 78 applications, 48 students were selected. Thirty-six students from 26 schools attended.  Due to visa issues, some students were unable to attend. Participants attended from many major minority schools. More than 50% of participants are from minority groups including African American, Hispanic, and Native American. There were 36% female and 47% undergraduate students who attended the conference as NSF travel support awardees. Pre and post surveys from the conference attendees were conducted. Thirty-five students participated in the pre survey and 29 students participated in the post survey. In the pre-survey, about 10% to 20% of students had good knowledge about Industry 4.0 areas and in the post survey, 20% to 40% of students mentioned that they have good knowledge about Industry 4.0 areas. After the conference, according to the survey results, individual improvements occurred in the students’ understanding and learning. In the survey on Internet of Things (IoT), 22% of the preconference participants mentioned that they were very knowledgeable and 52% of the post conference participants mentioned that they were very knowledgeable. It indicates that their conference attendance has helped in their understanding and learning of IoT. The percentage of the students’ mentioned very knowledgeable between preconference and post conference responses of other areas of Industry 4.0 are: 22% to 34% for artificial intelligences (AI), 31% to 48% for big data and data analytics, 14% to 34% for iCloud / cybersecurity, 25% to 41% for modeling, simulation and visualization, 19% to 38% for industrial automation, 6% to 38% for integrated systems, 6% to 28% for manufacturing execution systems, 8% to 31% for digital factory, 8% to 28% for product lifecycle management (PLM), 3% to 28% for digital twin and 6% to 31% for digital manufacturing. In the post survey, a question was asked if any students changed their career plan as a result of attending the conference. Ten students out of 29 responded that they have changed their career plan after attending the 4th NA IEOM Society Toronto Conference. This conference was supported by a strong collaboration with industry. Some of the industry participants were from Siemens, Tooling Tech Group, FCA, Ford Motor Company, General Motors, Quest Management Inc., Magna, Cintas Corporation, Harpco Systems, Inc., Medtronic, Larsen & Toubro Infotech Ltd., and Nexas Networks Inc.  They attended three panel sessions: Industry 4.0, Lean Six Sigma and Women in Industry and Academia. After the conference, the list of student awardees was announced on the main conference website, in the IEOM Society post-conference report, and in the IEOM Society Awards website. The survey was conducted before and after the conference to assess the attendees learning experiences. The learning outcomes were presented at the 5th NA IEOM Society Detroit Conference. In addition, the survey results will be presented at the upcoming INFORMS Annual Conference and ASEE Annual Conference. It will also be disseminated through IEOM Society Newsletter, announcements, and awards ceremony. The IEOM Society will use students’ Industry 4.0 learning outcomes at similar events for future IEOM conferences.

Planning IUCRC at University of Rhode Island: Center for Ocean System Simulation and Control
Ocean systems are critical to the well-being of the United States, providing goods through shipping and port systems, providing energy through offshore oil, gas, and renewable development, and providing national security through ocean based national defense systems. The integration of autonomy into ocean systems has the potential to significantly impact these fundamental industries by reducing costs and enabling new industry advances through unmanned navigation of transport vessels, development of underwater drones for oceanographic surveying or defense activities, digital twin development for operation and control of vehicles and platforms, and optimization in design for the development of competitive renewable energy technologies. The Center for Ocean System Simulation and Control (COSSC) seeks to integrate multi-fidelity computational modeling techniques with physical sensor systems to advance fundamental research in the controlled operation of smart ocean systems, including the use of advanced materials, the use of distributed sensing, autonomy and innovation of vehicles and platforms, energy savings, and fluid-structure dynamic modeling. Based on the research interests inputs given by the industry, both sites will form diversified research groups including underrepresented students, organize outreach programs for young generations, and design short courses for industry professionals.

The objective of this planning phase of the center is to assess the viability of the center, with two sites located at Texas A&M University (TAMU) and the University of Rhode Island (URI). This project will include a planning meeting to meet with prospective industry members of the center to present fundamental research topics and develop a set of initial industry vetted research projects to form the research basis of the center. The TAMU site will focus on industry collaborations in offshore oil and gas and shipbuilding, with example research topics in smart offshore system health monitoring, coupled fluid-structure modeling, CFD of extreme loading events, optimization of floating vessel geometry, and complexity reduction in CFD modeling. The URI site will focus on industry collaborations in defense and ocean renewable energy with example research topics in fast CFD hybrid methods, floating vehicle motion control from reconstructed sea states, hydrodynamic modeling for micro UUVs, moving vehicle digital twins, and interaction of wave energy converters with marine sediments.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content. This project was a planning grant to assess the establishment of an NSF IUCRC called the Center for Ocean System Simulation and Control.  The IUCRC has two sites, one at the University of Rhode Island and one at Texas A&M University, with this planning grant focused on establishment of the URI site.  The planning activities looked to establish a center focused on developing a fundamental understanding of integrated physical sensing with variable fidelity computational simulation for the control of smart ocean systems, focusing on transferring fundamental knowledge to the defense, renewable energy, and offshore industries. The planning process covered in this grant included training at the National Science Foundation in order to develop a center business model, value propositions, and recruitment/retainment strategies for industry members in the center.  As a result, the establishment of the COSSC IUCRC has the potential for ocean industry companies to invest in workforce development and fundamental research that can directly impact advances in ocean systems, including the development of digital twin technology that can incease safety and reduce operational costs, increase the use of autonomy in ocean systems, and introduce cost reductions through the use of modeling and simulation. The planning process also included holding a hybrid virtual and in-person planning meeting, held in Houston, Texas on July12-13, 2021.  The planning meeting connected potential industry participants in the center with participating faculty, established the operating membership requirements for the center, and introduced potential research projects for the center, that were ranked by participating industry members in the meeting.  This process established research themes for the center of particular interest to the potential industry members, including digital twins of ocean systems, artifical intelligence applied to ocan systems, ocean renewable energy, and multi-fidelity computational fluid dynamics.  The outcome of this meeting was identification of interested members in the center and the establishment of collaborative research projects between sites focused on these research themes. The planning meeting also outlined ways to engage graduate students through the formation of the COSSC center and included strategies for broadening participation and diversity, equity, and inclusion, including leveraging the LSAMP program at both universities for recruitment of participating students, early placement of underrepresented undergraduates in research labs participating in the center, providing funding for summer research opportunities to underrepresented students, and increasing networking opportunities for students with industry participants, with the goal of establishing regular hiring opportunities for graduating students. This project established the necessary framework in order to establish a COSSC IUCRC.  It was found that sufficient interest exists from companies in the defense, renewable energy, and offshore energy sector in order to maintain the financial membership requirements for the center to operate.  As a result of this planning process and recruitment activities, TAMU and URI will submit a full proposal to the National Science Foundation to request to establish the COSSC IUCRC.

Planning IUCRC at Texas A&M University: Center for Ocean System Simulation and Control
Ocean systems are critical to the well-being of the United States, providing goods through shipping and port systems, providing energy through offshore oil, gas, and renewable development, and providing national security through ocean based national defense systems. The integration of autonomy into ocean systems has the potential to significantly impact these fundamental industries by reducing costs and enabling new industry advances through unmanned navigation of transport vessels, development of underwater drones for oceanographic surveying or defense activities, digital twin development for operation and control of vehicles and platforms, and optimization in design for the development of competitive renewable energy technologies. The Center for Ocean System Simulation and Control (COSSC) seeks to integrate multi-fidelity computational modeling techniques with physical sensor systems to advance fundamental research in the controlled operation of smart ocean systems, including the use of advanced materials, the use of distributed sensing, autonomy and innovation of vehicles and platforms, energy savings, and fluid-structure dynamic modeling. Based on the research interests inputs given by the industry, both sites will form diversified research groups including underrepresented students, organize outreach programs for young generations, and design short courses for industry professionals.

The objective of this planning phase of the center is to assess the viability of the center, with two sites located at Texas A&M University (TAMU) and the University of Rhode Island (URI). This project will include a planning meeting to meet with prospective industry members of the center to present fundamental research topics and develop a set of initial industry vetted research projects to form the research basis of the center. The TAMU site will focus on industry collaborations in offshore oil and gas and shipbuilding, with example research topics in smart offshore system health monitoring, coupled fluid-structure modeling, CFD of extreme loading events, optimization of floating vessel geometry, and complexity reduction in CFD modeling. The URI site will focus on industry collaborations in defense and ocean renewable energy with example research topics in fast CFD hybrid methods, floating vehicle motion control from reconstructed sea states, hydrodynamic modeling for micro UUVs, moving vehicle digital twins, and interaction of wave energy converters with marine sediments.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
TAMU and URI held Planng Meeting where industry personnel related to Ocean Systems Simulation and Control provided the university researchers with industry challenges and long-term visions to specify research focuses and the university researchers identified and refined research proposals to be voted by the industry personnel. 
In the Planning Meeting, TAMU site was joined by 18 companies who are seriously considering to join NSF IUCRC, which include 16 companies who are currently participating an experimental consortium and agree with evolution of the experimental consortium to NSF IUCRC. Over the two cycles, the experimental consortium was successfully operated following NSF IUCRC guidelines, exceeding the membership requirements. 
Through the Planning Meeting and experimental consortium, we anticipate that the industry-selected research projects are to develop more efficient design of the ocean systems, smart health monitoring using digitalization, and autonomous offshore operations so as to achieve more profitable and reliable offshore operations. The NSF IUCRC research advancement will contribute to development of various ocean energy resources from natural gas and oil to renewable for more competitive costs, safer maritime transportation, and extended ocean survey and exploration.  
Students from underrepresented groups in STEM particpated in the Planning Meeting during the poster sessions and will continue the research through Phase I. Network of academia, industry, and end-user will be formed through the consortium to enhance the national competitiveness in the ocean energy, marine transportation, and ocean environment. 
Collaborative Research: SpecEES: Designing A Spectrally Efficient and Energy Efficient Data Aided Demand Driven Elastic Architecture for future Networks (SpiderNET)
Radio spectrum useable by mobile cellular networks is finite but cellular traffic continues to grow rampantly. This calls on scientific community to design networks that can push spectrum efficiency to its limits. On the other hand, making cellular networks energy efficient is a goal that is becoming more pressing than ever not only for operational cost reduction but also for minimizing the carbon foot print of the bulging information and communications technology industry. Recent studies show that unless new degrees of freedom and artificial intelligence based dynamic adaptability is added in the cellular architecture, any significant gain in spectral efficiency must come at cost of energy efficiency. The overarching goal of this project is to design, characterize, optimize and validate through a state-of-the-art testbed a new architecture that enables the additional degrees of freedom and intelligence to dynamically exploit these new degrees of freedom in the design and operation of the mobile network to yield substantial gains in both spectrum efficiency and energy efficiency, simultaneously. This proposed architecture is named as SpiderNET: Spectrally Efficient and Energy Efficient Data Aided Demand Driven Elastic Architecture for Future Networks. The key idea behind SpiderNET is to introduce additional degrees of freedom to relax the rigid spectrum efficiency-energy efficiency tradeoff and thus enable simultaneous enhancement of both. In wake of the internet of everything, as a key enabler of network resource efficiency, enhanced battery life and service level improvement, SpiderNET is bound to have a broad impact on nearly every aspect of evolving digital society that counts on wireless connectivity. In addition, as diminishing revenue per bit is already pushing cellular operators to reduce energy bills, huge energy savings enabled by SpiderNET can substantially reduce OPEX. Reducing the carbon foot print of cellular industry is also a key benefit of the proposed research. This project offers workforce training in a highly sought-after multi-disciplinary skill set while ensuring the participation of women and other underrepresented groups, and K-12 outreach. Compared to only theoretical or simulation-based research, a key distinction of the proposed research is the experimental research on a cutting-edge cellular testbed that is expected to cast a much broader impact. This project is collaborative undertaking with key national and international stakeholders in cellular eco-systems to ensure timely adaptation of the project outcomes by respective industry and government bodies.

The simultaneous enhancement of both spectrum efficiency and energy efficiency is achieved by shifting the pivot of operation from the rigid always-on base-station-centric cells to user-centric on-demand cells. To enable this, SpiderNET consists of a layer of low-density large footprint control base station underlaid by high-density switchable data base stations. The switching on/off the data base station, the size of user-centric cells (S-Zones) and other parameters are orchestrated proactively by a machine learning based self-organizing network (SON) engine that leverages a database of selected measurements at data and control base stations. Preliminary studies show that intelligent orchestration of the both, the size of the S-Zone and active data base station density, along with optimal design of the contents and spatiotemporal resolution of the database can substantially enhance both spectrum efficiency and energy efficiency without compromising quality of experience. The research will transform SpiderNET form an idea into a functional architecture by pursuing the following three research thrusts: 1) Developing analytical and simulation models to fully characterize the spectrum efficiency and energy efficiency of SpiderNET to determine the key design parameters that can be optimized to maximize its spectrum efficiency and energy efficiency gains. These models will then be leveraged to design algorithms for maximizing spectrum efficiency and energy efficiency in dynamic traffic conditions. 2) Designing the database of measurements and leveraging this data at control and data base stations to develop algorithms for proactive cell discovery and selection and radio resource allocation for jointly maximizing both spectrum efficiency and energy efficiency without compromising quality of experience. 3) Proof of concept of SpiderNET on TurboRAN (an NSF-funded end-to-end programmable testbed). This research leverages tools from domains of fluid modelling, stochastic geometry, game theory, machine learning and stochastic and multi-objective optimization to achieve its goals.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The legacy cellular networks have a fundamental limitation: they tightly link how efficiently they use spectrum (SE) and how much energy they consume (EE). This rigid, always-on approach lacks the flexibility to improve both SE and EE simultaneously, and it doesn't meet the high-quality service expectations for future networks. Our project set out to break this limitation by designing a new architecture that could flexibly manage both SE and EE without sacrificing service quality. The project has successfully achieved this ambitious goal, as evidenced by three Dissertation Excellence Awards, a Google Generation Award, 20 published or in-review journal articles, 10 conference publications and presentations, 2 patent applications, and significant influence on the cellular industry through invited talks and direct industry engagement.
 
Intellectually, our project laid the groundwork for a major shift in how cellular networks are structured. Our flagship achievement is the development of SpiderNET: a cutting-edge architecture designed for future networks that is both spectrally and energy efficient. Key innovations that brought SpiderNET from concept to reality include:
 
A digital twin-enabled learning framework that safely orchestrates network resources, avoiding the risks associated with online training.
A graph neural network-based network optimization method that uses deep learning to quickly and reliably adjust SpiderNET settings, based on an in-depth modeling of mobile network complexities.
A solution that minimizes unnecessary signal exchanges during user movement, enhancing both SE and EE.
A model that accounts for user movement in complex, layered network structures, including both ground and aerial components there by further improving SE and EE.
An expansion of network capabilities into three dimensions, offering significant improvements in efficiency and performance indicators without compromise.
Strategies to address the lack of training data for AI needed for network automation, enhancing system resilience and simplifying model selection and setup.
A method to compensate for imperfections in training data that used to train all AI for  SpiderNET's  or any other network orchestration, automation or optimization , ensuring reliability.
A novel approach to enhancing the resilience of SpiderNET optimization, even in scenarios characterized by scarce, unrepresentative, or erroneous training data.
A simplified hyperparameter tuning approach that leverages AI-based orchestration within SpiderNET, making the complex process more accessible.
 
These solutions stand out because they effectively tackle the challenges of automating a complex system like SpiderNET with minimal training data, ensuring resilience against data inaccuracies, and streamlining model and parameter selection.
The project also had broad impacts beyond technology. It supported the education and training of a diverse group of students, including two MS and four PhD graduates, many of whom received significant academic and industry recognitions including three dissertation excellence awards and Google generation Award. Project graduates have moved on to influential roles in the industry, contributing to innovation with numerous patent filings. We've widely shared our findings through keynotes, panels, and publications, reaching a vast audience across academia and industry. Collaborations with industry partners were strengthened, benefiting from our advancements. Moreover, project’s outcomes have enriched educational content at the University of Oklahoma, leading to new courses and learning opportunities on the NSF-funded TurboRAN testbed.

HDR: I-DIRSE-FW: Accelerating the Engineering Design and Manufacturing Life-Cycle with Data Science
The manufacturing life cycle begins with the discovery of new molecules and materials. This first step is often initiated through computer simulations that explore the space of possible molecules and materials, and identify promising candidates that can later be tested in laboratories. As simulations have grown in scale and complexity, this step has become a critical bottleneck. New data-driven approaches present the opportunity to increase the speed and accuracy of such predictions, with broad potential impact on the US Manufacturing sector. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) Frameworks award brings together Engineers and Data Scientists to conceptualize a new Engineering Data Science Institute where these tools can be applied for new discovery. The effort will develop new data science approaches to accelerate the engineering life cycle: design, characterization, manufacturing, and operation. This life cycle starts with the discovery of new molecules and materials, followed by advanced characterization with high throughput methods augmented by machine learning. Then, efficient manufacturing and operation of systems that use these materials can be designed and developed. By focusing on this holistic lifecycle, the researchers will build a broadly applicable foundation in Engineering Data Science methods. The new Institute will seek to create an Engineering Data Science environment that supports engineers and scientists (students, postdoctoral researchers, and faculty) through a synergistic set of collaboration and education activities.


This collaborative effort follows three thrusts. The first focuses on the reduction of the experimental design space with data science tools targeting the discovery of new molecules and polymers. The research develops a new, formal framework for pairing accurate predictive simulations with data-driven models to create a scalable and transferable workflow that can be deployed across multiple examples of molecular engineering applications. The second thrust addresses a manifold of cross-cutting needs at the intersection of image data analytics and characterization of materials and systems. It also builds community cyberinfrastructure through open-source software resources with support for execution in public clouds. The final thrust focuses on improving manufacturing, optimization, and control. It further enhances cyberinfrastructure resources through a suite of open-source software solutions to systematically develop digital twin models for complex engineering and manufacturing systems, and apply them for optimization and control. This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Office of Advanced Cyberinfrastructure.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content. This project developed new data science and AI methods to accelerate the engineering design and manufacturing life cycle. This life cycle starts with the discovery of new molecules and materials. The next step is their characterization. The final step is the efficient manufacturing and operation of systems that use them. By focusing on this life cycle, the project advanced the state of the art in methods that form a core of foundations in engineering data science, with broad applications. Specifically, the project developed new methods along three broad research thrusts. Thrust 1 focused on the reduction of the experimental design space with data science and AI tools. Thrust 2 focused on advancing characterization through imaging and analysis. Thrust 3 focused on improving manufacturing, optimization and control. The project resulted in publications, research presentations, and open source software. It also helped to educate multiple graduate students and postdocs. Beyond research, the project also contributed to creating an engineering data science community at the University of Washington through activities such as code reviews and networking events as well as by supporting initiatives that included the Computing for the Environment initiative and the AI @ UW initiative. The former brought together engineers, computer scientists, and data scientists to pursue research projects and develop data science and AI methods with applications to environmental challenges. The latter initiative brought together faculty, postdocs, and students on campus who are either developing novel AI methods or applying them to their discipline. The initiative helped participants make connections, learn about state-of-the-art projects, engage in discussions on a suite of topics, and start new collaborations.
MRI: Proteus++: Enabling Data-Intensive Computing at Drexel University
This grant will acquire, install, and maintain Proteus++, which is composed of Graphical Processing Unit (GPU) nodes and a high memory node. This project brings data-intensive computing (computing requiring large memory and high-throughput) hardware to Drexel University. The computing will strengthen research requiring large datasets in precision medicine (genomics, mapping the brain, and simulating molecules), advancing manufacturing, environmental modeling, and many other applications. The computing will also help train students in data-intensive computing through classes and co-operative work experiences and broaden participation in computing by engaging students in computing research as well as undergraduate institutions (including Historically Black Colleges and Universities and women's colleges) throughout the region to use the resources and enhance computing education. The research and training that results will not only advance fundamental research but enable innovation that will benefit the tech, health, and biology-enabled industries in the Philadelphia region.

Proteus++ will enable computational discovery that is data-intensive (needing large memory and high-throughput) and impact over 200 users by offering hybrid architectures that provide new scientific capabilities and enable faster computations. The intellectual merit of this project derives from a large collection of research topics that will be greatly enhanced by the sheer computational power and large-data processing that Proteus++ provides. Thanks to hybrid and large-memory computing, high-throughput genomics can finally enter the regime of searching simultaneously tens of thousands of microbial genomes from a large volume and diversity of queries, not least which involves discovery of currently unknown microorganisms in a variety of environments. Enhanced-sampling molecular simulations and hybrid molecular-dynamics/docking methods will significantly increase in speed under the hybrid architecture of Proteus++, enabling precise views of rare biomolecular event processes and investigation of an unprecedentedly large number of complex protein targets. Also, the prediction and simulation of material behavior and impurity benefits from co-processor architectures improving materials design for textiles, medicine, and energy. Finally, understanding brain activity is now within reach through GPU-accelerated Monte Carlo simulations, which will improve technologies that exploit light tissue interaction in the brain. Besides its direct impact on Drexel, the enhancement of URCF capabilities enables the creation of the Philadelphia High Performance Computing Consortium (PHPCC), a partnership among Drexel and a number of local/regional undergraduate-only institutions, for the purposes of exposing larger numbers of Consortium members, including Drexel, faculty, postdocs and students to the power and possibilities of computational discovery techniques. Capacity-building training in the form of workshops and courses will help new and existing PHPCC users learn not only about local University Research Computing Facility resources but also about advanced computational resources available at other institutions within the US. Furthermore, up to 2 million core-hours will be made available for use by non-Drexel PHPCC faculty and students.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project acquired the computing hardware necessary for data-intensive computing, which has propelled academic and practical advancements while nurturing a diverse spectrum of talent. Our primary goals revolved around fortifying research capacities in precision medicine, advancing manufacturing techniques, and refining environmental modeling through the installation of high-performance GPUs and large-memory nodes. Drexel University, as a result, has gained a cutting-edge computing infrastructure facilitating research requiring vast datasets, especially in genomics, brain mapping, and molecular simulations.
 
The intellectual merit of the proposal resulted in Drexel University acquiring 12 GPU nodes (48 GPUs) and 2 large memory nodes which have enabled data-intensive computing to solve scientific problems in brain modeling, genomics, microbiomes, manufacturing design, disease pathology diagnostics, and drug discovery. While 41 users from 22 groups currently use data-intensive hardware, 230 users from 85 groups have access to the entire cluster. Scientific and educational results are disseminated in reputable journals, ensuring the broader scientific community's access to our findings. 25 journal and conference papers/presentations have directly benefited from the proposed science, while many other researchers around the university in other fields, have also produced papers and products using the data-intensive nodes.
The major scientific results have been in:
•    Brain Imaging: This research studied how to use light-based sensors to detect bleeding and other injuries in the brain. This research is crucial for improving how we detect brain injuries, especially in younger individuals.
•    Metagenomics: We have used various machine learning, especially deep learning, to sift through complex genomics data, which can be used for health (like predicting covid severity from genomic information) and environmental diagnostics.
•    Computational Drug Design: Fragment-based drug design method aims to expedite drug discovery processes.
•    Molecular Dynamics Simulations offer insights into Alzheimer's disease pathology and potential physiological functions proteins, indicating broader implications for brain health.
 
The broader impacts have focused on educational enhancement, offering students exposure to data-intensive computing through specialized classes and cooperative work experiences. In total, there have been 4 PI/Co-PIs, 7 undergraduates (4 of them REUs on special research projects), 3 graduate students, and 2 staff scientists trained to conduct workshops and office hours to train the community-at-large. Approximately, 70 users came to these sessions for training.  Extensive training efforts resulted in monthly workshops, weekly office hours, and the mentorship of continual mentorship of graduate and undergraduate students in system administration and programming. Additionally, the establishment of the Philadelphia High Performance Computing Consortium (PHPCC) has fostered collaboration among other Philadelphia-region institutions (Swarthmore College, Haverford College, and Rowan University), empowering faculty and students with computational discovery. Finally, several classes in bioinformatics, physics, chemistry, and mathematics have used the equipment for conducting special topic courses. Also, areas such as economics, public health, and other non-traditional high-performance computing areas have used the data-intensive hardware and helping to advanced those fields.
This project has not just expanded technological capacities but has served as an educational launchpad and a hub for cutting-edge research, ensuring a collective step towards scientific progress and innovation.
 CPS: Small: Mitigating Uncertainties in Computer Numerical Control (CNC) as a Cloud Service using Data-Driven Transfer Learning
Computer numerical control (CNC) is a critical feature of modern manufacturing machines. It provides automated control based on a set of programmed instructions, which traditionally run on a local computer that is physically tethered to the machine. This work envisions a future where manufacturing machines are controlled remotely over the Internet using CNC installed on cloud computers. Among several benefits over traditional CNC, cloud-based CNC holds promise to significantly improve the speed and accuracy of manufacturing machines at low cost. However, a major challenge with cloud-based CNC is that, somewhat like video streaming, it controls manufacturing machines primarily using pre-calculated commands that must be buffered to mitigate Internet transmission delays. For this reason, cloud-based CNC is susceptible to anomalies that result from delayed transmission of information on how the controlled machine is actually behaving. The award supports a scientific investigation into approaches for predicting impending anomalies from data gathered from past experience, and using the predictions to avoid incorrect control actions resulting from inadequate feedback. The U.S. stands to benefit economically from a transition from traditional to cloud-based CNC, since the U.S. is by far the market leader in cloud-based services. The project also will include outreach to U.S. companies, educational curriculum development to increase the U.S. talent pool in manufacturing and data analytics, and activities for middle schoolers in the Detroit area to inspire them to pursue careers in engineering.

The objective of the project is to mitigate uncertainties associated with real-time control of manufacturing machines from the cloud using data-driven transfer learning. The knowledge gained will boost the performance of manufacturing machines at low cost by providing the machines with reliable cloud-based CNC. In cloud-based CNC, advanced feedforward control functionalities are transitioned to the cloud while fast feedback loops are retained locally. However, with emphasis on feedforward control, uncertainties in modeling the dynamic behavior of machines could degrade the reliability and performance of cloud-based CNC by causing failures, due to inaccurate control actions. The system will predict failures using measured signals and mitigate them in a redundant, cloud-based CNC architecture by switching control authority from a cloud controller to a back-up local controller in the event of an impending failure. To this end, a data-driven transfer learning framework will predict and minimize uncertainties using data obtained from other machines connected to cloud-based CNC. Such transfer learning leverages data from one source to learn a different, but related, target source. The framework will allow cloud-based CNC to: (i) learn from a combination of condition monitoring signals and past failure data to predict impending failures, (ii) reduce uncertainties by leveraging condition monitoring data to calibrate physical models whose parameters are functions of their inputs, and (iii) plan feasible trajectories for switching from a cloud to a local controller when an impending failure is detected. The project will address the shortcomings of existing transfer learning methods by: (i) predicting failure events from a combination of condition monitoring and past failure data, and (ii) calibration of physics-based models with functional parameters from condition monitoring data. The methods will be evaluated experimentally on a CPS test bed consisting of a 3D printer controlled from the cloud using a cloud-based CNC prototype.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Computer numerical control (or CNC for short) is a critical feature in modern manufacturing machines. It provides automated control of manufacturing machines based on a set of programmed instructions which traditionally run on a local computer physically tethered to the machine it controls. The proposed work envisions a future where manufacturing machines are controlled remotely over the Internet using CNC installed on cloud computers. Among several benefits over traditional CNC, cloud-based CNC holds promise to significantly improve the performance (e.g., speed and accuracy) of manufacturing machines at low cost. However, a major challenge with cloud-based CNC is that, somewhat like video streaming, it controls manufacturing machines primarily using pre-calculated commands that can be buffered to mitigate Internet transmission delays. For this reason, cloud-based CNC is susceptible to anomalies which may result from inadequate feedback (i.e., not immediately knowing how the controlled machine is actually behaving). This award has supported a scientific investigation into approaches for predicting impending anomalies from data gathered from past experience, and using the predictions to avoid deleterious control actions that may result from inadequate feedback.
Intellectual Merit
The research objective was to mitigate uncertainties associated with real-time control of manufacturing machines from the cloud using data-driven transfer learning. This intellectual merit was achieved by carrying out three tasks, namely:
Task 1: Determination of Uncertainty Prediction Horizon using Reachability Analysis
Task 2: Predicting Event Probabilities via Joint Models
Task 3: Calibrating Physics-based Models having Functional Parameters
As part of Task 1, the team developed a strategy to determine when and how to switch from an advanced controller to a backup local controller. The proposed method was tested using an advanced algorithm that optimizes the speed of manufacturing machines using linear programming. Whenever the algorithm experienced uncertainty or infeasibility, it switched to a backup algorithm. The algorithm was demonstrated on a 3D printer where it achieved up to 25% improvement in motion speed without sacrificing motion accuracy.
As part of Task 2, the team developed a method to predict future failure events based on joint modeling of both longitudinal and time-to-event data. The methods achieved the state of art performance in joint models. In addition, the team developed a method for extrapolating multi-stream longitudinal data where multiple signals from different units are collected in real-time. It demonstrated a non-parametric approach to predict the evolution of multi-stream longitudinal data for an in-service unit through borrowing strength from other historical units.
As part of Task 3, the team developed a controller that calibrates a physics-based model using data gathered online during operation. The uncertainty from the physics based and data-driven models were incorporated into the controller. This resulted in an uncertainty-aware digital twin. The outcomes of using this uncertainty-aware digital twin were demonstrated experimentally on a 3D printer and machine tool; it led to cycle time reductions of up to 38% and 17% respectively, without sacrificing motion accuracy.
Broader Impacts
Some of the results of this project are being translated to industry through collaborations with two U.S. companies. The first is Ulendo Technologies, Inc., a start-up company that is leveraging the cloud infrastructure to provide calibration as a service to 3D printers. The second is CISCO, Inc., which is collaborating with the project team to translate the machine-learning related aspects of the findings to an infrastructure the company has developed for distributed computing.
Through the project, four PhD students have been trained, including a female student. Two of the students have graduated and have taken up positions as assistant professors at U.S. universities. Another graduate trained through this project has taken up a position as an engineer at a semiconductor manufacturing company. The fourth Ph.D. student is working towards his graduation.
The results of this project have been disseminated at manufacturing and controls conferences. They have also been published in very reputable journals in manufacturing, controls, mechatronics and statistics. A total of 10 journal papers have been published or submitted based on this project.
Collaborative Research: Crosslayer Optimization of Energy and Cost through Unified Modeling of User Behavior and Storage in Multiple Buildings
The building sector is the largest energy consumer in the world, and in the United States it accounts for more than 40 percent of the total energy consumption and greenhouse gas emissions. Therefore, it is economically, socially, and environmentally important to reduce the energy consumption of this sector. The goal of this collaborative proposal is to develop novel machine learning based algorithms to address the problem of energy optimization at the building and district levels. These algorithms are integrated within a simulation framework that combines user behavior with the collaboration between buildings equipped with photovoltaic arrays, energy storage systems, and smart grid meters. The proposed research is expected to lay the foundation for robust multi-objective optimization for next generation district level distribution systems. The proposed research is closely integrated with a broad and diverse education and outreach plan aimed at inspiring young women to pursue careers in STEM through summer programs for middle school. Additionally, the project will train the next generation of engineers and researchers by involving graduate and undergraduate students through the proposed research as well as through classes taught by the PIs encompassing the proposed research methodologies. Overall, the outcomes of this proposal are expected to significantly advance the areas of energy optimization, electric power systems, and smart grid design, as well as to have a positive impact on the academic and industrial communities and society.

The project proposes and integrates, within the same software tool, novel machine learning models for complex user behavior at the individual building level, for energy load prediction and energy storage systems scheduling at the district level, and for cost reduction via energy peak spreading. These models are used to formulate and construct algorithmic solutions based on reinforcement learning, recurrent and deep neural networks, and deep reinforcement learning suitable for implementation in the future generation Virtual Power Plants. The methodologies employed for energy reduction and cost minimization include: 1) alter user behavior through personalized recommendations regarding changes in the appliance states (e.g., heating and air conditioning settings), 2) district-level scheduling of energy storage systems among buildings equipped with photovoltaic arrays and smart grid meters, and 3) building-level scheduling of energy consumption events for smart appliances equipped with smart Internet-of-Things controllers to take benefit of different energy prices.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The primary outcomes of this project include the following:
1) The development of several machine learning (ML) models for hourly 24-hour ahead prediction of energy usage in heating, ventilation and air conditioning (HVAC) systems. These models were integrated within a custom co-simulation framework of buildings and distribution network and utilized to develop optimization algorithms for reducing the energy costs and usage of HVAC systems. These demand side management algorithms employed techniques based on game theory as well as on model predictive optimal control combined with Q-Learning reinforcement learning.
2) The development of several frameworks for i) optimization of energy sharing among prosumers; ii) incentive-based demand response with human-in-the-loop; and iii) appliance recognition through smart outlets with user-oriented stream-based learning.
3) The development of models and algorithms for HVAC energy component disaggregation and HVAC controls. Long short-term memory (LSTM) models were investigated for prediction of total and HVAC energy consumption as well as photovoltaic (PV) generation in buildings. The prediction models were used to develop combined control home energy management systems (HEMS) for HVAC, PV, and battery storage systems for behind-the-meter (BTM) demand response. Novel methods were developed to separate the HVAC dominant load component from the house load. The proposed algorithms are based on deep learning techniques and on the physical relationship between HVAC energy use and weather.

Collaborative Research: Software Engineering Workforce Development in High Performance Computing for Digital Twins
This project will contribute to the national need for well-educated engineers and technicians in production engineering. It will do so by supporting two, two-day workshops on production engineering education by the University of Tennessee Chattanooga and Tennessee Technological University. The overall goal of the workshops is to recommend educational approaches for the rapidly expanding production technology of digital twin research and development. A digital twin is a virtual model of a process, product, or service. This pairing of virtual and physical worlds enables analysis and improvement of systems, including production engineering systems. For example, by developing new efficiencies first in the digital twin, and then testing them in the physical twin, processes and products can be improved. It is expected that these workshops will contribute to achieving scalable, reliable, and cost-effective strategies for creation of digital twins that accurately represent complex systems, such as manufacturing and chemical plants, airplanes, and wind turbines. The workshops will also contribute to developing guidelines for training the STEM workforce needed to develop and implement digital twins in production engineering and other applications. The first workshop will be held at the University of Tennessee Chattanooga and will result in a report describing the intellectual contributions of multiple disciplines to the use of digital twins in production engineering. The second workshop, to be held at Tennessee Technological University four months later, will result in a report describing curriculum recommendations for digital twin production engineering education.

These workshops will convene thought leaders in high-performance computing, software engineering, uncertainty quantification, control system, digital twin modeling and simulation, and engineering education to develop recommendations for effective, innovative, distributed education related to digital twins, particularly in production engineering. In addition, they are expected to lead to convergent research projects at the intersection of educational research and the multidisciplinary areas needed to design, implement, and operate digital twins. At the workshops, these disciplines will come together to establish conceptual models for education that will prepare workers to design digital twins that better integrate with production engineering environments. Collaborations are expected to emerge between educators and domain experts, yielding outcomes such as the enhanced design of high-performance simulation and machine learning software systems that are both responsive to evolving needs of production environments and suitable for use in classroom and virtual environments.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project was funded by NSF's EHR Core Research: Production Engineering Education and Research (ECR: PEER) program, which seeks to improve the education of future and current professionals in production engineering. It also aims to study the effectiveness of the innovative educational strategies adopted by these projects. This project involved two workshops on Education in digital twins that were held jointly between University of Tennessee at Chattanooga and Tennessee Technological University, funded by the National Science Foundation under grant number 1935583, in response to the proposal entitled "Collaborative Research: Software Engineering Workforce Development in High Performance Computing for Digital Twins'' in the NSF Division of Undergraduate Education (DUE) program.

The overall goal of these workshops--which was achieved--was to recommend educational approaches for the rapidly expanding production technology of digital twin research and development. A digital twin is a virtual model of a process, product, or service. This pairing of virtual and physical worlds enables analysis and improvement of systems, including production engineering systems. For example, by developing new efficiencies first in the digital twin, and then testing them in the physical twin, processes and products can be improved. These workshops emphasized high performance computing as a topic within the digital twins area.

The workshops convened thought leaders in high-performance computing, software engineering, uncertainty quantification, control system, digital twin modeling and simulation, and engineering education to develop recommendations for effective, innovative, distributed education related to digital twins, particularly in production engineering. At the workshops, these disciplines came together to establish conceptual models for education that will prepare workers to design digital twins that better integrate with production engineering environments. The key outcomes achieved:

● The workshops contributed to developing guidelines for training the STEM workforce needed to develop and implement digital twins in production engineering and other applications.

● As post-workshop outcomes, sustaining collaborations are expected to emerge between educators and domain experts, yielding outcomes such as the enhanced design of high-performance simulation and machine learning software systems that are both responsive to evolving needs of production environments and suitable for use in classroom and virtual environment,
Recommendations/observations included the following:

● The lack of a formal definition of a digital twin than available now was seen as a gap.

● Broad applicability across engineering domains were noted by the participants.

● The need for digital twins undergraduate education is not yet apparent to the communities of interest (research, education, industry).

● The interdisciplinary nature of all digital twins activities, and in particular, education was noted as fundamental.

● Many social and ethical issues were raised concerning digital twins by the workshop participants. Privacy, verification/validation, and cyber-security issues were noted.

● Targeting single large assets or a large number of small, similar or identical assets, was identified as the valuable domain for digital twins, rather than for all systems.

● A digital twin's applicability occurs whenever operational or maintenance decisions are needed concerning the physical twin.

● Digital twins were seen to have many applications in a given field, rather than simply having a ``killer app'' in each field; also, specialization of a digital twin within a domain,rather than high-level simulators, was seen as important. However, having a ``killer app'' in each field, a canonical example, is noted as important to drive progress/motivate in educational settings.

● The skills/knowledge most applicable to digital twins include: data science, domain knowledge, modeling & simulation, machine learning, programming, software engineering, physics, cybersecurity, and mechanical engineering.

● At present, working with digital twins is an important knowledge and skill-set. However, in future, it may emerge as its own discipline and career specialty.

● Regarding undergraduate education, two views emerged: including data-science and machine learning courses in engineering curricula to support work with digital twins; and, conversely, the opinion that digital twin concepts should be introduced into such courses and specific engineering courses. An "optimal approach'' was seen as a core undergraduate degree (e.g., Electrical Engineering, Mechanical Engineering) plus skills from on-the-job training. Others involved in the workshops considered it to be too early to design curricula or minors, favoring inclusion of digital twin concepts in major-discipline courses first.

● Three models were considered for digital twins education (with pros, cons, and challenges described herein based on participant input): Integration Model (addition of digital twins concepts in existing courses), Certificate Model (a model in which course concepts of digital twins supplement a degree program), and Targeted Concentration/Minor & Full Degree Program (following common precepts for minors and accredited engineering degree programs).

Specific recommendations to overcome the challenges of digital twin education (both supply and demand for it), were noted, including:

● Identify different groups/types of personnel (roles) and what knowledge they need to perform their job functions

● Develop various pathways for education and training that target such groups/types of personnel (roles)

I-Corps: Smart and Holistic Agent Management System
The broader impact of this I-Corps project is to develop a software system with geographical, weather, and behavioral information to produce timely, interactive, and continuous improvement results for logistics operations. The tool is aimed at food and beverage distributors, business and service companies, and those in the retail industry who need on-time delivery with an excellent level of predictability. Home-delivery operations account for around 53% of total logistics costs in the retail industry. This project will develop a solution for the last-mile delivery industry, predicted to be a $55 B industry by 2025. The envisioned solution will result in increasing customer satisfaction and strong economic value by reducing costs and maximizing utilization of resources.

This I-Corps project will advance and automate the organization, dispatching, and routing of vehicles in urban environments using hierarchical and distributed decision-making using network modeling, agent-based simulation, and machine learning. These hierarchical decision-making processes can aid businesses in optimizing loading and sorting, dispatching, and routing operations, . Unlike existing schemes that have only partial near-optimal routing, and do not effectively take feedback from the different elements of the supply chain (e.g., customers, drivers, etc.), the proposed technology scheme also includes packing, loading and sorting optimization in synchronization with dispatching and routing.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Transportation managers need to improve daily transportation operations and noted how it had become a source of competitiveness growth and cost reduction. Routing planners struggle to accurately forecast delivery routes and volumes based on the day of the week, time, location, customer, and driver behavior. High traffic in urban areas and issues such as invalid or uncertainties in address accuracy, parking availability, among others, are significant problems to consider. Figure 1 depicts one route based on an actual operation in a megacity that meets all requirements thanks to SHAMAN (Smart and Holistic Agent Management System).
 
Broader impact
One of the main lessons from our I-Corps experience was defining who our target customers can be. Before going through I corps, we were only thinking about food and beverage and retail distributors. Thanks to the client's knowledge, we understood that commercial and service companies also need a tool to increase efficiency in their deliveries. Another significant impact that our technology can have is the support for Reverse Logistics processes. With our system, it is possible to synchronize the product's demands to be collected to pass any recycling or remanufacturing process. Figure 2 depicts the simulation environment that acts as a digital twin for the entire operation.
Our technology developments using network modeling, agent-based simulation with geographic, meteorological, and behavioral context, Machine Learning (with elements of deep reinforced learning) consider various factors to produce continuous, interactive, and timely improvement results. Precisely being able to synchronize the needs of different urban logistics actors gives great commercial and competitive value to our system. These benefits translate into more efficient use of resources, the reduction of gasoline or diesel in vehicles, and, of course, less environmental impact. For large logistics companies, having savings in fuel use in each of their vehicles, which can be many per day, per week, and per year, ensures a good return on investment for those who decide to invest in advanced decision technologies. E-commerce sales were shown to have increased during the pandemic and are expected to continue to grow. The last-mile delivery industry is predicted to be a $ 55 billion industry by 2025. Figure 3 illustrates the principal flows of product in the last-mile for forward and reverse logistics.
 
Intellectual merit 
With I-Corps, we meet the objectives of advancing in the market's implementation and knowledge to provide a tool that can automate the dispatch tasks and the routing of vehicles in urban environments. SHAMAN is a decision-making platform based on network modeling, agent-based simulation, and machine learning. These decision-making processes help companies optimize operations for loading and sorting products of different sizes and volumes, dispatch, and routing.
The system architecture of the system is supported in more than 100 interviews with various stakeholders, including several transportation managers. Consequently, SHAMAN meets the requirements and is designed to be a competitive tool by reducing costs, maximizing the use of resources, and increasing customer satisfaction. Unlike existing schemes that only have near-optimal partial routing and do not effectively take feedback from different supply chain elements (e.g., customers, drivers, etc.), our system architecture is designed to perform packaging and loading. Figure 4 depicts the software architecture.
RII Track-4: Understanding the Fundamental Thermal Physics in Metal Additive Manufacturing and its Influence on Part Microstructure and Distortion.
The 3D printing of metal parts promises to transform U.S. manufacturing. For example, metal additive manufacturing (AM) has the potential to reduce time-to-market for a new jet engine from five years to one year, while simultaneously increasing fuel efficiency and power by 10%. Poor consistency in part quality, however, limits the use of AM. As a result, safety-conscious industries (e.g., aerospace and biomedical fields) are reluctant to use AM processes to make mission-critical parts. The root cause for flaw formation in metal AM is the uneven temperature distribution inside the part during printing. To ensure a steady temperature distribution inside the part, practitioners currently use trial-and-error studies that require experimenting with different process settings and part designs ? an expensive and time-consuming approach. A more efficient solution involves encapsulating the fundamental thermal physics of the printing process using computer simulation models. These simulation models can be used to identify and correct problems that can lead to an uneven temperature distribution in the part before it is built. The PI has advanced a new mathematical approach to predict the temperature distribution in AM parts that takes less than one-tenth of the time required by existing techniques and has an error of less than 10%. Rigorous validation of this concept with experimental data is the next step to scale this new concept to practice.

The objective of this fellowship is to test the hypothesis that the instantaneous spatiotemporal distribution of temperature generated in a metal AM part as it is being deposited layer-upon-layer is predicted by invoking the novel theory of heat dissipation on planar graphs (spectral graph theory) with an accuracy comparable to existing finite element techniques but within a fraction of the computation time (less than 1/10th). To realize this objective, this fellowship provides the PI access to the Open Architecture Laser Powder Bed Fusion metal AM system at the Edison Welding Institute (EWI). This system has eight different sensors and allows the in-situ measurement of thermal signatures at scales ranging from 5 micrometer to 400 micrometers. Access to this unique apparatus will allow the PI to measure the instantaneous temperature distribution in a part and track changes in its shape with unprecedented precision. Using data obtained from experiments on the open architecture metal AM system at EWI, the PI will: (1) explain and an quantify the causal factors governing the temperature distribution in metal AM parts and link it to part quality; (2) achieve near real-time prediction of the temperature distribution, which will significantly reduce the experimental tests needed to optimize the part geometry and process parameters; and (3) establish the digital twin concept for qualification of metal AM parts by augmenting in-situ sensor data with physical process models. This work will result in experimentally validated, physics-based tools to aid rapid optimization of process settings and part geometry, which in turn will shorten time-to-market for AM parts and reduce scrap rates by up to 80%. This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Laser powder bed fusion (LPBF) is a metal 3D printing (additive manufacturing) process in which layers of tiny metal powder about half the diameter of a human hair are deposited and melted with a rapidly moving high energy laser. The LPBF process has the capability to produce parts of complex shapes that are difficult, if not impossible, to produce with conventional manufacturing processes. For example, when General Electric Aerospace used the LPBF process to make aircraft engines, it reduced material waste by a factor of 5, decreased the number of separate parts from 800 to less than 20, and increased engine power by over 10%. Similar examples of making high-performance parts rapidly and with minimal impact to the environment with the LPBF process are evident in industries critical to the nations security and prosperity, such as aerospace, defense, automotive, energy generation, and biomedical. 
Despite these demonstrated capabilities, the LPBF process is afflicted by high failure rates - close to 30% of the parts fail to build. To overcome these high failure rates, industry practitioners spend several million dollars in materials and expert time to fine tune the process to produce one type of part. For example, the aircraft engine parts built by General Electric using LPBF required an expensive multi-year trial-and-error effort of a large engineering team. 
The scientific reason for high failure rates in LPBF is because the part is subjected to high heating and cooling rates during printing. These rapid heating and cooling cycles can approach nearly 1 million degrees a second. The manner in which the temperature is distributed in the part during printing, called thermal history, determines the quality of the part, such as its internal structure (microstructure) and the accuracy of the final part shape (geometric integrity). An uneven distribution of temperature can cause the part to become distorted or even develop minute cracks, which in turn may lead to catastrophic failure during operation - imagine an aircraft engine disintegrating mid-flight. Therefore, an open challenge in the LPBF industry is to replace trial-and-error testing with computer simulations that can accurately predict the thermal history. Using such tools, practitioners can anticipate when flaws will form, and take preventive steps, such as changing the processing conditions or modifying the design of the part to avoid failures.  
Researchers have responded to this challenge by forwarding computational models for predicting the thermal history. Majority of these solutions rely on finite element or finite difference techniques that are accurate but computationally tortuous. The time to predict the thermal history can take hours, if not days, on a supercomputer. Proprietary solutions also make assumptions to simplify the heat transfer calculations. These assumptions are opaque to users, and indeed, there is a 60% variation reported between solutions offered by various commercial software vendors. 
The PI's work addressed this challenge through a new computational heat transfer approach invoking the theory of heat dissipation on graphs (spectral graph theory). This novel approach allows the thermal history in a LPBF part to be simulated layer-by-layer within a few minutes on a desktop computer, instead of days on a supercomputer using existing approaches, while maintaining accuracy within 90 percent. Published results showed that the simulation time is about 1/100th of the part build time.The new approach was deployed in practice at Edison Welding Institute (EWI) and verified with experimentally acquired measurements.
This EPSCOR Track IV fellowship provided critical experimental data in an industrial setting to advance the following new scientific knowledge and methodological breakthroughs in LPBF. documented via peer-reviewed publications:
Explained and quantified the link between the process parameters, part geometry, thermal history, and part quality, such as microstructure and distortion.
Achieved the near real-time prediction of thermal history using a novel mathematical concept of heat diffusion on graphs. 
Established the digital twin concept for rapid qualification of metal AM parts by augmenting in-situ sensor data with physical models, and 
Introduced a physics-based feedforward control in LPBF to  anticipate flaws before the part is printed, threby signficantly reducing the experimental tests needed to optimize the part geometry and process parameters.
From a broader impacts perspective, the generated data was successfully leveraged by 3 Ph.D. students and 2 MS students, with 2 more Ph.D. students continuing the work.  The parts made during this project was used to train: (i) 42 undergraduate students via a new hands-on additive manufacturing class; ( ii) 1 high-school teacher; and (iii) 4 native American scholars. The methods and approaches from this work are being pursued towards commercialization, with trial tests on commercial LPBF machines at EWI, SigmaAdditive Solutions, and Open Additive Inc.
The results from this work will allow US manufacturers to accelerate the time-to-market for LPBF parts, reduce waste, and usher green, sustainable manufacturing of high-value components.  
SBIR Phase I: Automatic Reconstruction of As-is Building Information Model from Indoor Point Cloud Data for Planning Purposes
The broader impact of this Small Business Innovation Research (SBIR) Phase I project will develop a new platform automating the 3D model generation process from cloud data. Several companies have developed tools to facilitate the modeling process, but despite the benefits offered by their tools the process is still semi-automatic, expensive, time-consuming, labor-intensive, error-prone, and requires a designer. The proposed project will create 3D solid representations of structural, architectural, and mechanical components (e.g., beam, ceiling, column, floor, pipe, wall); openings (e.g., door, window); and furniture (e.g., sofa, bed, chair, table). Furthermore, the platform will allow use of the generated model for visualization, coordination, and scene management; simplify planning tasks; and improve communication and collaboration. The proposed solution will enable users with little or no experience designing 3D models to have the capability to increase productivity, reduce planning time and cost, and increase collaboration.

This Small Business Innovation Research (SBIR) Phase I project aims to develop a platform that provides a quick, easy, and economical solution to generate 3D solid representations of indoor scenes. The platform will use artificial intelligence (AI) to extract the geometric and semantic information embedded in point cloud data and fit a solid geometry for generating the 3D solid representation of indoor scenes such as living areas, offices, utility rooms, and mechanical rooms. The design of the proposed platform will consist of three major objectives: (1) automatic generation of 3D solid models, (3) new tools to use the model for planning purposes, and (3) integration with industry-standard communication tools. The first task has three principal research objectives: (a) multi-scale feature extraction, (b) semantic identification of the point cloud main elements through machine learning algorithms, and (c) 3D solid model generation using the elements' primitives and attributes.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The proposed work investigates the generation of realistic 3D solid representations, digital twins, of residential buildings to plan the asset's decoration, renovation, repair, and maintenance work. Professionals use digital twins to study physical structures, including their structural components and the people's interaction with the elements. The digital twin model for physical buildings is a dynamic model that requires real-time data, including updated building drawings, 3D models, documents, building performance, and notes that contractors and designers compile during construction. Even though digital models facilitate planning the work that must be done (e.g., maintenance, renovation) during the building life, the digital model is not always available since collecting the data to maintain the model is often very costly and time-consuming. Also, although digital twins have started to be used by industries ?such as construction, facility management, interior design, remodeling, and real estate industries-- the model's maturity level is in its early stages. The twin model requires a greater maturity level and digital transformation for digital twins to offer a more significant value to businesses and owners. 
The main component of the digital twin model is the 3D solid representation of the physical building, also known as the as-built model. The traditional approach for modeling the as-built environment is a complex process that involves several tasks, such as measuring the area, drafting 2D plans, producing 2D floor plans, and recreating the 3D model. Furthermore, to use the model for planning the activities related to maintaining, renovating, or decorating the residential building, the model needs to be updated constantly and before starting to plan the work activities. The model must be updated since buildings are living assets in constant change; the changes are produced by factors such as the condition of the building's elements, the variation in the number of elements, constant changes in the position or location of the elements, constant interaction with the building elements, and building additions made during its life. These factors increase the complexity of planning any work. Traditionally, updating or editing the model is done manually, which could take several hours, depending on the Level of Development (LOD) or the type of work to be conducted. Digital models reduce the complexity of planning the activities conducted during the construction and operation phases since it is a virtual representation of the physical asset. Pearnets effectively extends the digital twin model by making available 3D representations of residential buildings and simplifying to update the model information. 
The SBIR Phase 1 supported to start the development of a new Artificial Intelligence (AI) for assisting in the reconstruction of a 3D solid model (i.e., the main component of the digital twin model) of indoor areas in residential buildings using point cloud data. Additionally, the prototype of a Software-as-a-Service (SaaS) platform where the data (e.g., point cloud, mesh) and the model (generated by Pearnets) can be visualized and managed. The award supported the following activities:
(1)  Developed a solution for room segmentation. Generating a 3D model from point cloud data presents the challenge of managing the point cloud size. Aiming to reduce the processing time and the computer power required to semantic segment the point cloud, the team developed a room segmentation method that divides the point cloud into its principal rooms (e.g., living room, bedroom, bathroom, kitchen, closet).
(2)  Developed a solution for semantic segmentation of point clouds. To facilitate the process of generating solid models from point clouds, the team developed a method for semantically segmenting the main components of indoor scenes in the point cloud of a residential building. During Phase 1, the team developed classifiers to identify points representing components such as beams, ceilings, columns, floors, indoor objects, pipes, walls, and windows.  
(3)  Developed a solution for instance segmentation of point clouds. Pearnets' semantic segmentation method returns the class that every point in a point cloud of an indoor residential scene represents (e.g., ceiling, door, floor, wall, window). To use the semantic information for generating the solid model, Pearnets developed a method for segmenting the point cloud into meaningful subsets. This method divides the classified points into subsets where each subset represents one component (e.g., Wall1, Wall2, Floor1).  
(4)  Developed a solution for 3D solid model generation. The team developed a method for generating solid residential building models from point cloud data. The method takes the point cloud instances and returns the structural elements' solid representation. Pearnets received point clouds from different equipment (e.g., highly accurate laser scanners, iPhone lidar sensor, Azure Kinect DK camera). The method reduces the noise in the point cloud before generating the elements model.
These innovations are already being used in several homes and apartments. These technologies will help to reduce the cost and complexity of generating and updating the model of residential buildings.
CAREER: A Scalable Occupant-Driven Energy Optimization System for Commercial Buildings
This project seeks to fundamentally change how people live their everyday lives towards a more environmentally responsible and sustainable future and has the potential to play a major role in reducing our reliance on fossil fuels and combating climate change. This research will help enable individuals to reduce their personal energy usage in indoor commercial building settings by providing real-time visibility into the energy cost of each action. This will enable energy-accountability in commercial buildings by providing actionable recommendations with quantifiable savings, as well as insights for both occupants and building managers so that they can act in a timely manner. These small savings can add up: even a small 1% reduction in commercial building energy consumption translates to 1.5 billion dollars of annual savings for the nation. This project will help advance research at the intersection of building-scale systems, Internet-of-Things, wearable systems, and recommender systems. Course modules developed on embedded systems, mobile computing, Internet-of-Things, and deep reinforcement learning will be used to train undergraduate and graduate students.

This proposal aims to significantly reduce energy consumption in commercial buildings by examining each occupant?s unique and individualized energy usage, or ?energy footprint?, and providing occupants with actionable and measurable energy-saving recommendations. The proposed system comprises of several components, including real-time sensing and actuation, building energy monitoring, indoor localization, large-scale time-series data analytics, and recommender systems. This project is organized into three research thrusts: (1) develop a digital twin model of a commercial building to simulate energy savings from human-driven actions, and research efficient algorithms for computing each occupant?s ?energy footprint? in shared environments. (2) advance knowledge in deep reinforcement learning and design a recommender system to discover actions that have the best potential for saving energy while adapting to user preferences. (3) investigate effective feedback mechanisms and incentive schemes to encourage energy saving behaviors, increase recommendation quality, and improve recommendation acceptance rate.
SCC-PG: Coordinated Safety Management Across Smart Communities
This planning grant enables research at the University of Florida (UF) and the City of Gainesville (City) to guide UF and City communities on how best to integrate and coordinate safety-relevant data, decision-making, and protective interventions. A joint UF-City strategic plan, among other objectives, anticipates the deployment of smart sensing and adaptable infrastructure for data-informed coordinated management of community safety. UF and the City are deploying smart-lighting infrastructure on multiple streets across UF and neighboring streets which also provides the scaffolding needed to deploy sensors of several kinds. Multiple scenarios can be envisioned for safety enhancement, including talk-down speakers for warnings or instructions, gunshot/active shooter and duress sounds detection, remote image/video capture and light and mobility management. It is anticipated that human sensors (e.g. people using smartphones) will provide information via social networking platforms, home-security systems and safety apps. Data generated from social networks can complement data from infrastructure sensors with early warnings and help implement and evaluate interventions to address potential risk areas and actual safety events.

Several multidisciplinary questions are being considered in the context of the project. How can data from UF and City locations be streamed, correlated, integrated, analyzed and visualized jointly by separate safety management entities? How can safety incidents be prevented and/or detected in real time from observed data patterns? How can data from infrastructure sensors and human sensors be integrated? How can humans (managers and citizens) access, visualize and interact with data analytics systems used to extract and present safety information? How are safety, security and privacy issues collectively addressed? How can distributed computing systems be architected to support computing, storage and network needs of such workflows across a smart city infrastructure, including devices owned by users in the community? The anticipated outcomes of this project include (1) well formulated research problems that integrate socio-technical perspectives in the context of UF-City communities, (2) identification of academic and community partnerships with the necessary knowledge to address the problems and (3) preliminary work and testbed designs to be included in a future SCC-IRG proposal to address the problems.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This planning grant has enabled research at the University of Florida (UF) jointly with the City of Gainesville (City) to guide UF and City communities on how best to integrate and coordinate safety-relevant data, decision-making, and safety-protection interventions. In this context, the specific goals of the project include the exploratory research, interactions with stakeholders, experimental testbed creation and other preliminary work needed to build the necessary capacity, research foci and stakeholder engagement to conceptualize a larger Integrative S&CC research effort. This project has introduced and bonded multi-disciplinary UF faculty from architecture, urban planning, computer engineering, criminology/behavioral science, and virtual reality with each other and UF and City of Gainesville experts creating significant capabilities and spawning side collaborations. The major activities and outcomes under this project fall under the following categories: 
Initial development and deployment of a living laboratory (called the SaferPlaces lab) for instantiation and study of safety-related city and campus scenarios. The laboratory, located near campus and downtown Gainesville, occupies several rooms in a building, where command center, dashboard, monitoring devices and visualization capabilities are available. The building surroundings within its block are also part of the laboratory, including several instrumented, intersecting car lanes and parking areas with smart lighting, cameras and other sensing and communication devices. 
Early research into predictive modeling and edge-computing analytics for safety assessment and built environment adaptation. The team has researched approaches to support simultaneous execution of multiple models in resource-constrained edge devices, and evaluated the performance of a container-based orchestration platform (Kubernetes) in edge devices across edge virtual private networks to assess the viability of supporting unmodified edge applications. ?human sensor? data collection efforts from multiple social networking platforms and City?s databases. The data sources include Twitter, NextDoor, Google Reviews, Google Street Imagery, crime reports from Gainesville Police Department, spatiotemporal environmental and weather data from state and national open databases among others. These data have been used to develop preliminary deep-learning models to map social and environmental factors to urban safety outcomes (such as crime rates and safety perceptions).
Initial development of a virtual-reality "digital twin" of a retailer's parking lot and its use by customers under varying lighting conditions. A virtual-reality environment has been developed to mimic the parking lot of the SaferPlaces living laboratory. It has been used to collect safety perceptions by a representative number of Gainesville residents and UF students.
Initial research on public sentiment regarding the impact of lighting on the public perception of safety. A preliminary study regarding street lighting and Gainesville residents? perceived safety at night has been conducted in the context of a parking, aiming to assess the impact of lighting intensity and light color temperature on people?s perception of safety. 
 Meetings amongst the PIs and staff and/or representatives of UF Security department, UF Facilities, UF Office of the Chief Operating Officer, UF Facilities, Gainesville Technology Department, Gainesville Smart City Coordinator and Gainesville Economic Development Department. 
Broader Impact
The SaferPlaces laboratory is being used by students, faculty and practitioners to study issues related to safety. Data-driven cyber-infrastructure for digital data collection across platforms and data modalities has also been developed. The project has contributed to the education of two STEM PhD students from urban planning, one of which is a female, who have a commitment to pursue academic careers in the areas of smart cities and urban analytics. The project has also enriched the research agenda of the junior faculty involved in the project and promoted truly interdisciplinary research across disciplines and backgrounds. The virtual-reality environment and associated models developed in this project are published on online platforms with over two million users and available for free download and public viewing on virtual-reality headsets. Students, faculty, and other researchers can use this virtual-reality environment to study the relationship between environmental features and safety perception. 

RAPID: A Computational Model for Multiscale Investigation of Regional Lung Dynamics
This Rapid Response Research (RAPID) grant will offer new fundamental insights into the mechanical aspects of breathing and mechanical ventilation based on imaging data from hospitalized COVID-19 patients. While mechanical ventilation is often used, our scientific understanding of how it works is sparse. A multidisciplinary team of engineers and physicians will work together to better understand how the lungs respond to mechanical ventilation. This work will use imaging data from COVID-19 patients to create detailed computer models of the infected and inflamed lungs. These patient-specific computer simulations will enable mechanistic understanding of lung function in healthy and diseased states. The virtual lungs created in this study will then be used to simulate a digital twin of the COVID-19 infected lungs on mechanical ventilation. Thus, our fundamental knowledge of the mechanical aspects of lung disease and the physical response to ventilation will be advanced. Furthermore, the computer models developed in this project will be used as instructional material for multiple university courses. This will introduce low-income and first-generation college students from rural communities to cutting-edge modeling and simulation techniques. This experience can have a profound impact on these students' interest in foundational research and career goals.

Ventilation and respiration events in the lungs span multiple length and time scales. Furthermore, these events involve multiple physical phenomena, including deformation and motion of the lung parenchyme, fluid-solid interaction between air, airway walls and the alveoli, and finally diffusion and gas exchange with the blood flowing through the alveolar capillaries. While modest progress has been made towards understanding the mechanics of the lung at the macroscale, our knowledge of the microscale and mesoscale mechanics of the lung at the level of alveoli, the smallest functional unit of the lung, remains rudimentary. Our proposed research on multiscale and multiphysics computational modeling of the lung in a disease state, based on CT imaging in COVID-19 subjects, offers the potential to advance our fundamental and mechanistic knowledge of lung dynamics. Understanding and exploring the mechanisms of reduced alveolar diffusion in COVID-19 patients through patient-specific multi-physics and multiscale modeling can advance our knowledge base and provide new insights into the role of mechanics in lung inflammation. Furthermore, multiscale simulation of the lung using an acute inflammation model and CT data can inform and educate about the science of inflammation-induced changes in the lungs and the alveolar-capillary mechanics.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
In this project, a multidisciplinary team of engineers and physicians worked together to better understand how breathing dynamics and lung mechanics are impacted by COVID-19 infection. This work used imaging data from COVID-19 patients to create detailed computer models of the infected and inflamed lungs. These patient-specific computer simulations enabled a deeper understanding of lung biomechanics in healthy and diseased states. The virtual lungs created in this study were also used to simulate a digital twin of the COVID-19 infected lungs on mechanical ventilation. Thus, the project advanced our fundamental knowledge of the mechanical aspects of lung disease and its physical response to mechanical ventilation.
Therefore, in terms of the intellectual merits, our project on multiscale and multiphysics computational modeling of the lung in a disease state, based on CT imaging in COVID-19 subjects, advanced the fundamental and mechanistic knowledge of lung dynamics. Understanding and exploring the mechanisms of reduced alveolar ventilation in COVID-19 patients through patient-specific modeling provided new insights into the role of mechanics in lung inflammation. With respect to the broader impacts of the project, the computer models developed in this project were used as instructional material for multiple undergraduate and graduate university courses. Therefore, the project introduced students belonging to underrepresented social groups in STEM, including first-generation college students from rural communities to cutting-edge modeling and simulation techniques. The research and educational activities resulting from this project had a profound impact on these students' interest in foundational research and career goals as evidenced by two of the students joining a prestigious PhD program in Biomedical Engineering program after working on this project and a student receiving the prestigious and highly competitive NSF Graduate Research Fellowship in the course of the project.
Furthermore, multiscale simulations of the lung using an acute inflammation model and CT data informed and educated a wide audience about the science of inflammation-induced changes in the lungs and the alveolar-capillary mechanics. The results of the projects were presented at several national and international conferences and were published as journal papers. East Carolina University serves a large population of low-income and first-generation college students from rural communities. Exposing this student population to the data and modeling techniques of this study had a profound impact on developing their interests in STEM and foundational research. Hence the project fostered the integration of research and education supporting NSF's core strategic objectives.
SBIR Phase I: Reinforcement Learning for Guidance and Control of Spacecraft
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is a coupled guidance and control flight software solution that enables multi-spacecraft systems to be truly autonomous in their station-keeping and self-distribution. The proposed innovation is for a deep reinforcement learning (DRL) agent to learn how to autonomously determine and command maneuvers that yield a desired spacecraft formation. The proposed project could reduce customers? cost of operations by removing humans from the closed loop control system. The proposed innovation scales to systems of large numbers of spacecraft without scaling the cost of flight operations. Other potential benefits to customers are risk reductions: a DRL agent does not require an exact model of spacecraft subsystems and orbital dynamics and can learn in real-time, thus being robust to off-nominal system performance and unexpected perturbations. Benefits of this project may include a DRL agent discovering novel guidance and control solutions for mission designs that are not known from legacy orbital dynamics approaches. Potential broader societal impacts include enabling Deep Space Gateway operations and science missions to sample in-situ, simultaneous measurements over a large area, resulting in valuable science data returns for research or commercial applications in Earth orbit or deep space.

This Small Business Innovation Research Phase I project will demonstrate the technical feasibility of implementing DRL as a solution for truly autonomous spacecraft guidance and control. The challenge motivating this project is the control of multi-spacecraft systems, where maneuver planning is neither intuitive nor straightforward due to the nonlinear equations of relative motion. Historically, solutions are found by making simplifying assumptions of circular orbits and linearized equations of relative motion. Such assumptions are avoided in this research plan. The primary research objective is to train a DRL agent using the high-fidelity model of NASA?s General Mission Analysis Tool (GMAT). First, the problem of achieving a particular formation or distribution will be formulated as a Markov Decision Process. Next, software infrastructure will be developed using TensorFlow, Python, and GMAT. Within this framework, the DRL agent will be trained to learn a policy that maneuvers the spacecraft into a specified formation, subject to operations constraints like available propellant and actuation limits. Anticipated technical results include comparisons of on-policy versus off-policy approaches to achieving coordinated spacecraft mission, demonstration of the technical feasibility of DRL-based flight software for guidance and control, and a characterization of the limitations of DRL-based control.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The objective of the project was to design, train, and test a Deep Reinforcement Learning (DRL) agent to achieve swarm formation flying missions. DRL is a type of Machine Learning (ML) where we seek to train an agent to achieve a goal. We followed the standard Markov Decision Process framework and designed a spacecraft simulation test environment, DRL agent algorithms, and overarching Jupyter notebooks that managed training and testing our models. We designed and implemented several reward signals for various swarm mode types, ranging from a simple target-chaser mode to a more complicated “fishbowl” swarm. The software modules and process flowchart that we designed are shown in Figure 1. The pseudocode flowchart for how the DRL agent gains experience in the simulated environment is shown in Figure 2 and the pseudocode for how models are trained and tested is shown in Figure 3.

Central to this project was the design and implementation of the DRL agent itself. Feedback from our customer interviews dictated the use of deterministic policies for predictable actions. We applied two state-of-the-art actor/critic network models: Deep Deterministic Policy Gradient (DDPG), and Twin Delayed Deep Deterministic Policy Gradients (TD3). Both model types were tested extensively and hyperparameters were tuned via ablation studies. We followed software best practices including: GitHub repo for version control of source code, adherence to a coding standard, documentation, package management with Conda, etc.

Analysis of our prototype DRL models confirms that the agent is able to “learn” the maneuvers to complete a Hohmann transfer orbit. While this orbit has a well known solution in orbital mechanics, it demonstrates that a machine learning algorithm “finds” the solution we know to be optimal, without prior knowledge of the physics of spacecraft dynamics or knowledge of the oblate Earth’s gravitational field. A Monte Carlo simulation study was performed to evaluate the performance of the trained agent. The initial location of the spacecraft on the starting circular orbit was sampled from a uniform distribution to conduct 50,000 simulation runs. Our results show that the agent is able to conduct the orbit transfer within the tolerances set during training 100% of the time. This gave confidence that our framework and source code is appropriate for studying more complex spacecraft swarm guidance and control scenarios.

The fishbowl swarm configuration is defined as multiple spacecraft moving around a defined “center” spacecraft, where the spacecraft can drift in relative motion but must stay within a defined radius from the center of the swarm. The spacecraft must make maneuvers to stay within this defined virtual boundary, but cannot move so close to another spacecraft as to risk collision. We performed numerous ablation studies to optimize the performance of the DRL agent to control a fishbowl swarm, both with the DDPG and TD3 approaches. Results of our initial ablation studies demonstrate that our first prototype guidance and control models have success for simpler spacecraft swarm configurations. As the number of spacecraft and constraints increase, however, simulations show the agent has a more difficult time “learning” the desired goal. Training can be unstable or take millions of experiences to gain traction. Applying state-of-the-art techniques, such as normalization and hindsight experience replay, may improve performance, but extensive ablation studies are still required to find the optimal hyperparameters and network architectures to manage larger spacecraft swarm systems.

Another outcome from this project included clearer understanding of customers' needs. Customers want more than a modular guidance and control software system; they are looking towards future missions where the multi-spacecraft system is “truly intelligent,” where the system can self-diagnose problems, be self-reliant, react to real-time in-situ conditions, coordinate to perform a commanded task, and be commanded by other AI-based models to optimize data collection, all without intervention from human flight controllers on Earth. Customers are not interested in discussing the many interesting and novel algorithms that exist only in the research/academic space. Rather, customers require that ML solutions are demonstrated to perform on the computationally constrained compute platforms of CubeSats. Therefore, performance for any ML product we develop must be demonstrated on a “digital twin” with testing, validation and verification in high-fidelity simulation.

Our assumption that customers wanted a software application that runs autonomously onboard the spacecraft was incorrect. There are several use cases where customers would be happy with an autonomous guidance and control system that runs on computers on the ground (i.e. at the mission operations center). The tool could be used throughout the mission design and planning process, and then run on the ground during real-time mission operations. In fact, humans-in-the-loop would be acceptable for initial verification or even for the delivered product, if it meant that the maneuver planning process could be sped up faster than what is required today.
FW-HTF-P: Design of Tools and Technologies for Industry 4.0 Manufacturing Work
As information technology has matured, a Fourth Industrial Revolution?often referred to as Industry 4.0?is emerging. Industry 4.0 aims to use technology to integrate design, manufacturing, and consumer activities seamlessly, resulting in increased productivity, reliability and customer satisfaction. However, the process of transitioning to Industry 4.0 can be challenging to both companies and workers. Companies cannot stop operating while their production systems and processes are being updated. Due to the cost of the technology and the need to train the workforce, the transition may need to take place over a period of years. Workers who cannot adapt to the new technologies and processes risk losing their jobs. This project will bring manufacturing industry representatives and researchers together to develop a plan for future research. The research plan will lay the groundwork for development of tools and technologies to make the transition to Industry 4.0 more straightforward and cost-effective for companies and more successful for workers. Examples of possible research topics include road maps for transitioning from conventional manufacturing systems to Industry 4.0 systems; tools for ergonomic design of workstations and assembly lines; cognitive virtual assistants for workers; guidance for human interface design; and automated generation of knowledge and skill ?crosswalks? to help companies identify existing skills that will still be needed and new skills that workers need to learn.

The proposed planning grant aims to develop a research agenda related to the implementation of Industry 4.0 within the manufacturing industry. The work domain is manufacturing; sub-domains could include smart design, smart machining, smart assembly, smart monitoring, smart control and smart scheduling. The workplace is the factory, which could include production lines, engineering design, and inspection and testing areas. The workers include the engineers and operators who work in these areas. The proposed planning activities will focus on gaining a better understanding of industry needs which, together with a convergent research approach, will lay the groundwork for development of tools and technologies that can enhance the human-technology partnership and augment human performance in Industry 4.0 manufacturing work. Major tasks will include: (1) survey Industry 4.0 manufacturing system development, applications, and constraints; and (2) develop a convergent research agenda in close collaboration with representatives from a variety of manufacturing sectors and researchers with expertise in manufacturing engineering, human factors, human resource development, sociology, and computer science. Intellectual merit: Although Industry 4.0 has many potential benefits, at this time, the process of transitioning from a traditional manufacturing system to an Industrial 4.0 manufacturing system is expensive and fraught with risk to both the company and its workers. The proposed convergent approach will allow scientists to learn from plant technical managers about the operational challenges of implementing Industry 4.0 within their companies. Involving researchers with diverse areas of expertise will provide a richer understanding of the issues and convergent approaches to problem-solving. Broader impacts: Industry 4.0 is projected to add $2.2 trillion to domestic GDP by 2025. The value of the operational transformation to the global manufacturing industry is estimated to be $3.7T/year. To remain competitive, manufacturers need to be able to rapidly adapt to changing markets, which requires having a well-prepared workforce. The proposed planning activities will identify tools and technologies that could be developed to enable successful implementation of Industry 4.0, and thereby enhance work environments, position workers to be successful at their jobs, and increase competitiveness of U.S. companies.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
As information technology has become robust and mature, developed countries such as Germany have advanced the concept of the Fourth Industrial Revolution, often referred to as Industry 4.0. The intent is to integrate design, manufacturing, and consumer activities seamlessly to increase productivity, reliability, and customer satisfaction. Industry 4.0 is projected to add $2.2 trillion to domestic GDP by 2025 and the value of the operational transformation to the global manufacturing industry is estimated to be $3.7T/year. To remain competitive, manufacturers need to be able to rapidly adapt to changing markets, which requires having a well-prepared workforce; 27% of U.S. manufacturers report they are not able to expand production due to lack of a properly skilled workforce. This challenge is exacerbated by a declining baby-boomer workforce, with an estimated 10,000 boomers retiring each day through 2029. The transition to new technology could potentially lead to current workers losing their jobs if they are unable to adapt.
Although Industry 4.0 has many potential benefits, the process of transitioning from a traditional manufacturing system to an Industrial 4.0 manufacturing system is not straightforward. First, companies are not able to completely stop work while their production systems and processes undergo transition--the show must go on. Second, transitioning to new technologies is expensive; in many cases, changes in work sequence, workstations, and workforce training will need to be incremental, and development of new tools and gadgets will take place over a period of years. Third, there is no one-size-fits-all Industry 4.0 solution. Different industries have different needs, and even companies in the same industry may have different needs based on factors such as size and location.
This FW-HTF planning grant developed a research agenda related tools and technologies to enhance the human-technology partnership and augment human performance in Industry 4.0 manufacturing work. To develop this agenda, the project team surveyed 71 companies, led seven workshops focusing on I4.0 implementation in industry reaching out to 55 companies, reviewed I4.0 technology implementations by industry, led five workshops focusing in I4.0 research agenda development that reached out to 65 academic researchers, and conducted seven individual interviews with industry.
The team found that Industry 4.0 technologies are most often used by large corporations. Small companies face significant barriers to entry including cost of implementing new technology, the need to redesign production systems and processes, and inability to stop work while production systems and processes undergo transition, and inability to hire/retain skilled workers. planning tools such as a road map of the process for transforming a traditional manufacturing system into a cyber-physical system with the least amount of disruption to production and to the workforce, methods for calculating the trade-offs between the additional resources that will be needed in smart factories and the potential generated savings, and tools to help decide which I4.0 technologies to implement would be helpful. Workers need to be able to use computers and new technological tools such as smart glasses. Training is needed both to help current workers to update their skills and to prepare future workers. Technical skills and critical thinking skills are particularly important.
Over 50 potential research topics were proposed, relating to five Industry 4.0 technology areas: augmented/virtual reality, digital twin/cyber physical systems, collaborative robots, Internet of Things, and data analytics/machine learning. The topics relate to making these technologies easier to use, more easy to plug-and-play with existing system and processes, more secure (both in terms of physical safety and protecting information from cyber-attacks), more transparent, and more reliable and powerful. In addition, barriers to acceptance such as lack of training, fear of losing one’s job, and lack of understanding of the benefits of the technology need to be addressed.
The research agenda developed from this project will increase opportunities for convergent multidisciplinary collaborations. The impact will extend beyond the research community, to manufacturing industries, and ultimately the economy as whole.
NSF Convergence Accelerator Track D: Rapid Development of Intelligent, Built Environment Geo-Databases Using AI and Data-Driven Models
The NSF Convergence Accelerator supports use-inspired, team-based, multidisciplinary efforts that address challenges of national importance and will produce deliverables of value to society in the near future. The broader impact and potential societal benefit of this Convergence Accelerator Phase I project is to achieve what is seen as the Holy Grail by the Built Environment research community and industry. Infrastructure designers and planners increasingly require detailed three dimensional (3D) geospatial databases of the Built Environment, in order to consider human scale perceptions, which today are often overlooked but critical to improving cities in order to better meet the needs of its citizens and other emerging technologies, such as autonomous vehicles. The impact on society of applying Artificial Intelligence (AI) to rapidly create secure and intelligent 3D models of the Built Environment, including horizontal (e.g., highway, bridges) and vertical (e.g., buildings, plants) facilities, by turning raw data into actionable information cannot be overstated. However, to-date, assembling and maintaining those databases has been too expensive for even the most progressive cities due in large part to the cost of manual feature extraction. Built Environment data that is accurate and reliable is critical for the well-being of communities as such data is used for a variety of purposes including emergency preparedness, asset operations, maintenance, public safety, and more. A convergent, innovative team will be formed with industry partners to ensure that the knowledge developed through this research effectively transitions into many aspects of practice. From the outset, and through both phases of the project, the team is taking steps through team building and intentional engagement with a variety of stakeholders to broaden the scope of potential impacts of the proposed innovation. Specifically, the research and implementation activities will be designed to consistently include dialogue and invite input from a broad range of interests, intentionally seeking involvement with segments of the public that are traditionally left out or neglected in technology implementation endeavors. This research also implements activities for workforce development in computer vision and geomatics, which currently has a large gap between employment needs and a workforce of appropriately skilled personnel, particularly from underrepresented backgrounds.


Current workflows and procedures to develop 3D models of the Built Environment require substantial manual effort. Those processes that are automated are limited to small datasets that are not representative of the current point clouds and other data being acquired or needed for Building Information Modeling (BIM). They are also limited in the types of objects that can be modeled. To this end, the interdisciplinary research team will work with stakeholders to develop a more holistic Scan-to-BIM process. Phase I of this project has two primary goals: (1) provide a scan-to-BIM validation tool by compiling a sizable collection of benchmark datasets with annotated point cloud scans and corresponding BIM models, creating a prototype validation server with metrics related to parameters of interest to stakeholders (e.g., evaluate the accuracy of modeled door widths, which are important to ADA compliance assessment, or evaluate these models for urban renewal, redevelopment projects), and create and host a Scan-to-BIM challenge for researchers all over the world to participate, and (2) develop a prototype tool to implement a holistic Scan-to-BIM framework to rapidly and reliably generate BIM models from scan data that can be used not only to facilitate the development of the benchmark datasets but also used by stakeholders. Based on the research resulting from these challenges, the project team plans to build a comprehensive cloud-based service for Scan-to-BIM, which will be deployed to serve the Architectural/Engineering/Construction (AEC) community, and ultimately the public in general as these models can decrease construction or renovation project costs of public infrastructure funded with taxpayer?s money. Users would be able to select desired algorithms for key stages of the Scan-to-BIM framework based on their performance for specific applications and use cases.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Architects, Engineers, Contractors, and Owners need reliable 3D digital models of the physical world to render informed decisions regarding the planning, design, construction, and management of the Built Environment. The rise in 3D sensor technologies such as lidar and 360 cameras have enabled the rapid collection of rich 3D data but transforming this data into useful 3D models is tedious, time-consuming, and expensive. Our prototype AI-based InstaTwin technology developed through the Convergence Accelerator Program automatically segments, classifies, and extracts real-world features from 3D data to create digital representations of reality to address the barriers of creating 3D models.  Furthermore, the AI components of InstaTwin have broad applicability and far-reaching impact on equity, energy, and sustainability. 
Buildings account for 40% of the world?s energy consumption primarily because older buildings are not efficient. Transitioning old building stock to energy-efficient buildings is an essential component of reaching global ecological sustainability. To achieve this transition, we need better ways to assess, analyze, monitor, renovate, and alter the behavior of buildings in real-time to optimize and reduce the use of fuels for heating, lighting, and cooling. Fundamental to this idea is having a digital representation of a building.
The InstaTwin platform we developed during this project enables 3D models of the Built Environment to be created through a digitization process of the physical world. The process consists of collecting 3D data from sensors such as laser scanners (i.e., lidar) and 360 cameras and the converting these data into 3D objects. The resulting 3D model is sometimes called a ?digital twin? or building information model (i.e., BIM).
To better understand this digitization process, consider the process of flatbed scanning, where a flatbed scanner captures a pixelized image of a physical document. The captured image is helpful as a digital record; however, the true value of the scan is the words contained within the image. Transcribing these words manually is a time-consuming process, and technologies such as Optical Character Recognition (OCR) significantly increased the accessibility to and utility of the information in the scanned document.  
Similarly, scanning a building is analogous to scanning a document. Instead of a flatbed, however, a 3D laser scanner spins around on a tripod and measure the contents of a building. The resulting scan data, called a ?point cloud,? is useful as a digital record, but the true value is the objects and building elements contained within the scan. Transcribing these objects is considerably more time-consuming than the transcription of words and, as of today, there exists no equivalent to OCR for the creation of BIM.
InstaTwin is a prototype platform that automatically generates 3D building models from scans. This conversion is an incredibly difficult problem since buildings can be very complex, but with the power of AI, InstaTwin empowers asset owners, architects, engineers, and facility managers to make better, faster, and more efficient decisions about the Built Environment. Fundamentally, InstaTwin is a data repository and processing framework that can be accessed online. Collaborators and users submit data, test algorithms, process data into 3D models, view results, and download BIMs. Implicit in our design is a framework that can provide immediate industry benefit and thus incentivizing users to supply data, train models, and support its continued development.
To develop the prototype Insta-Twin platform, the team participated in the extensive NSF accelerator training course to learn effective strategies for convergent research and ideation. They also conducted several interviews of industry professionals to identify key bottlenecks as well as determine how it would integrate within their workflows.  Several of these professionals attended ideation workshops and provided feedback on the prototype.  In parallel to developing the prototype platform, the research team compiled a vast database of point cloud data with corresponding building information models and CAD floorplans. These data were shared for a challenge competition at two workshops focused on converging the computer vision, architecture, engineering, and construction communities.  
MRI: Acquisition of a Shared High-Performance Computing System for Cyber-Enabled System Design
This instrument, a new high-performance computing (HPC) platform, aims to accelerate knowledge discovery by supporting the research of 32 investigators from 15 departments. Building upon an institutional focus on simulation- and data-driven science, it will enable significant advances in two types of research, foundational and translational, and support training and development. Leveraging the HPC cluster, investigators in computer science, engineering, statistics, agriculture, and biology will conduct research on cyber-enabled systems design, thermal flows in the built environment, the design of materials, and phenotyping methods for plant and animal agriculture. This HPC cluster will enhance existing collaborative efforts and enable building new ones, support advanced student training in computational modeling and algorithm development, and allow sustained workforce development.

This HPC system for Cyber-Enabled System Design includes a mix of computing nodes and nodes with general-purpose graphics processing units (GPGPUs), with sufficient memory, disk storage and parallel input/output capability to support cutting-edge research. The instrument will enable: (a) foundational data science advances with a focus on algorithms for system design under uncertainty, for cyber-physical data assimilation, and analysis of complex flow and material systems, (b) translation research in three thematic areas: energy, materials and manufacturing, and precision agriculture, with a focus on ?materials by design?, smart power grid, quieter aircraft and wind turbines, and precision animal science, and (c) HPC training for undergraduate, graduate, and postgraduate students, to prepare a globally engaged, HPC aware workforce. Research using this computer system has the potential to improve food production, energy usage and manufacturing.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Acquisition of a Shared High-Performance Computing System for Cyber-Enabled System Design
 
The NOVA-Extension machine was to enable ISU scientists and researchers to make significant advances in ISU’s signature areas with the following themes.
 
foundational data science advances with a focus on cyber-physical system design algorithms under uncertainty.
translation research in Energy, Materials and Manufacturing, and Precision Agriculture with a focus on material by design.
HPC research, training, and development that prepares an HPC-aware workforce.
 
Selected science outcomes enabled by NOVA-Extension are described below.
 
Efficient Hardware-Aware Neural Architecture Search (NAS) and Benchmarks. The end-to-end design of a convolutional neural network (CNN) is a challenging and time-consuming task requiring expertise in multiple areas and knowledge of different hardware platforms. We automate the architectural design process of deep neural networks (DNNs) to alleviate human effort and generate efficient models with acceptable accuracy-performance tradeoffs. The goals are to minimize the need for significantly large computation resources for training and fair evaluation of various search methods. NAS Benchmarks can provide critical insights and a deeper understanding of the design process. The benchmarking process lays the groundwork for future advancements in this area.
 
Multigrid Distributed Deep CNNs Solvers. We develop deep learning (DL)-based methods on high-resolution 3D geometries to predict the optimized topology. We use a multi-resolution CNN approach, where we train the network at a lower resolution in a distributed manner and then transfer the learned network to continue training at a higher resolution. Our scalable framework integrates two distinct advances, accelerating training a large model and implementing a distributed deep learning framework which significantly reduces the training time. We prove theoretically and perform experiments for convergence to suggest that a mesh-based neural network approach is a promising approach for solving parametric partial differential equations.
 
MDPGT: Momentum-Based Decentralized Policy Gradient Tracking. We developed a novel policy gradient method for multi-agent reinforcement learning. Specifically, we proposed a momentum-based decentralized policy gradient tracking (MDPGT) to approximate the local policy gradient surrogate with importance sampling. Our novel discrete-time dynamical system-based algorithm aims to find the saddle point of a min–max optimization problem in the presence of uncertainties. We support our theoretical findings through empirical results on a multi-agent reinforcement learning benchmark environment. The code is available on GitHub.
 
Deep learning-based multigrid topology optimization of manufacturable designs. We develop a novel framework for enhancing the accuracy of additive manufacturing simulations, particularly in extrusion-based processes, using a digital twin of the dynamic printing process through physics simulations with intermediate print geometries. Effectiveness is achieved using DL-based approaches to generate high-resolution topology-optimized 3D geometries. Our results confirm scalability to high voxel resolutions, leading to a foundation for building real-time digital twins. We also address statistical quality assurance by performing the similarity evaluation leveraging high-density data to improve the quality and repeatability of additive-manufactured parts.
 
Precision Agriculture. Computing on language-based descriptions of gene function and phenotype relies on the use of controlled vocabularies and strict graphs called ontologies.  We have advanced tools for computing gene function and phenotype.  For gene function, our system predicts functions more accurately for plant genomes for over twenty crop genomes.  Our methods also allow researchers to compute directly on phenotype descriptions directly using natural language processing to recover biologically relevant associations to collect and document phenotype data in human-readable, highly technical terms.  We also developed a probabilistic voxel carving algorithm to efficiently reconstruct 3D models of maize plants and extract leaf traits for phenotyping robustly. Our approach shows that 3D reconstructions of plants from multi-view images can accurately extract multiple phenotypic traits, enabling better plant breeding programs.
 
Developing an insect classifier using Self-Supervised Learning and citizen science data. We developed an advanced insect classification system, InsectNet, through a pipeline of data collection and selection of appropriate pretraining strategies. The classifier achieved an impressive accuracy (96%) across the diverse insect species. We successfully integrated cutting-edge DL techniques, large-scale datasets, and high-performance GPUs, leading to the creation of a robust insect classification tool with practical applications in the agricultural sector.
 
Design of ultrasonic bat deterrents. The deterrents are based on the idea of aerodynamic whistles wherein ﬂow-acoustic resonance is used to produce high-amplitude tonal sound at desired ultrasonic frequencies. The passive deterrents are designed to be mounted on wind turbine blades and are “powered” by the energy in the air moving past the rotor blades. Two prototypes were fabricated and acoustically tested for stability.
D-ISN: TRACK 2: Using Enterprise Network Models to Disrupt the Operations of Illicit Counterfeit Part Supply Chains for Critical Systems
Counterfeit parts have become widespread over the last 20 years, and their infiltration into critical systems poses a significant economic risk as well as a risk to human life and national security. Critical systems include those associated with human safety (e.g., transportation, medical), the delivery of critical services (e.g., infrastructure, energy generation), humanitarian and military missions, and global economic stability. These systems are expensive to procure, must be supported for long periods, and the consequences, if compromised by counterfeit parts, can be catastrophic. This project will develop methods of disrupting the counterfeit hardware networks for critical systems guided by socio-technical network development and modeling, and through consideration of the counterfeiting network?s connections to other illicit trafficking networks. Counterfeit hardware addressed in this project includes electronic and non-electronic hardware and hardware that is created via additive manufacturing. The work performed in this project will inform decisions and policies about the economy, security, and resilience of the Nation and the world concerning illicit supply networks for counterfeit parts.

The majority of the treatment of the counterfeit part problem to date is focused on the detection of counterfeits, which is necessary, but represents treating the symptom, not the cause. In this project, we will conduct a series of workshops that include stakeholders from industry, policy, legal, supply chain, government acquisition, industrial security, and anti-trafficking organizations. The workshops will be used to guide socio-technical network development and modeling, and include consideration of the counterfeiting network?s connections to other illicit trafficking networks. The technical attributes of the problem include: part flows within sustainment networks for critical systems, procurement and acquisition practices, counterfeit detection technologies, and inventory management. The socio-attributes include trust, communication (including significant counterfeit reporting stigmas), and incentive reactions. The network models developed will consider both commercial transactions and military acquisition systems to enable identification of possible points of disruption in those networks. The model for the counterfeit part network developed in this project will form an important and necessary enabler for possible future blockchain-based solutions and is an essential component of a digital twin for the supply chain for critical systems.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Counterfeit parts have become widespread over the last 20 years, and their infiltration into critical systems poses a significant economic risk as well as a risk to human life and national security.  Counterfeit parts are parts that are misrepresented to the customer and may have inferior specifications, quality, reliability, or contain knowingly malicious functionality. Counterfeit hardware can take many forms, it may be used (salvaged) parts misrepresented as new, re-marked parts, manufacturing rejects, parts manufactured during factory shutdowns, and others. Impacted critical systems include those associated with human safety (e.g., transportation, medical), the delivery of critical services (e.g., infrastructure, energy generation), humanitarian and military missions, and global economic stability.  These systems are expensive to procure, must be supported for long periods of time, and the consequences, if compromised by counterfeit parts, can be catastrophic.
The goal of this project was to develop methods of disrupting the counterfeit hardware networks for critical systems guided by socio-technical network development and modeling, and through consideration of the counterfeiting network?s connections to other illicit trafficking networks. The counterfeit hardware addressed in this project included electronic and non-electronic hardware and hardware that is created via additive manufacturing.
A team of University of Maryland centers consisting of critical-system sustainment (CALCE), public policy (CPPPE), terrorism/trafficking (START), and intelligence/security (ARLIS), along with stakeholders from industry (ECIA, ERAI, and Lockheed Martin), legal, and the supply chain, conducted four public workshops: Compromised Additive Manufacturing Supply Chain Workshop, Enterprise Network Models for Counterfeit Electronic Parts Supply Chains Workshop, Intersections of Illicit Trafficking and Counterfeit Parts Workshop, and Safeguarding Critical System Supply Chains Against Compromise Workshop.  These workshops served as the basis for developing methods for disrupting the counterfeit hardware network for critical systems and guided socio-technical network development and modeling. 
A preliminary enterprise network model for the electronic part supply chain, including the sources and modes of distribution of counterfeit parts was developed. The objective of the model is to capture the key participants in the supply chain and their connections (e.g., Figure 1). The resulting model can be used to assess the risk of various supply-chain disruptions and other relevant supply-chain events culminating in a quantification of the counterfeit risk to the final customer (e.g., the ?Operator? in Figure 1).  The network model allows the testing of policies through the following process. The network can be ?seeded? with events such as: a part discontinuance (obsolescence), short-term shortage, or changes in trade policies. The probability of counterfeit parts appearing and reaching the customer can be assessed. If counterfeit parts do appear, then the length of time after the discontinuance seed can also be assessed. These risks can be assessed with and without the application of policies like those listed above to assess the policy?s effectiveness quantitatively.
The work performed in this project will inform decisions and policies about the economy, security, and resilience of the Nation and the world concerning illicit supply networks for counterfeit parts.
 SCC-CIVIC-PG Track B Strengthening Community Resiliency through Extended Reality (XR): Engaging Small, Under-Resourced Municipalities in Planning & Execution of Government
Remote governing as a result of the COVID-19 pandemic creates the need for new approaches to effective collaboration and governance during crisis, particularly for under-resourced municipalities that typically have had limited access to novel technologies. The proposed research aims to determine whether Extended Reality (XR) is an effective technology for engaging under-resourced municipalities in disaster response planning and governing. By understanding the distinct needs of small municipalities and exploring the effectiveness and potential limitations of XR technology, this research aims to develop a pilot XR toolkit with potential for wider replication. This work has the potential to make meaningful cross sector contributions to advances in promising applications of XR, municipal governance, and disaster planning toward reducing disruptions in governance in future pandemics or other disasters.

While XR technology has been deployed in disaster preparedness and community planning, it has primarily been used by the federal government and states, counties, and cities. This work aims to explore the effectiveness of using this novel technology in small under resourced municipalities, particularly those with high levels of fragmentation. The proposed research pilot will engage leaders from 24 under-resourced municipalities in North St. Louis County, Missouri in the use of novel XR technologies for governing during pandemics or other disasters. Specifically, this project will introduce the technical capabilities and immersive virtual experiences of XR as a viable tool to assist in planning for, facilitating and maintaining government operations during disasters. Methods will include implementation meetings and concept mapping to determine municipal priorities to ensure the technology is focused and developed based on input of communities most impacted by the COVID-19 pandemic. Stage 1 will bring together government, industry, nonprofit, and academic stakeholders to identify community assets and vulnerabilities that may affect the acceptance and adoption of XR technology while jointly co-developing a full proposal for a Stage 2 research pilot. Stage 2 will jointly explore and test novel uses of XR technologies for effective governance during pandemics or similar disasters and identify potential applications of XR with the potential to reduce interruptions in government activities during times of disruption. This research aims to define the user requirements toward developing an XR tool kit that will allow small municipalities to plan for and respond to future disasters or pandemics. XR will enable municipalities to maintain city assets, plan public works projects, and maintain and improve public health and safety in real time. This project is supported by the CIVIC Innovation Challenge program Track B - Resilience to Natural Disasters: Equipping Communities for Greater Preparedness and Resilience to Natural Disasters through a collaboration between NSF and the Department of Homeland Security.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This study engaged leaders from the "24:1 Municipal Partnership" - a coalition of 23 under-resourced municipalities in North St. Louis County, Missouri - to envision and strategically plan for using Extended Reality (XR) technologies for governing during pandemics or other disasters. To our knowledge, this study was the first to apply a community-based participatory framework called Group Concept Mapping (GCM) in the smart and connected communities (S&CC) literature on resilience and disaster preparedness. First, the study empirically discovered shared priorities and core components of the 24:1 Municipal Partnership related to community health, safety, and recovery after times of crisis. Using an affirming, community-based approach, this study collaborative of this planning project leveraged trust and social capital and built a respectful and reciprocal partnership. Resident voices yielded a visual map of community priorities along the following core themes: community input; policing; reducing fragmented government supports; supporting families and parents; youth; and, community infrastructure. Second, this study specified an XR technological solution that validly and reliably aligned with community priorities. Digital Twin Technology (DTT) best captured the 24:1 leaders' design preferences and would provide the best mode of reaching community-driven goals that were expressed in the derived concept map. While many digital twin technological designs exist, applications that align with the expressed interests of the action team were selected: 1) build consensus for decision making; 2) understand the urban landscape in the 3rd (aerial views) and 4th (across time) dimensions; 3) crowdsource resident input and experiences to incorporate perspectives and experiences from groups that are often marginalized or less involved; and, 4) create simulations to allow municipal leaders to visualize and prepare for rare or unexpected events. This project makes important and timely progress in racial equity. A primary outcome of this project is increasing ownership and creative direction of technological solutions in under-resourced communities that have historically been left behind due to the technology gap and have limited exposure to state-of-the-art toolsets.
Ultra-high Precision Assembly of Aerospace Composite Structures: Fusing Physics-Based and Data-Driven Models
Composite structures have increasingly emerged for aerospace and other applications because of their high strength-to-weight ratio, good resistance to harsh environments, and great performance reliability. Because composite structures have nonlinear, anisotropic, and compliant properties as well as inherent manufacturing variability, conventional process modeling and quality control methodologies for metal structure assembling are not adequate to composite components. This award supports fundamental research that integrates physics-guided and machine-learning models to advance ultra-high precision assembly of aerospace composite structures. The research involves seamless integration of physical and digital product connections, data science, advanced statistics, and comprehensive manufacturing knowledge. The research has the potential to minimize the material loss, decrease the production flow time and achieve high productivity and quality control for aerospace manufacturing. The scientific findings from this project may also be extended to other composites demanding industries, e.g., automotive, spacecraft and solar energy, and thus, increase the global competitiveness of the U.S. industry. The interdisciplinary nature of this project will provide students with unique educational and research experiences and cultivate a diverse and qualified workforce cognizant with combined manufacturing and data analytics abilities. In addition, the project will develop new learning modules for an undergraduate core course, recruit and mentor underrepresented students, offer industrial short courses, and develop open-source software for precision assembly, all potentially leading to profound impacts to the society.

This research aims to develop fundamental knowledge and transformative technologies for ultra-high precision assembly of large complex-shaped composite structures by advancing physics-guided machine learning. Specific research activities include: (1) developing digital twin for ultra-high precision assembly of composite structures, (2) conducting physics-constrained active learning with safe exploration and efficient exploitation for experimental design and predictive modeling, (3) studying sparse machine learning for an optimal actuating strategy in composite structures assembly, and (4) analyzing theoretical properties of the methodologies. The project will research small-sized key aerospace structures as well as large-scale carbon fiber reinforced composite fuselage of ultra-high precision, e.g., less than 0.2 mm for a diameter of about 5 m. The methodologies will be computationally and experimentally evaluated in the verification and validation phase. The research outcomes will (1) expand the scientific understanding of engineering-driven data analytics and ultra-high precision quality control theory, (2) bridge the knowledge gap between predictive modeling, active learning, sparse learning, and composite structures assembly, and ultimately, (3) realize the effective all-inclusive integration of machine learning methodologies with advanced manufacturing.
SCC-CIVIC-PG Track A:CaReDeX: Enabling Disaster Resilience in Aging Communities via a Secure Data Exchange
Disasters over the years have shown that a disproportionate number of older adults suffer fatalities and injuries during extreme events. A large number of older adults live in age-friendly communities and senior health facilities (SHFs) that promote independent living. During a crisis, older adults are often unable to shelter safely in place or evacuate on their own due to a range of physical conditions (need for life-sustaining equipment, impaired mobility) and cognitive afflictions (e.g. dementia, Alzheimer?s). Seamless access to information about the living facilities (e.g., floor plans, operational status, number of residents) and about the residents (e.g.,health conditions such as need for dialysis, oxygen, personal objects to reduce anxiety) can empower first responders to improve response outcomes during disasters. Such information, typically held by caregivers and in the logs maintained by the organizations, is inaccessible and/or unavailable at the time of response. This proposal seeks to create a novel community contributed data-exchange platform, entitled CareDEX, that will empower organizations to readily assimilate, ingest, store and exchange information, both apriori and in real-time, with response agencies to protect and care for the elderly in extreme events. Using CareDEX, SHFs will be able to share information about an individual?s changing health conditions, their personalized needs and identify those in need of specialized triage and critical care. Given the sensitive nature of personal information, e.g. health profiles, ability status, CareDEX will incorporate techniques to balance the need for individual privacy with authorized release of information to responders when needed. The pilot 4-month planning effort will involve 2 workshops and a demonstration pilot study that will focus on bringing stakeholders from the emergency response and senior living communities to identify specific information needs, design protocols for collection and sharing of such data, and explore privacy/security concerns. The workshops and pilot will address simulated emergency scenarios, e.g a wildfire event requiring evacuation of seniors and a complex event when the fire occurs during a pandemic such as Covid19. The goal of the workshops would be to help identify the requirements which will guide the research, design, and development of CareDEX. The planning workshops will engage a diverse group of students and postdoctoral researchers and thereby contribute to the education of the next generation of citizens in interdisciplinary topics that are relevant to communities worldwide.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This planning grant pilot addressed a critical national need -- improving the resilience of our aging populations to large disasters.   Disasters over the years have shown us that a disproportionate number of older adults suffer fatalities and injuries during extreme events and natural disasters, from hurricanes to wildfires; morbidity and mortality is further increased in multi-hazard situations as we have experienced with the recent COVID-19 pandemic. Older adults living in retirement communities, senior health facilities (SHFs) and congregant living face specific challenges in disaster preparedness.  Despite progress in creating age-friendly communities that promote independent living and improve longevity, continuity of care and safety for older adults during disasters is often limited to high-level emergency plans.  During a crisis, older adults are often unable to shelter safely in place or evacuate on their own due to a range of physical conditions (need for life-sustaining equipment, impaired mobility) and cognitive afflictions (e.g. dementia, Alzheimer’s). The recent COVID-19 events have further exposed the vulnerability of older adults -  an added natural disaster during a pandemic requires planning that is both complex and multidimensional, and our pilot identified key areas to increase resilience for older adults. 
This 4-month pilot initiated an effort to create information preparedness to help build disaster resilience for our aging communities.  Seamless access to information about the living facilities (e.g., floor plans, operational status, number of residents) and about the residents and their changing situations  (e.g., health conditions such as the need for dialysis, oxygen, personal objects to reduce anxiety) is typically held by caregivers at the SHF, and can empower first responders to improve response outcomes and safe evacuations during disasters. Our driving vision in our pilot was to create situational awareness for first responders using community-contributed information using emerging IoT and big data technologies.  Acquisition and timely access of situational information about the facility (floor plans, operational status) and  its individuals ( health profiles, care instructions) can guide responders to triage, shelter-in-place or evacuate our seniors appropriately, transforming eventual outcomes. 
Building on past experience with disaster resilience, geriatric medicine and smart community efforts, this project launched a pilot effort by bringing together an interdisciplinary group of stakeholders (computer scientists, disaster response experts, experts in geriatric medicine and nursing, first response agencies, senior care facility partners) to address a data-centric approach to disaster resilience for the aging. The 4-month planning effort involved many focus group meetings, 2 workshops, and a demonstration pilot study, Key steps taken during the planning grant and insights that helped refine the CAREDEX vision are listed below. 
Multisector Needs Assessment through Workshops with First Responders and Congregant Living Partners.  2  workshops helped bring experiences and concerns from the emergency response and senior living side regarding specific information needs,  the need to design protocols for collection and sharing of care data, and techniques to mitigate privacy concerns.  
Design of an initial prototype digital twin for Walnut Village,  our partner older adult residential community, a Front Porch Senior Living community in Anaheim, CA.  The needs assessment workshops helped us crystallize key factors to guide the research, design, and development of CareDEX.  
Mock Scenario Design to validate the CAREDEX platform:   Working with our emergency response civic partners, the Fire Protection Research Foundation, we designed an initial simulated mock emergency scenario - a large WUI fire event with the evacuation of seniors.  In addition to illustrating information views that would be useful in this scenario, it enabled us to refine our next stage of development and testing.
Multisector team formation. We developed and enhanced connections to build a multidisciplinary research and civic partner team towards the next stage of the project. 
From a broader impacts perspective, the planning award illustrated the increasing need to protect older adults due to changing demographics in the US (and elsewhere) that points to a  significant rise in the population of older adults.  The potential for information preparedness through emerging Iot and data technologies in multi hazard settings is enormous -  from hurricanes, tornados, blizzards, tsunamis, power outages, and other unpredictable disasters both now and in the future.  The planning grant involved Ph.D., MS, and undergraduate students at UCI who gained experience related to disaster resilience for our vulnerable populations.
I-Corps: AI-enabled automation intelligence software that can detect micro-anomalies in machine and robotic operations
The broader impact/commercial potential of this I-Corps projects is the development of an Artificial Intelligence (AI)-enabled automation software that detects micro-anomalies in machine and robotic operations and pinpoints defective components before catastrophic equipment failure. Defective components cause unscheduled downtime and are a significant challenge for manufacturers. In a typical factory, an engineer may spend 30-60 percent of their time collecting information related to defective parts. Eighty-two percent of companies have experienced unplanned downtime that costs an average of $2 million annually. However, 70% of manufacturers still lack awareness of when equipment assets are due for maintenance or upgrade, and 72% of manufacturers identify unplanned downtime as their top priority or a high priority. The high penetration level of advanced automation and sensor technologies in industrial operations also has increased demand for methods to effectively monitor and manage complex operations in a systematic and timely manner. The proposed technology addresses this need by delivering capabilities to maintenance and industrial engineers to achieve near zero Mean Time-to-Resolution (MTTR) of costly manufacturing equipment failures. In addition, the technology eliminates the machine failures from occurring by advancing approaches that analyze data and provide insights after failure occurs.

This I-Corps project is based on the development of 1) a proprietary Programmable Logic Controller (PLC) driver that captures machine component micro degradation trends; 2) digital twins of the machine ecosystem that mirror the physical system performance and interactions between machines; and 3) an Automation Intelligence network. The proposed technology?s unique machine performance datasets, created from its PLC-driver and sensor feedback, captures the ?normal? operation of the machines. These datasets are then used to create causal relationship maps between different elements of the system. Causal maps accelerate the detection of root causes of potential machine failure and anticipate the potential risks and weaknesses within a system. In addition, digital twins provide a cyber-physical platform that integrates physics based models of machine components, spatial relationships from Computer Aided Engineering (CAE) models, and data-driven correlations inferred from self-adapting machine learning methods. The AI platform runs on a distributed system/edge analytics network to ensure real-time monitoring and diagnosis on the local node (e.g., individual machines and its components) and system (e.g., production lines) levels. Integration of these three elements establishes a scalable digital twin that leverages element interactions to continuously discover and refine causal relationships that rapidly and reliably detect, identify, and pinpoint issues.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project focused on an innovative manufacturing intelligence platform that addresses the commercial barriers for small to medium manufacturers. The proposed FE@ST (Factory Execution @ Speed of Thought) platform consists of 1) A proprietary data collector that captures micro degradation trends at the machine component level; 2) A digital twin of the machine(s) ecosystem (robotics and automations) that mirrors the physical system performance and interactions between machines; and 3) A FE@ST Automation Intelligence network. FE@ST’s unique machine performance datasets captures the performance baseline and “normal” operation of the machines, and efficiently models causal relationships between different elements of the system. Our manufacturing innovation helps small and medium manufacturing maintenance managers to significantly lower capital investment and maintenance operating budgets by being easy to implement (plug an produce), decreasing Mean Time-to-Resolution (MTTR), and achieving near-zero unscheduled machine downtime, avoiding expenses related to the productivity loss and customer churn.
The goal of the FE@ST I-Corps project was to explore the customer segments and value proposition of the FE@ST technology. Customer segments for this project were validated as manufacturing engineers and maintenance supervisors in highly automated tier 1, 2, and 3 manufacturing suppliers. FE@ST's value propositions of eliminating unscheduled failure induced downtime, reducing mean time to resolution, and decreasing maintenance site visits, were validated by customer discovery interviews and demonstrated via a pilot project with a tier 1 automotovie supplier. Interviews with the maintenance supervisors of the pilot project factory indicated that FE@ST's capabilities and insights proved to be valuable and novel for the industry.
As a result of this NSF award, the team was able to perform extensive customer discovery activities, which proved fundamental in translating the team’s idea towards a commercial product. Leveraging the insights gained from the I-Corps project, the team was able to perform a successful and focused demonstration on foundational aspects of the framework, published a patent elucidating the proposed approach, and submitted a publication to disseminate certain technical aspects of the underlying technology. This award enabled the team to gain a deeper understanding of the commercial needs and execute initial prototype development that will be used for generating a Minimum Viable Product (MVP). It also was instrumental in demonstrating to the academic technical leads an entrepreneurial mindset that has also contributed to enhancing their research endeavors. The industrial collaborations secured from this project, and clarity on the planned framework will guide the team as it puruses further commercialization efforts.
SBIR Phase I: OptimizerAero - A robust production scheduling optimizer for aerospace manufacturers
The broader impact of this Small Business Innovation Research (SBIR) Phase I project will be in making US manufacturing base more competitive. The aerospace supply chain contains thousands of less digitally sophisticated Small and Medium Enterprise (SME) manufacturers. The SMEs constitute a vast aerospace supply base across the country and have largely remained unaffected by the advances in operations research. Consequently, local manufacturers compete unfavorably with those in low-cost countries. While the computing power and speed of optimization techniques have increased to a point where one could now solve large-scale industry problems in real time, little attention has been given to the many modeling decisions that need to be made to accurately capture the complexities of real factory physics into a prescriptive mathematical model. This project will develop of a plug-and-play software for SMEs this estimated $850 M market that will improve efficiency along the entire supply chain. The resulting solution is expected to be a production optimizer that can be implemented in less than two weeks at any SME aerospace manufacturer using their existing data streams. Use of digital technologies and operations research advances embodied in this project will ultimately play an appreciable role in bringing outsourced manufacturing back to the United States.

The proposed project will advance translation of powerful optimization tools in production planning and execution. The proposed contributions include 1) a hierarchical approach that allows for planning and scheduling at different timescales, 2) computational improvements to classic operations models to incorporate real-time issues such as incomplete orders, carryover of production and continuation of setup activities across periods, 3) the design of algorithms to adapt the shop?s data to the different timescales while preserving model accuracy, 4) the analysis of the impact of planning horizon, time period choice and objective coefficients, as well as the creation of systematic schemes to determine their best value to meet user needs, and 5) the development of heuristics for quick re-optimization to respond to small deviations from the plan.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
INTELLECTUAL MERIT
Aerospace manufacturing is distinctly more complex than other industrial manufacturing owing to the following characteristics -
1. Low volume - High variety product lines
2. Very long lead times (12 to 24 months for castings and forgings)
3. Long set up times that fundamentally challenge the concept of ?one piece flow?
4. A very broad distribution of process times in the value stream (ranging from minutes to days)
5. Special processes (e.g., thermal barrier coatings) that are inherently batch processes
It is, therefore, not a surprise that general purpose ERP (Enterprise Resource Planning) schedulers are inadequate, especially for the longer time horizons that are required for aerospace parts. We conducted a survey involving 62 aerospace suppliers who almost uniformly expressed their dissatisfaction with the state-of-the-art long-term planning and scheduling tools. Medtech manufacturing shares some of these features and is our next target market.
We solved the modeling and computational challenge of predicting factory performance in the long term by overcoming the following technical barriers -
 Modeling Challenges Solved:
 1. Model generalization to handle any realistic aerospace factory.
2. Hierarchy of resources to account for resource subgroups that have some commonality but not the same capabilities.
3. Resource-dependent timeline or variable time steps. Capture the dynamics of the system at a fine granularity that allows higher accuracy.
4. Capture the capacity constraint for resources that require long setups and processing.
5. Cyclical routings.
6. Translation from a bucketized solution to a continuous time solution that is executable.
Computational Challenges Solved:
1. Computationally tractable scheme that is sufficiently fast (target computer time less than 10 minutes on a laptop).
2. Genetic-algorithm-based factory generator overcomes the difficulty of creating test data that has the right level of complexity and capacity loading; runs in seconds on a regular laptop.
3. Generic schema that is robust to map any factory into its digital twin.
4. Input processing algorithm to convert factory data into optimizer formulation.
The attached image shows that for a complex test factory, our approach gives better than 20 points improvement in factory performance. In Phase 1, we were able to eliminate the technical risks and demonstrate the feasibility of applying our method to real factories.
 
BROADER IMPACTS
 
1. Increasing the economic competitiveness of the United States
Helping the long tail of aerospace suppliers become more efficient individually
The weakest links in the supply chain tend to be the smaller and less digitally sophisticated manufacturers. These so-called SMEs (Small & Medium Enterprises) constitute a vast aerospace supply base and have largely remained unaffected by the advances in operations research. Our solution can help these shops grow margins and cash flow by double digits. OptimizerAero will provide an affordable and easy to use solution to this target market. The size of this market is estimated at $1.5B in Aerospace and Medtech in the United States alone.
Unlocking millions in cash flow in aerospace supply chain
The broader impact of our research will be in making the entire aerospace value chain more efficient and unlocking billions of dollars in trapped inventory in the supply chain. OptimizerAero result show a double digit improvement in efficiency (or throughput as measured by OTD or On-Time-Delivery), thereby potentially unleashing billions in cash flow across the supply chain, if our tool is uniformly applied across the supply chain. Even if a fraction of the aerospace suppliers use OptimizerAero, millions of dollars of cash flow will be available to companies to invest in people, technology and infrastructure.
 
2. Supporting the national defense of the United States
Facilitating On-shoring
With increased need for on-shoring aerospace and defense manufacturing, our solution will provide a competitive edge to US manufacturers over the alternatives, reducing our dependence on low cost but high risk overseas manufacturers. On-shoring benefits have particularly been highlighted during the current supply chain disruptions and a need to bring manufacturing home. Our tool helps make domestic manufacturer compete favorably with foreign competition, so on-shoring is not necessarily a high cost option.
Strengthening US defense industrial base
The benefits of OptimizerAero are equally applicable to complex defense manufacturing like defense electronics, ship building, etc. OptimizerAero shows improved benefits as manufacturing processes and the bill of material gets more complex.
3. Preparing the next generation of data savvy industrial engineers
We utilized student interns (4 overall during the project as technicians; 2 from computer science and 2 from industrial engineering) working on the project getting unparalleled factory analytics experience. They were involved in the algorithm development, implementation, software design and code writing and testing. They also got hands on training in AI as we used Genetic Algorithm in our Factory Generator task and they experienced various other aspect of data analytics.

I-Corps: A modeling-simulation-visualization framework for decision making in agricultural systems using data analytics and visualization
The broader impact/commercial potential of this I-Corps project is the development of technology for improving the efficiency and productivity of agricultural systems through data analytics. The technology and framework addresses issues relevant to smart farming including the use of digital data and technology to improve decisions and reduce risk throughout the entire agricultural value chain. The project focuses on high value specialty crops including grapes, hops, hemp, and medicinal herbs, through modeling and simulation of site selection and site geodesign. The operations of the farm through the creation of a digital twin of the agricultural system for alignment and harmonization of the management of physical and digital assets and processes will also be considered. The technology reduces risk of site and variety selection in the US through assimilation of data related to soil, climate, and topography alongside algorithms that evaluate specific grape varietal fit to the site based on historic and future environmental conditions. The platform has the potential to integrate disparate activities on-farm and off-farm (e.g., marketing and supply chain). The project will greatly enhance the commercial potential for this research to make an impact in high value, specialty crops in the US and worldwide.

This I-Corps project is based on the development of software infrastructure for on-line decision support tools for wine makers, grape growers and vineyard consultants. Historic, real-time, and forecast weather data is utilized in the application to provide a climatological and environmental analysis of a site, as well as pest and disease model outputs to evaluate risk for the grape varieties in the vineyard. In addition, a site assessment and evaluation is produced based on soils, topography and derived information in the form of maps, graphs and statistical summary information. While numerous pest and disease models exist from various researchers, government agencies and commercial sources, there are few applications that integrate winemaking and vineyard site assessment, geodesign and management into a coherent and automated platform. The scientific basis for the multi-criteria decision analysis included in the applications provide a robust foundation for scaling the application to new features, new crops, and new geographic areas. The proposed technology solution incorporates multiple disciplinary expertise to produce start-of-the-art data driven digital tools for planning and decision making.
SBIR Phase I: Unified data description layer for magnetic resonance imaging scanners
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to improve radiology and management of medical imaging equipment. The proposed analytics platform will provide detailed insight into device utilization to help oversee operations, optimize workflows, better leverage existing equipment, and evaluate the success of investments. More efficient use of scanners is expected to substantially benefit the patient population as it will reduce the wait time for magnetic resonance imaging (MRI), increase patient access, shorten imaging protocols, reduce sedation duration, reduce and predict delays, and ultimately improve the patient experience. The data unlocked by the platform will also open new avenues of research for radiologists and researchers.

This Small Business Innovation Research (SBIR) Phase I project aims to develop a technology that repurposes the Digital Imaging and Communications in Medicine (DICOM) data created by magnetic resonance imaging (MRI) scanners to build a unified, query-able source of knowledge about imaging exams. This project will harmonize DICOM metadata and build upon it to create an ontology that describes all the facets of imaging exams. Areas of development include recovering acquisition duration and scanner activity through algorithms that analyze images and exams to infer when the scanner was truly active. The project also demonstrates the impact of the data source by training a machine learning model to automatically detect repeated images, a prominent source of schedule delays. Overall, the developments from this project construct key aspects of timing and workflow from DICOM data to enable a new form of data analytics in Radiology.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Today, MRI scanners are idle, not taking images more than 50% of operational hours, wasting $10B+ in potential revenues annually, and delaying patient care. Despite MRI being the most significant capital purchase a hospital can make (~$1M-$4M), and despite continued innovation in hardware and software to achieve faster imaging times, there remains a massive challenge in understanding how these machines are currently being used. There is just no access to operational data. As Peter Drucker says: “If you can’t measure it, you can’t improve it.” 
The research goal of this Small Business Innovation Research Phase I project was to develop a technology that analyzes, cleans, and harmonizes MRI scanner data to build a unified, query-able source of knowledge about imaging operations so that imaging facilities can monitor and optimize operations with data-driven strategies, exactly like airline companies optimize operations by analyzing data from jet engines. 
  
The R&D in this NSF SBIR Phase I focused on technical contributions to harmonize the metadata generated by scanners and create a new cleaned, harmonized ontology of operations that can be queried to unlock operational insights. 
1.We collected scanner data from many publicly available databases to test our algorithms on many different scanners;  
2.We designed, developed and tested various algorithms to harmonize data, and especially extract key aspects of timing from the scanner metadata to derive meaningful metrics of scanner utilization. For example, we designed and tested a 3-stage algorithm to accurately recover the duration of each acquisition. We also designed and tested an algorithm that extracts the concepts of examinations and acquisitions from the scanner data by analyzing the overlap in times of objects generated by scanners.  
3.Beyond harmonizing the data, we used machine learning and AI to augment the data and unlock new descriptors of operations. We developed a classifier to detect repeated images, a major driver of increased scan time, scheduling uncertainty, and delays. We tested the use of deep-learning to detect the presence of contrast in MR images. Finally, we developed a new distance metric between examinations, each examination being described by the list of technical parameters of its acquisitions. This is a theoretically challenging question because examinations may have a different number of acquisitions, which amounts to computing a distance between objects in spaces of different dimensions. We used the metric to learn imaging protocols on-the-fly (unsupervised learning) and identify deviations. 
 
Our R&D effort led to seven abstracts being sent to international conferences, and three journal papers are now in preparation. Our theoretical work on protocol learning led to a provisional patent application filled in May 2022 (US 63/337,827, Identifying Medical Imaging Protocols Based on Radiology Data and Metadata). 
Beyond demonstrating the feasibility of our vision, the R&D carried out was key to building a first minimum viable product (MVP) and showing early traction with customers. By the end of the project, we had four paying customers with an annual recurring revenue (ARR) of $207k / year. We are now collaborating with these early adopters to develop proof-points and demonstrate the impact of data liquidity on their operations. 
Furthermore, the support from the NSF and the first evidence of product-market fit allowed us to raise a $1.7M pre-seed round with venture capital in February 2022 to scale the executive team and support customers. 
 
More generally, the work initiated in this NSF SBIR Phase I sets the ground for our longer-term vision to develop the digital twin of radiology departments that will allow not only monitoring of operations but also simulations of interventions, without the need for expensive physical experimentations. 
Ultimately, the availability of operational data and simulations is expected to radically reshape the way MRI is managed, urgently needed 1) to help the industry survive declining reimbursements, cost pressures and inefficiencies; and 2) to increase access to imaging for the population. 
 Mechanistic Machine Learning and Digital Twins for Computational Science, Engineering and Technology (MMLDT-CSET) Conference 2021; San Diego, California; September 26-29, 2021
This 3-day conference is organized into fourteen technical tracks and two short courses in a hybrid format. This conference will provide a forum to exchange new ideas among students, researchers, high school teachers, and practitioners in the fields of mechanistic machine learning, artificial intelligence, and digital twins. Application areas include civil infrastructures, natural hazards engineering, geosystems and petroleum engineering, reliability-based engineering and design, material systems, manufacturing, mathematical and scientific computing, natural and life sciences, and healthcare. A short course called ?Mechanistic Data Science? (MDS) targeted at high school students and teachers, and STEM undergraduates is designed to provide participants with a big-picture perspective related to machine learning and digital twins, and to demonstrate how to apply MDS to combine data science tools with mathematical scientific principles to solve intractable problems through daily-life examples. A short course called ?Mechanistic Machine Learning for Physics and Mechanics? will be offered for graduate students and researchers to introduce machine learning techniques for the participants with a background in physics and mechanics. These courses will also be integrated with the mentoring and networking activities under a Technical Track ?Education, Outreach, and Funding Opportunities?, and with a panel and Q&A sessions for each of the three days. NSF Fellowship will be used to support undergraduate and graduate students, postdoctoral fellows, high school teachers and students, as well as students from underprivileged schools to attend the conference and short course activities. Undergraduate and graduate students from historically black colleges and universities and minority-serving institutions as well as underprivileged high schools will be recruited.

This conference introduces ?Mechanistic? Machine Learning and Digital Twins (MMLDT) as an integrated methodology for coupling data with mathematics and scientific principles to solve otherwise intractable problems. This conference also identifies ?Digital Twins? as important machine learning applications for improved product designs via computational science, engineering, and technology (CSET). The main objective of MMLDT-CSET is to bring together these diverse communities that are interested in learning, developing, and applying machine learning and digital twins via mechanistic methods and computational science and engineering tools for a broad range of engineering and scientific problems, while promoting transdisciplinary collaborations among engineers, physical and biological scientists, data and computer scientists, and mathematicians from federal agencies, academia, and industry. The discussion of future MMLDT research and technology developments will be driven by societal needs and grand challenges presented by practicing engineers, technology firms, and computer/software companies.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This MMLDT-CSET 2021 Conference was organized to facilitate the transition of Machine Learning (ML) and Digital Twin (DT) from fundamental research to mainstream fields and technologies through advanced data science, mechanistic methods, and computational technologies. This conference was held during September 26-29, 2021, at the Hyatt Regency Mission Bay, San Diego, in a hybrid format, featuring both on-site and virtual sessions. This three-day conference featured 8 technical tracks, 98 sessions with 450 technical presentations, 8 plenary lectures, 2 short courses, 3 lunch-time panel discussions on MMLDT-CSET Education, Outreach, and Funding Opportunities, and a Special Track: Mechanistic Data Science for High School and Undergraduates STEM Education and Applications. A total of 796 attendees participated in this conference. Among them, 350 (44%) are students, in which 205 were sponsored by the NSF fellowships. Out of the 796 participants, 166 (21%) attended the conference in person. 
 
The NSF Fellowship was used to promote awareness of mechanistic machine learning and digital twins and knowledge dissemination to all levels in education, including high school, undergraduate, and graduate levels. A portion (50%) of the fellowship was reserved for underrepresented groups to promote equity, diversity, and inclusiveness. The NSF fellowship fund of $99,550.00 was used to cover the conference and short courser registration fees. A total of 314 fellowship applications were received, and 205 of them were selected to receive the NSF Fellowship support. About 25% of the Fellowship awardees are female students, while 35% of the awarded undergraduate and graduate students are first generation college students, and the 20 high school students are Hispanic students from El Paso, TX.
 
To promote awareness of mechanistic machine learning and digital twins to the public, two short courses were offered for attendees with diverse background, knowledge, and interests. A total of 242 conference participants attended the two courses: (1) Mechanistic Data Science (MDS) for STEM Education and Applications, and (2) Mechanistic Machine Learning for Engineering and Applied Science. A Special Track “Mechanistic Data Science for High School and Undergraduates STEM Education and Applications” was offered during the three-day conference to promote mechanistic data science (MDS) in STEM education. This technical track consisted of three two-hour lectures with exercises. It was supplemented with six two-hour panel discussion sessions focusing on how similar MDS examples can be used in high school and undergraduate education.
 
Three lunch-time panel discussions on Education, Outreach, and Funding Opportunities were organized to brainstorm ideas and provide recommendations on knowledge dissemination and promotion of research and education in ML and DT to the communities. While panel discussions were held on-site in the conference rooms, they were broadcasted through the conference virtual platform to allow in-person and remote discussions and participations. (1) Education Panel: Mechanistic Data Science (MDS) is a structured problem-solving methodology combining mathematical science principles with data science. The education panel explored the implementation of MDS into existing and future STEM education programs, ranging from K9-12 through graduate school. (2) Outreach Panel: The goal of the outreach panel was to identify, particularly for females and underrepresented minorities, the barriers that prevent them from entering and excelling in the broad disciplines of machine learning and digital twins. The panelists’ experiences and success stories of overcoming these barriers were highlighted as examples on how to improve outreach and broaden the participation of diverse individuals. (3) Funding Opportunities Panel: The funding opportunity Panel was designed to help PIs investigate funding opportunities related to MMLDT-CSET research in government agencies.
 SBIR Phase I: Measuring and Mapping Pain Intensity onto a 3D Image of the Human Body
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will significantly improve the way clinicians and researchers measure, study and treat pain. Chronic pain affects 20% of U.S. adults and costs $600 billion annually. Opioids are often the first treatment option offered to patients with chronic pain. Currently pain is assessed using simplistic, subjective and generalized visual/verbal scales (1-10) with little ability to localize the origin. It is essential for clinicians and medical researchers to have calibrated and accurate measures of pain to assist patients in determining the underlying disease and directing patients to the most effective therapies. This project will develop a new biomedical tool to quantify pain data for multimodal treatment plans to potentially minimize opioid use. This tool will improve quality of life for people suffering from chronic pain and help those on opioids find better therapies. Furthermore, it will help clinicians with varied levels of training and experience achieve the highest level of exam skill.

This Small Business Innovation Research (SBIR) Phase I project will digitize a physical exam by capturing palpation location and pressure and patient pain expression via multiple biosensors to create a personalized ?contour? map of pain intensity onto a 3D image of the human body. This comprehensive and calibrated data set can then be used to assist clinicians in developing a more accurate differential diagnosis for the underlying pain condition. This project will advance the hardware and software components for future integration with augmented reality and artificial intelligence modules.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Pain was invisible. Until now.
 
PainScan helps solve the challenge inherent in assessing and treating complex pain. 50 million adults in the United States suffer with chronic pain and pain is the #1 reason for medical visits. Even with state-of-the-art radiographic tools like CT scans and MRIs, assessing complex pain is challenging for doctors and often frustrating for patients.
 
PainScan helps patients and their doctors shorten the path to accurate diagnosis and effective treatment by producing a vivid 3D representation of pain intensity on a personalized human digital replica. The resulting PainScans capture the patient’s pain picture more clearly and visibly than any verbal explanation or the 1-10 pain scale ever could.
 
With our patented system, patients no longer need to worry if their healthcare providers believe the pain they are experiencing is real. Instead, their healthcare team can focus on diagnosing those underlying conditions that would likely produce the patterns seen on the patient’s PainScan.
 
Chronic pain disparately impacts women, elderly, and rural community members. Additionally, people of color receive lower quality care for pain treatment. Pain impacts relationships, work, mental health, and finances. We are dedicated to addressing health disparities in chronic pain assessment and treatment, helping ensure inclusivity and positive impact for all communities that will benefit from this new technology.
 
Thanks in large part to this National Science Foundation (NSF) SBIR Phase I grant, we have been able to assemble a team who believe in the mission, have the necessary technical skills, and look forward to bringing PainScan to the healthcare market.
 
What is PainScan?
 
PainScan is a digital health solution that automates pain assessment to augment pain diagnosis. PainScan engenders a collaborative process between doctor and patient with both participants working together during a physical exam. The shared experience of generating the pain map ensures that the doctor and patient are both on the same page, sharing the same understanding of the full pain picture and validating the patients pain experience. PainScan lets doctors see what their patients feel.
 
How does PainScan work?
 
The PainScan system is comprised of three components:
 
-        a computer vision camera system that utilizes machine learning algorithms to locate clinician touch and place it accurately onto a life-like 3D image of the patient, called a digital twin
-        a clinician glove with sensors that measure the pressure applied during each touch
-        A hand-held device which the patient squeezes to silently share the pain level each time a clinician gently touches
 
As the clinician examines the patient, data and images are analyzed by proprietary software to find where the clinician is touching, how hard the clinician is touching, and how much pain the patient is feeling. At the end of an exam, all points touched and the patient’s pain levels at each location are displayed on the digital twin for the clinician and patient to see. 
 
The patient and doctor are then provided a 3D version of the painscan which can also be shared with other doctors and specialists, similar to sharing x-rays. However, PainScans are non-invasive and non-radiological. Just as x-rays made bones visible, PainScan now makes pain visible.
 
What was funded by NSF?
 
NSF funds helped develop the software system and the method to identify the location of touch. Prior to this funded work there were no known methods to automatically track clinician touch and reproduce it on a 3D digital twin of the patient.
 
Our team outlined ambitious R&D goals for this NSF SBIR Phase I Award. Many technical, business and customer discovery milestones were accomplished. The NSF Phase I investment helped show that the core technical risk of mapping clinician touch onto a digital twin of the patient is attainable. The accuracy of our version 1 system is not yet sufficient for clinical use, but the prototype developed is a compelling proof of concept for digitizing the medical physical exam.
 
The benefits of PainScan include personal precise 3D pain assessment, longitudinal calibrated pain tracking, and comparison of pain patterns across large groups of people to improve diagnostic categories and find effective therapies. PainScan is the evolutionary next step in pain assessment technology, providing a comprehensive, modern software platform and human body database that will be adapted over time to include multiple pain biosensors.
 
With further investment, the system will be improved to a level of accuracy that offers clinicians, patients, and researchers the ability to confidently capture and document a holistic and comprehensive view of the full pain picture. We thank the NSF community for this opportunity to develop our vision for improved pain assessment for the benefit of so many people suffering with chronic pain, in the United States and around the world.
IUCRC Phase I University of Miami: Center for Accelerated Real Time Analytics (CARTA)
CARTA (Center for Accelerated Real-time Analytics) is a multi-university Industry-University Cooperative Research Center that started in 2018. As more devices are deployed that produce real-time data, technologies beyond simply storing and archiving future use data are in demand. CARTA aims at developing such techniques by exploiting artificial intelligence and machine learning, which would make an impact on a wide range of areas, including science, engineering, medicine, marketing, and finances. University of Miami will join CARTA as a new partner site (called ?CARTA Miami?), support the mission of CARTA, and expand the scope of research of CARTA.

The real-time analytics research at CARTA Miami will be focused on neuroscience, signal processing, IoT, genetics, UAVs, and marketing. Initial research projects will focus on a) using real-time EEG feedback from portable EEG systems to produce music recommendations for controlling mood, b) combining real-time audio and video captured with UAVs to detect anomaly, c) adding genetic information to real-time product recommendation, d) delivering real-time recommendations to shoppers, and e) collecting IoT signals in smart homes to make real-time decisions. CARTA Miami will leverage the high-performance computing available at the Institute for Data Science and Computing at the University.

CARTA Miami will offer a unique opportunity for graduate students to improve their research and communication skills. They will participate in collaborative research with industrial partners. The experience will teach the graduate students how to succeed in industrial R&D, which is essential for their careers. Furthermore, the industrial partners can expand their research portfolio by exploring new ideas at CARTA. Because CARTA is a multi-site center, sharing of information across sites can bring additional benefits to both its partners and students. The site will communicate its activities through research publications, conference presentations, and reports.

At all partner sites of CARTA, the private industry partner data is kept confidential. CARTA has a center-wide 6-month embargo policy. CARTA Miami follows this Center-wide policy. Under the policy, publicly-available data generated at CARTA Miami will be made available after a 6-month embargo through the University of Miami?s data repository. Interested researchers, both CARTA members and non-CARTA members, may request copies of data stored in the repository via email.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
As a new site to the IUCRC CARTA (Center for Accelerated Real-time Analytics), the University of Miami teams conducted research in several research areas.
(1) Predicting adverse reactions to skin products by integrating multiple types of data sources. The teams were able to integrate the information to generate recommendations for safe skin product selections. The team also developed a simulation model that extends aggregate SNP (single nucleotide polymorphisms) to a full-scale DNA sequences data, where an individual's DNA sequence can be search for a match.
(2) Predicting recalling events of listening to known pieces of music through observations of the headmounted EEG (brainwave) data. The experiments show that the algorithm can detect the first-time and second-time listening events of individuals.
(3) Developing a testbed for medical hard and soft devices, where developers can deposit their products for verification and assurance. The target in the project was to develop a system for testing radiology-based cancer detecting algorithms. The team developed a platform for the test.
(4) Developing a method for predicting the tendency to drug addiction based on an individual's medical record. The team was able to sufficiently accurate (65-75% in accuracy) for a patient's drug addiction based on the record.
(5) Developing a platfomr for large-scale simulations (a digital twin) for cyber-systems for the purpose of testing the cyber-system's security. The team began exploration and started collaborated discussions with teams at other sites. These research projects were supported by the memberships from The Legacy Research Insitute, The Rockefeller Institute, Neurotargeting, YE Ventures, and Resilence. These project's broader impacts are strong since they pertain to the product safety, mental health, and cyber-security.
The projects also involved one undergraduate student, four graduate students, and four postdoctoral research scientists and helped them advance their career.
I-Corps: Digital twin technology via synthetic data generation to predict outcomes of clinical trials
The broader impact/commercial potential of this I-Corps project is the development of a platform technology that is applicable to fields where experimental data collection is expensive or infeasible. A first application will be in the domain of clinical trials, where there is a growing trend towards precision medicine, yet experimentation with randomized control trials is often extremely costly and time consuming. This is particularly true for mid-sized pharmaceutical companies with more limited resources. The digital twin technology provides companies with synthetic data to help optimize trial design and control risks in how to allocate limited financial and personnel resources. The technology may also have far-reaching applications in potential application areas of e-commerce and precision agriculture.

This I-Corps project develops a new causal inference technology to accurately predict outcomes of clinical trials on a patient-by-patient basis. This technology builds a digital twin of each patient using historical clinical data across different patients, treatments, and diseases. A novel benefit of this approach is that despite very scarce data on patients that go through the full clinical trial, the technology can accurately simulate the outcomes of the entire patient population. Additionally, this approach offers interpretability of how such digital twins are created, a critical feature in medical applications.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project is based on research that resulted in a new framework for causal inference and machine learning to answer “what-if” questions. Accordingly, the technology is equipped to estimate heterogeneous treatment effects, identify meaningful subpopulations, and simulate counterfactual scenarios. 
Today, decision makers in even top companies are guessing not because of a shortage of data, but rather because of a lack of insights caused by technical shortcomings. Current AI is not able to predict the outcomes of situations that haven’t happened before, which results in companies being reactive as opposed to proactive with their strategies. As we continue to see a shift in consumer behavior being more digital, and companies facing more competitive pressure than ever before, this problem is only increasing and causing more financial loss and suboptimal customer journeys. The technology empowers decision makers with state-of-the-art prediction capabilities at their fingertips. It uses business data to simulate the outcomes of hypothetical strategies. 
This I-Corps Team explored the various potential applications of the technology. Early evidence from potential users suggested business leaders would like tools to make faster and better decisions. During this award, the Team has conducted over 200 customer discovery interviews. The team has pivoted several times in response to the evidence collected.  The team was to generate sufficient evidence of potential product-market fit in the area of business intelligence and marketing that they decided to found a new startup venture.  They have raised initial funding, have run pilots with potential customers, and were accepted into a premier accelerator program.
Excellence in Research: A Cyber-Physical System Framework for In-process Quality Assurance of Inkjet-based Additive Manufacturing
This Historically Black Colleges and Universities - Excellence in Research (HBCU-EiR) grant supports research that contributes new knowledge related to quality assurance for additive manufacturing, promoting both the progress of science and the advancement of national prosperity. Inkjet printing is one representative additive manufacturing process based on thermal or acoustic formation and ejection of liquid droplets through a nozzle. Its great promise has been demonstrated in electronics, energy, healthcare and biomedical industries. However, inkjet printing is sensitive to environmental, material, mechanical and electronical factors, and the process can easily deviate from the desirable working status, resulting in defective parts. This tends to lead to material and energy waste and affects the structural health and functional integrity of many important engineering systems. This award supports fundamental research to provide needed knowledge for the development of a holistic framework involving neural networks for quality assurance in inkjet printing. This project holds the potential to significantly improve productivity, quality and material efficiency for inkjet-based additive manufacturing processes, thus benefiting the U.S. economy and society. Using a multi-disciplinary approach involving manufacturing, computer vision, control theory, and machine learning, this research helps broaden participation of underrepresented groups in research and promotes engineering education.

The goal of this project is to establish a comprehensive framework that seamlessly integrates in-process video-based monitoring with closed-loop control and compensation to effectively detect and subsequently correct the process drift and anomalies toward high-quality inkjet printing. The framework consists of three synergic digital twins based on neural networks, a technique that mimics the operations of a human brain in the artificial intelligence field. The first digital twin aims at closed-loop control of the kinematic and morphological status of the micro droplets. The second digital twin focuses on closed-loop control of the geometrical and morphological status of the printed patterns. The third digital twin determines and implements compensation strategies for defective patterns. Specific objectives are to 1) identify methodology for creation and integration of digital twins to maintain desirable droplet status, obtain required patterns and implement effective compensation, 2) derive practical guidelines of using neural network in quality assurance, including input selection and preparation, network design and optimization, output selection and usage, and transferability and adaptability, and 3) understand the relationship between material properties, control variables, in-process parameters and print outcome in inkjet printing from the perspectives of neural network. This project is expected to provide fundamental understanding of the design, development, and implementation of cyber-physical systems in additive manufacturing. The developed framework can be adapted to other macro- and micro-scale additive manufacturing processes.
I-Corps: Application of deep generative models for simulating biological systems
The broader impact/commercial potential of this I-Corps project is the development of an artificial intelligence (AI) based platform for simulated biological systems that can predict the true phenotypic outcome of any perturbation prior to wet lab experimentation. The development of the proposed technology addresses the need for a highly predictive, efficient, and cost-effective platform that has potential applications in drug discovery, gene therapy and personalized medicine in the biopharmaceutical industry. Traditionally, the development of drugs, vaccines, and therapies is carried out in biological wet lab settings from preclinical cell and animal models to clinical phase human trials. This research is often expensive, requires years of effort, and can struggle to achieve suitable efficacy. The proposed technology may offer prediction of biochemical changes with high accuracy and maximum efficacy and safety, thereby reducing the burden on payers and stakeholders.

This I-Corps project leverages artificial intelligence (AI) and machine learning (ML) through deep generative models (DGMs) to accelerate prediction of phenotypic outcomes in biological systems. Deep neural networks combined with progress in stochastic optimization methods have enabled scalable modeling of complex, high-dimensional data and has become the often preferred artificial intelligence method in computer vision, speech and natural language processing, graph mining, and reinforcement learning. However, there are few examples of its application to biological systems. This project combines the biological processes and DGMs to train the deep neural network in characterizing the phenotype unique to each process. The trained DGMs can then predict the best outcome for any perturbation to these processes in any given environment. The proposed technology is reproducible and scalable, and designed to provide high-content structural, phenotypic, and morphological profiles of the effects of biological and pharmacological substances.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project was undertaken to explore industrial applications and potential pain points and challenges involved in industrialization of a new technology based on PhD and postdoc research work of the team members. The technology was the application of Machine Learning (ML) methods to the problems in biology and health.  The team had basic understanding of the application of machine learning practices for medical imaging and diagnosis but lacked first-hand knowledge of the customer?s relevant pain points and cost-drivers from an operational and economical perspective.
The team participated in the I-Corps National Teams program which teaches researchers how to translate academic research into commercial applications that solves a problem for society and captures its value.   This immersive 7-week program provided important experiential training on customer discovery and validation. In total, we performed over 150 interviews during the award period.  Our initial idea was to offer a generalized phenotype-to-genotype ML model in a yeast system to researchers. Through the discovery process we found that would not find a market amd pivoted to exploring applications in sickle-cell disease.  During the cohort we did observe a product market fit for identifying therapeutics/gene therapy for sickle-cell disease using our ML models.  However, further discovery work revealed that needed technical and performance requirements could not be met by our technology in this area.  This led to another major pivot to build a ?digital twin? of viruses for identification of therapeutics to cure viral diseases starting with chronic Hep B as the beachhead market.  
As a result of the training and the evidence developed during the award, the team believes that there is a viable business model to bring this technology to market.  This has led to the formation of a startup company with the team members involved in the venture. The team intends to pursue further development of the technology and have identified various funding opportunities to pursue. The participants on the team have been introduced to, and have had the opportunity to explore in depth, the key steps needed to successfully translate technologies from labs to practical use. In other words, commercialization. They have gained new skills that can be applied to a technology transfer situation whether for the technology involved in this project, or for other technologies throughout their careers.

I-Corps: A spatiotemporal simulation system to predict COVID-19 case trajectories in schools
The broader impact/commercial potential of this I-Corps project is to enable safe reopening of college campuses and school systems, as well as companies and other organizations with a campus setting after the COVID-19 crisis. Due to the sudden outbreak of COVID-19, college campuses and school systems were shut down globally to slow down the spread of the disease. Despite being largely successful in controlling the spread of COVID-19, school closings caused health problems, including physical and psychological stresses, for students, staff, and faculty in addition to direct economic impacts. This project develops a system to better support school re-openings with specific policies based on scientific data and information. The potential product can simulate the trajectory of COVID-19 cases and provide potential customers with a reliable method to scientifically monitor the status of their campuses. This product has the potential to be used globally and also can be used to manage large events with a focused gathering site setting.

This I-Corps project is based on a spatiotemporal simulation/prediction system developed to address the urgent campus reopening questions caused by the COVID-19 pandemic. The system integrates investigation on spatiotemporal infrastructure including big data, cloud computing, analytics, and community outreach. Human behavior simulation, campus spatiotemporal dynamics, and public health data are combined to predicate possible viral case trajectories. The novel spatiotemporal simulation integrates agent-based people movement and public health risks as well as control policies (such as mask mandates, classroom size restrictions, test and tracking, and vaccination status) to derive accurate predictions. The system has been validated using data from several college campuses.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Introduction: Since mid-March 2020, a sudden outbreak of COVID-19 has caused campuses and school systems to be shut down nationwide and globally. While considering how to safely reopen schools, many school systems soon turned into 100% online instruction mode due to case surge, which caused many economic and employment issues. In order to solve those problems, we translated our previous fundamental research into a commercial product to address this national and global societal challenge which is to simulate the trajectory of the COVID-19 cases and provide a reliable method to scientifically monitor the campus status, to promote a safe reopen strategy which brings people confidence to return to campus.
Intellectual Merits: After 123 ICORP customer interviews, including universities, k-12 schools, high-tech companies, public museums, gyms, theaters, and tourist attractions, we discovered this system not only can be applied to school systems but many other public gathering areas such as gyms, theaters, and museums. We designed and optimized a system that can guide their reopening based on the information they provide. We have validated our hypothesis that businesses that engage large publics, such as theaters and gyms, are losing customers and profits because they do not have enough safe configurations, which cause customers to lose confidence to participate in public activities. This system can bring customers? confidence to go to those places, which can help store owners bring profits back. In addition, we investigated similar products in the current market. We discovered that software in the current market only provides functions to manage their customers and incomes but does not provide real-time capacity control advice that can help them construct a safer environment. We optimized our reopening model and added more functions based on feedback from our customers. Finally, we engaged several universities in the United States and Africa to test our applications and use our results as advice to construct the daily COVID-19 measures.
Broader Impact: By January 2023, although many places have reopened, and most of the public do not worry about getting infected, there are still many other countries that might need this kind of system to control other viruses. We engaged several universities in Africa to test our applications and use our results as advice to construct the daily COVID19 measures. It can be used for other diseases control such as Ebola for campus and public gathering places or events in Africa. In addition, this customer interview has guided us on how to investigate the market for future products. We learned plenty of lessons on building our business model including customer segments discovery, revenue model, value proposition, and others. We will optimize our marketing strategies for transferring future scientific projects into the commercialized products.
GOALI: Collaborative Research: Generation versus Degradation: Striking the optimal balance for wind farm profitability via digitization, predictive and prescriptive analytics
The rapid increase in scale and sophistication of wind farms poses a critical challenge relating to the cost-effective management of wind energy assets. A defining characteristic of this challenge is the economic trade-off between two concomitant processes: electricity generation (the primary driver of short-term revenues) and asset degradation (the major determinant of long-term expenses). This NSF project aims to formulate a decision-theoretic approach to jointly optimize generation and maintenance in wind farms. The project will bring transformative change into the status-quo of asset management in the wind industry which, to-date, relies on single-faceted strategies that largely overlook the dependencies between the generation and degradation in wind turbine assets. The intellectual merits of the project include the formulation of novel data and decision science models, blended within a digitization platform, to predict and co-optimize operations and maintenance requirements. The broader impacts of the project include disseminating research findings via coursework, publications, data/software, and industry-academia workshops. A set of use case demonstrations, co-developed with industrial partners, will accelerate the translation of scientific knowledge into tangible industrial impact, in a step towards meeting the 35%-by-2050 U.S. wind energy target. Summer internships and undergraduate researchers, especially from underrepresented groups, will contribute towards educating the next-generation workforce in data, decision, and energy sciences.

Without formally considering the intrinsic dependencies between electricity generation and asset degradation, wind farm operators reap sub-optimal benefits from their operations and maintenance policies. This project aims to formulate a decision-theoretic framework which seeks an optimal balance of how wind loads are leveraged to harness short-term generation revenues, versus alleviated to hedge against longer-term maintenance expenses. The framework comprises decision-aware predictive models for power and asset health degradation forecasting, integrated within mixed integer programs with decision-dependent uncertainty. New reformulations and constraints will ensure an effective predictive-prescriptive coupling, thereby enabling the optimization to search within the prediction space for an optimal prediction-decision pair. An end-to-end digital twin of the wind farm will bind the proposed predictive-prescriptive methodologies within an integrative asset management solution. Engagement of industrial partners and a national laboratory will ensure a sensible impact on asset management in the wind industry.
SBIR Phase I: Creation of a virtual population of older, black patients with hypertension and comorbidities for improved treatment development
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is a tool that will streamline the clinical trial process, reducing time and money spent without sacrificing safety or an understanding of a treatment?s efficacy. Medical treatments do not work equally well on all patients. Finding a responsive population on which to perform a study directly impacts the likelihood of a new therapy?s success while minimizing risk to patients who would see little or no benefit. The tool will allow clinical trials to be run virtually as computer programs in software that simulates human physiology. This technology will be valuable at all stages of treatment development: suggesting new therapies, testing for adverse effect, planning clinical trials, interpreting trial results, and predicting results when a new therapy is introduced outside of the clinical trial. This project is focused initially on the development of a critical under-represented population: older black men with hypertension and associated diseases, including heart disease and diabetes. Simulations of specific populations will allow researchers and clinicians to anticipate issues, enabling more effective treatment plans.

This Small Business Innovation Research (SBIR) Phase I project will develop a virtual population of 50-70 year old hypertensive African-American men with and without common cardiovascular comorbidities including congestive heart failure and type II diabetes. The virtual patients will be instantiations of a differential-algebraic model of human physiology, reflecting interactions between factors such as hormone concentrations, electrolytes, and more. The virtual population will be generated from clinical data collected from other studies and the approach can be used for other specific populations who may be historically under-represented in trials. This model can be used to estimate responses to clinical interventions (device and pharmaceutical therapies) to inform treatment development and further trials.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Variation is a feature of any population of humans: it is our goal to predict and manage variation using physiological models.  Inclusion is a suite of physiological modeling software and other applications that will, in its final form, automate the calibration of a physiological model, HumMod, to a population specified by a collection of data. Here “calibration” means that the virtual population will respond to the full variety of interventions captured in the data as the real cohorts did, lending confidence that the population will respond correctly to a novel intervention. 
All humans have essentially the same physiology: a list of ways the body interacts with itself and its environment to preserve life.  While the interactions are the same in all individuals, the details of the interactions differ due to genetics, lifestyle choices, the environment the individual has been exposed to, or a host of other factors.  Mathematically, this means some interactions are stronger in some people than in others or take place in slightly different conditions. Because the body interacts with itself in hundreds of ways to preserve blood volume, oxygenation of tissues, the proper balance of electrolytes, proteins, and metabolites, and other important quantities, small changes in one system ripple out to affect other systems. Hence predicting a particular human’s response to a therapy is difficult without a technological aid. 
In this work, we developed the tools necessary to calibrate a physiological model, HumMod, to a given collection of humans with similar clinical experiences. We used these tools to create a virtual population of 45-65 year old Black males with hypertension and common comorbidities including heart failure, diabetes, and renal failure along with their response to amlodipine, a commonly used first-line antihypertensive.  Our goal was to create a technological tool that would predict individual responses to aid clinicians in finding the best intervention for a given individual.  Additionally, the tools might be used to predict adverse effects, help find the best candidates for clinical trials of new interventions, and to address research questions concerning why particular individuals do not respond to treatment.
We created three types of tools in this project. First, an extractor tool that converts medical records (stripped of identifying information) into a medical history that automatically creates scripts that make a simulation repeat the patient’s experience. Second, a tool that extracts measurements for comparing the simulation’s outputs to the patient. Third, we created a tool that uses the scripts and comparators to generate a population of virtual patients whose experience matches the that of the real patients, and used probability theory to characterize the relationships that made the virtual population replicate the real population. After construction, the matching virtual population functions as a sort of digital twin for the real patients and can be used to test treatment options. 
This technology, once mature, will streamline the development of new therapies, the clinical trial process, telehealth, and clinical medicine.
Excellence in Research: Convergent Physics-based Data-driven Bioprinting of Regenerative Tissues for Future Biomanufacturing
Regenerative tissue engineering holds great promise to replace diseased and dysfunctional organs with stem cells. The fabrication of tissue scaffolds in stem-cell engineering is, however, dependent on an interplay of several factors such as biochemical signaling, cellular arrangement and related process parameters. Key impediments in progressing biomanufacturing research are the lack of formal guiding principles and real-time process monitoring, in addition to the exorbitant resources required to conduct stem-cell based bioprinting experiments. This critical barrier has limited the ability to control the growth behavior of multiple cell types to form viable tissue constructs for organ replacement. To address these issues, this Excellent in Research award will investigate physics-based models that integrate sensor data with machine learning algorithms and experimentation to create a digital twin of bioprinting processes. The discovery-driven research will generate a body of knowledge to guide researchers and industrial users through an open-source repository of Bioprinting Design and Manufacturing rules for regenerative tissue engineering. The education efforts including the development of biomanufacturing coursework will impact underrepresented students at the North Carolina Agricultural and Technical State University, one of the nation?s largest historical black colleges and universities, and beyond. A scholar exchange program with the Wake Forest Institute for Regenerative Medicine (WFIRM) will train student cohorts in biomanufacturing, data-analytics and guiding procedures.

The overall goal of this project is to establish a physics-based data-driven structure in hybrid bioprinting to custom engineer stem-cell based tissue constructs. The specific objectives include (1) creating a robust framework integrating computational modeling, experimental results and industrial internet of things based scaffold health monitoring techniques for bioprinting, (2) understanding the combinatorial effect of adsorption configurations of biochemical cues and nanoscale topologies using hybrid physics-based data-driven models, and (3) investigating relationships among interacting materials, process parameters and microenvironmental variables of bioprinting for closed-loop control. The team plans a convergent approach wherein, computational modeling data, experimental research, real-time in-situ sensors and diagnostics will be augmented to investigate bioprinting process parameters. Machine learning algorithms will be applied to the consolidated data sets to unravel the underlying hidden patterns between topography, mechanical stimuli and biochemical cues in determining cell fate and function. The hybrid predictive models will be developed to enable real-time monitoring and control of the bioprinting process and material formulations. Cell proliferation, histological staining, and biochemical assays will be performed at the WFIRM to validate the hybrid models. Input-output relationship mappings will enable integrated process control, monitoring and smart process data analytics towards a Biomanufacturing Industry 4.0.
Collaborative Research: SCH: A Causal AI Digital Twin Framework to Transform Intensive Care Delivery
This Smart and Connected Health (SCH) award will contribute to the advancement of the national health and welfare by developing a causal AI digital twin framework to support critical care delivery and facilitate the realization of ?Healthcare 4.0.? Critical illness from sepsis and pneumonia is the leading cause of in-hospital mortality and a global health priority. While early diagnosis and error-free treatment consistently achieve good outcomes, the progression of critical illness to multiple organ failure often translates to either death or loss of independence. The COVID-19 pandemics have exposed long standing deficiencies in critical care knowledge and practice in hospitals worldwide. The National Academy of Medicine has called for a novel systems science approach to clinical medicine and new methods and strategies to facilitate timely and accurate interventions are needed. This project will provide valuable solutions to critical care delivery by developing a virtual counterpart to the intensive care unit (ICU) bolstered with decision support to inform patient health and care delivery at multiple levels. A multidisciplinary team with engineers, scientists, and clinical professionals has established an ongoing, successful collaboration, and will be committed to this research. In addition, through this research, a diverse group of students and clinical fellows will receive a blend of interdisciplinary training in machine learning, systems engineering, and critical care medicine. This causal AI digital twin framework will be a critical leap forward in support of a more efficient medical education and eventually less error-prone bedside decision making.

The goal of this collaborative research is to tackle the challenges in critical care delivery through three integrated tasks: (1) learning a causal AI model underpinning the clinical pathway of critically ill patients, (2) investigation of optimal treatment decisions for critically ill patients in the first 24 hours, and (3) enabling system-level interventions through a digital twin framework. Supported by expert knowledge, the clinical pathway of critically ill sepsis patients will be represented by causal Bayesian networks. Computationally efficient approaches will be developed to learn the networks given high-dimensional and unobservable variables. Reinforcement learning approaches will be developed to investigate the optimal treatment for individual patients in the early stage of patient care. The Systems Engineering Initiative for Patient Safety (SEIPS) 2.0 model will be adapted to the ICU system to identify the principal factors affecting critical care delivery. Lastly, to enable the analysis of the patient-level and the process-level interactions of critical care delivery, the hybridization structure and design between agent-based simulation and discrete-event simulation will be investigated, thereby achieving a reliable hybrid simulation model. The application of this research is expected to enable an ICU digital twin platform that supports the bedside clinicians, educators, and hospital administrators to choose optimal strategies for critical care delivery, thereby mitigating risk of real-life patients.
Planning Grant: Engineering Research Center for Integrated Systems Operations and Planning of a Carbon Neutral Electric Grid
The Planning Grants for Engineering Research Centers competition was run as a pilot solicitation within the ERC program. Planning grants are not required as part of the full ERC competition, but intended to build capacity among teams to plan for convergent, center-scale engineering research.

The pathway to a sustainable carbon neutral electricity ecosystem requires a paradigm shift. While the new operational paradigm is not yet fully known, the new paradigm must cultivate an ecosystem that optimizes for stability, reliability, affordability, and carbon neutrality, while also accommodating a relentless list of needs: new stakeholders, industry models, and regulatory policies, the ever more rapid and numerous digital and technical advances, the increasing pervasiveness of the digital and technical (i.e., IoT), the variability and uncertainty in human behavior, the expected and unexpected transitions to more sustainable energy solutions, the need for flexibility and agility, and the anticipated climate variability. To achieve this, there must be a recognition of the significance of the value of the services enabled by energy rather than the energy itself. If successful, the planned ERC will be the first of its kind to bring convergence to power systems modeling and simulation with human centered modeling and simulation. The approach is critical, transformative, sustainable, and realistic, which not only leads us to a carbon-neutral energy grid, but also provides full human interaction and customer participation on a highly automated, large scale planning and operational framework.

This ERC planning grant will further identify gaps, potential stakeholders including team expansion, key personnel and evaluate the goal and overarching objectives for the future center. Additionally, the potential ERC will have a broad set of university researchers and key industrial stakeholders across a broad range of disciplines and backgrounds including (1) power systems; (2) optimization and control; (3) high performance computing; (3) software engineering; (4) computational social science; (5) data science; (6) policy and government; (7) economics; (8) behavioral science, (9) environmental economics; (10) building architecture and design; (11) transportation planning; (12) cybersecurity. There is no base of modeling and simulation tools, methods, or experience to support such an unprecedented effort. Forecasting has been a science that has evolved slowly, relying heavily on experience from past data. But new science, enabling technology, and systems integration based on revolutionary improvements in predicting interactions by prosumers, a new blend of consumer and producer who responds to the available energy supply in complex ways governed by a system of transactional energy. The proposed center liberates the proposed team from the fundamental problems existing within the scope of current funding which achieve incremental improvements; this ERC aims to achieve a quantum leap by utilizing a digital twin that simultaneously manages complex scale and complex interdependencies. The PIs? direct involvement in cutting-edge digital-twin development provides them the perspective to note that effectively addressing the challenges of integrating technologies at scale in an uncertain political, financial, and socio-economic landscape requires the type of transformative and convergent work that can only be performed through an ERC.
EAGER: SAI: Synchronizing Decision-Support via Human- and Social-centered Digital Twin Infrastructures for Coastal Communities
Strengthening American Infrastructure (SAI) is an NSF Program seeking to stimulate human-centered fundamental and potentially transformative research that strengthens America?s infrastructure. Effective infrastructure provides a strong foundation for socioeconomic vitality and broad quality of life improvement. Strong, reliable, and effective infrastructure spurs private-sector innovation, grows the economy, creates jobs, makes public-sector service provision more efficient, strengthens communities, promotes equal opportunity, protects the natural environment, enhances national security, and fuels American leadership. To achieve these goals requires expertise from across the science and engineering disciplines. SAI focuses on how knowledge of human reasoning and decision making, governance, and social and cultural processes enables the building and maintenance of effective infrastructure that improves lives and society and builds on advances in technology and engineering.

Coastal flooding and storms present a growing global challenge. This SAI project focuses on strategies, technologies, mechanisms, and policies for increasing coastal community resilience. The project centers on the use of digital twins ? virtual copies of physical objects and systems that update in real time to match real-world conditions. Digital twins can provide the insights needed to inform resilient decision making in coastal communities. An initial case study is developed through the construction of a digital twin of Galveston Island and portions of other coastal Texan communities. The research adopts a holistic and integrated approach for evaluating, modeling, and testing resilience scenarios. It brings together multiple disciplines including geography, urban planning, landscape architecture, computer science, construction science, and marine science. A participatory and community engagement platform is used to collect ground truth data and gain further in-depth understanding of coastal infrastructure mechanisms at multiple scales. Residents and stakeholders will gain insights into: (1) comparing the pros and cons of different planning efforts; (2) the joint impacts that existing and future planning efforts may have on stakeholders? individual goals and objectives; and 3) the assets and capacities involved with current dynamic sensors used in digital twin-based information modeling. Decision-makers can leverage the capabilities of this platform to test incremental and place-based planning approaches with real-time priorities, policies, and suggested infrastructure changes. Through software and hardware integration, this digital twin serves as a platform for pursuing solutions to coastal infrastructure challenges. The potential reward is high, as more informed decisions and better affordances for inter-agency coordination may lower the costs of maintaining or replacing the coastal resilience protective system. The digital twin-based decision-support framework serves as a catalyst for further research in data-driven decision making by connecting different datasets and by providing training and collaborative research opportunities for local project participants as well as graduate and undergraduate students.

This SAI project supports the resilient design, planning, and development of sustainable infrastructure in coastal communities. It integrates physical, cyber, and social infrastructure data into an analytics platform for real-time, dynamic scenario testing for decision support. This digital twin-based decision support system allows (1) collection, compiling and sharing data on physical, cyber, and social infrastructure; (2) engagement of communities to disseminate information and facilitate citizen science; and (3) promoting a human- and social-centered approach for infrastructure planning and integrated social-environment system dynamics modeling in the context of short-term disasters and long-term climate change. The digital, data-driven decision-making framework integrates a variety of data sources, digital modeling and analytics platforms, and participatory-enhanced infrastructure management considerations. It creates a visualized common operating procedure within a digital twin of local circumstances that local residents and decision-makers can use to better reason about the relationships among different planning efforts, including disaster management, new construction, repair, rehabilitation and retrofitting activities, regular maintenance, system performance, and infrastructure additions. The digital platform collects and simulates highly dynamic and massive volumes of independently-acting, reacting, and interacting agents (such as people, vehicles, structures/infrastructure, and institutions) under different policy or hazard response scenarios. Coupled with immersive technologies, the platform allows people to better understand built and natural environment changes by visualizing how planning and infrastructure alteration and addition can alter resilience levels (positively or negatively). Local knowledge is combined with expert evaluation across multiple flood scenario types and infrastructure change scenarios to test different resilience levels to urban change. By revealing fundamental design and planning principles with implications for action, the research improves U.S. infrastructure for disaster resilience, in support of science-based measures for accessible, affordable, and universal geospatial design interventions.
Planning Grant: Engineering Research Center for Connected Eldercare
Over 80% of COVID deaths in the United States were people aged 65 and older, and over 40% of these deaths took place in nursing homes and long-term care facilities. Current in-person eldercare practices caused numerous cluster infection cases in nursing facilities. The pandemic has also severely impacted home care, day care, visiting nurses, and all in-person care services in the US and worldwide, showing current nursing practices and systems are vulnerable. Those tragic cases could be better handled if in-person care could be supplemented by other means. This planning project will explore a new remote care system for supplementing all in-person services by exploiting telemedicine technology. During the pandemic, we have observed a rapid growth of telemedicine services; people can easily and safely get access to medical professionals from a distance. Once people use telemedicine and realize its usefulness, they will continue to use it even after the pandemic winds down. In this planning project, the feasibility and potential impact of if elderly people can be supported physically from a remote site, so that they can live safely and independently, will be investigated.

The goal is to provide elderly people with both informational and physical supports without on-site in-person care. Remotely-operated robots will assist elderly people in moving around and performing daily chores. Three major challenges will be investigated. First, the technology must be established for designing a walker-type, mobility-aid robot that can prevent the elderly from falling, assist the elderly in transitioning between sitting and standing, and provide a seamless support throughout daily activities. Second, under the supervision of a remote caregiver, the robot must communicate with the elderly user and gain a cooperative attitude from him/her in providing physical supports. Third and key to successful deployment of the technology, a workforce development curriculum must be created. While the technology must fit the environment of eldercare, effective and engaging curricula must be developed for training care providers. To this end, broad expertise, ranging from robotics, biomedical and rehabilitation engineering, human-machine systems, networking and AI to geriatrics, nursing, psychology, business administration, and policy, will be brought together. In this planning project, a converging research team will be formed through expert interviews, site visits, online and in-person workshops, and webinars. A broad spectrum of input will be sought from technical, social, and economics experts and stakeholders for better planning for the establishment of a new NSF engineering research center for connected eldercare.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Eldercare has been a serious social and economic challenge, impacting over 54 million Americans. Family caregivers experience mental, physical, and financial stress, while care at nursing facilities is extremely costly. The shortage of care workers is getting worse every year. The COVID-19 pandemic made eldercare even more challenging. The pandemic has severely impacted all in-person eldercare services, including long-term care in nursing facilities and in-home care. In addition to the death toll of over 800,000 older adults and disruptions in care, many have suffered from decline in physical and cognitive functioning due to prolonged social isolation.
An effective, pandemic-resilient eldercare system must be established. The objectives of this planning project are to investigate technological solutions to the serious eldercare problem and make strategic plans for overcoming the socioeconomic difficulties faced by the society. To better understand the eldercare challenges, the planning team interviewed broad stakeholders of eldercare, ranging from older adults and their families, caregivers, care service providers, physicians, and healthcare policy experts to researchers of robotics, biomechanics, rehabilitation, geriatrics, and AI. Based on the input from the diverse groups, the planning team has made strategic plans.
First, technology must be established for assisting older adults to live in their own home: Aging-in-Place. It is a traumatic experience for many older adults to leave their home and familiar community and to move to a nursing facility. Care at nursing facilities is still required as the disability level increases, but delaying transition from home to nursing facilities can not only meet older adults? desire but can also significantly reduce the cost. The second strategic plan is to extend the technologies of pre-clinical diagnosis and provide older adults with proactive support. The risk of falling, for example, can be evaluated in the home setting and necessary aids for improving body balancing can be given proactively. These support services can be provided remotely; with new technologies, telemedicine can be extended to physical telemedicine where physical aids and remote monitoring are seamlessly integrated to perform early diagnosis of medical conditions and provide safety measures and daily aid. Establishing these technologies requires multi-disciplinary, convergence research at the scale of NSF Engineering Research Centers (ERCs). The planning team now proposes to establish an NSF ERC for Connected Health and Aging-in-Place Technology, which will establish a pandemic-resilient, remote support system for older adults living in the home.
 
Intellectual Merit:
The planning team has identified two technological gaps that can be filled through convergence research:
The gap between remote monitoring and physical aid: Extending eldercare services from merely monitoring to physical assistance is a high-risk, high-payoff challenge. Physical interaction with older adults must be safe in the strictest sense. We have found it technically feasible to build new-generation physical aids that allow for safe interactions with older adults by using novel materials and components of soft robotics. A Digital Twin health watcher that keeps track of an older adult's health conditions can be constructed based on advanced sensing, testing, and simulation technologies. Evaluation of physical and cognitive functioning can be performed frequently in the home setting, and the integrated Digital Twin and physical aid system will proactively and timely provide necessary support.
The gap between in-person care and remote care: Existing physical care services are all in person. We can provide quality care services remotely with an avatar of remote caregivers that can interact with older adults not only by voice and video but also by physical and haptic effectors. A reliable and trustworthy relationship between a caregiver and the care receiver can be established through humanized interactions overcoming the distance separation.
Broader Impact:
We have found that deeper social and economic barriers, including the digital divide, disparity in healthcare access, and low self-efficacy among care workers, must be overcome, so that the new eldercare technologies can benefit broader people. The ERC planning team has created three strategic plans, which we found feasible and actionable. First, the workforce development effort can be extended to upskilling and re-skilling of care workers and those non-STEM workers having no engineering background or interest. We can develop novel curricula and outreach programs to Engage, Enlighten, and Educate (E3) those groups, who would otherwise be left behind. Disparity in healthcare service access and the digital divide are serious challenges for those financially disadvantageous people. We can develop a low-cost digital health technology, the cost of which will be less than the budget ceiling of Medicare and Medicaid. The potential market of remote eldercare is huge. The ERC can offer to its industrial members broad access to ERC-created technologies, needs assessment, and socioeconomic impact studies to benefit all Americans.
RUI: Development of Fast Methods for Solving the Boltzmann Equation through Reduced Order Models, Machine Learning, and Optimal Transport
The Boltzmann equation arises in a wide range of applications from external aerodynamics and thruster plume flows to vacuum facilities and microscale devices. Emerging applications of the Boltzmann equation include self-organizing systems and flocking, as well as networks and bacterial dynamics. Continuing technological advances in these areas require improvement of algorithms and models to enable development of a ?digital twin? for the problems at hand. While the Boltzmann equation provides the most accurate model for these systems, its use in multiple spatial dimensions remains limited due to its prohibitive computational costs. The goal of this project is to leverage data-driven reduced order models, machine learning, and optimal transport theory to make deterministic solution of the Boltzmann equation tractable so it can be applied to simulation of novel engineering applications. The project will support new courses and new training opportunities at California State University Northridge which is a minority serving institution.

The key difficulties in solving the Boltzmann equation are its high dimensionality and the prohibitive costs of evaluating the five-fold collision integral. To address these, this project will focus on the development, implementation, and validation of data driven low dimensional discretizations of the Boltzmann equation. The project will develop methods to enforce long term stability of the reduced order models and to design stable macroscopic models that are based on solution data. Fast models for kinetic equations will be developed using deep residual neural networks and numerical gradient flow approaches. Additionally, efficient evaluation of convolution on octree meshes and optimal transport formulation of the Boltzmann equation will be studied. The solvers will be validated using available deterministic high order accurate solvers. The project will deliver algorithms for three-dimensional solutions of non-continuum flows, benchmark solutions as well as new approaches to develop and test approximate kinetic and macroscopic models of gas. The techniques will increase the range of applicability of non-continuum solvers and will include multiple physics into models that were previously prohibitively expensive. The new methods will apply to the simulations of atmospheric re-entry, hypersonic flows and also gas-driven lab-on-the-chip technologies, micropropulsion, and atomic force microscopy.
SBIR Phase I: Semantic Annotation of Hypertext and its Application in Event Mapping
The broader impact of this Small Business Innovation Research (SBIR) Phase I project will be to help applications of augmented reality, location-based search, and real-time live mapping of the world, and to build a digital twin of the earth. Event information is dynamic and hyperlocal. Thus it is important to ground the location and time of events to real GPS coordinates and universal time. This creates technical challenges of entity disambiguation and resolution, and real-time processing event information to reflect last-minute changes. The proposed natural language grounding work also helps create a mirrorworld with information interlinked between the virtual internet and physical spaces and events.

This Small Business Innovation Research (SBIR) Phase I project will be the first to investigate neural language model pre-training over semi-structured hypertext on a web scale (55TB of compressed data monthly). This will greatly accelerate understanding of noisy web text, while the majority of research to date has been conducted on clean plain text only. This project will also attack the challenge of machine reading with document-level annotations in a semi-supervised fashion, while the predominance of study has been carried out with more precise word-level annotation in a fully supervised way. The technical goal of this project is to create a scalable infrastructure that allows quick iterations of mining web scale data, and an ensemble of algorithms that are adapted to learning structured information over hypertext. It is an unsolved challenge for most small businesses that in the past only the internet giants have attempted. The resulting machine learning algorithms will be capable of interpreting semi-structured web data, in contrast to typical structured annotation to understand hypertext.
EAGER: Adaptive Digital Twinning: An Immersive Visualization Framework for Structural Cyber-Physical Systems
Infrastructure systems in the United States include a diverse series of assets, systems, and networks that are vital to the nation?s economy, security, and integrity. Members of every community, ranging from individual families to global corporations, rely on these infrastructure systems to thrive and maintain a high quality of life. This infrastructure is complex, interdependent, interconnected, and diverse, encompassing the water that we drink, the power that we use, the transportation services that move us, and the communication systems that connect us. Many of these infrastructure systems that serve society today were built during the second industrial age, and in many cases are in a state of disrepair with decreasing resources to preserve them. While we have continued to improve design approaches and implement more sustainable preservation strategies, modern infrastructure systems still follow many of the historical approaches used in their early development and have not been modernized. As societal dependence on technology continues to grow, the underlying physical infrastructure systems must be preserved, but also modernized to ensure that these systems are equipped to serve as the smart and agile cyber-physical systems (CPS) the future demands. This project will explore a high-risk/high reward approach to modernizing infrastructure systems using artificial intelligence-informed digital twins. The digital twinning of an infrastructure system will form a collaborative feedback loop between the measurable data of the physical world and simulated processes in the virtual world, providing a domain-specific adaptation of the broader CPS framework necessary to inform decision-making.

Applied to the domain of large-scale structural systems, this project will test the hypothesis that immersive engagement using a digital twin representation of these structural systems will enable participants to observe, interact, and contextualize the complex behavior mechanisms associated with these systems in their operational environment. To test the hypothesis, the research design will explore a series of technology innovations including the formulation of artificial intelligence models to emulate both simulation-based results and experiment-based measurements. Leveraging these technology innovations, we will be able to 1) understand to what extent can artificial intelligence formulated models effectively emulate the complex mechanical behaviors of simulation and experimentation of large-scale structural system; 2) evaluate to what extent does the development of artificial intelligence formulated models enable the real-time, bi-directional interaction between simulation and experimentation required of a digital twin; and 3) characterize how the deployment of artificial intelligence formulated models within an immersive environment allow end users to observe and characterize operational states of an in-service structural system. Success of this work will be realized through the fusion of experimental and numerical descriptions of these complex cyber physical systems and the creation of novel processes necessary to overcome the knowledge gap that exists between the theoretical descriptions of behavior and real-life structural response, forming a foundation for real-time decision-making for structural systems in their operational environments. Results from this project will be disseminated to the broader research community through refereed journals, conference proceedings, and student dissertations.
CPS: Medium: Hybrid Twins for Urban Transportation: From Intersections to Citywide Management

This Cyber-Physical Systems (CPS) grant will focus on the development of an urban traffic management system, which is driven by public needs for improved safety, mobility, and reliability within metropolitan areas. Future cities will be radically transformed by the Internet of Things (IoT), which will provide ubiquitous connectivity between physical infrastructure, mobile assets, humans, and control systems. In particular, IoT and smart traffic management have the potential to significantly improve increasingly faltering transportation systems that account for over 25% of greenhouse gas emissions and over one trillion dollars of annual economic and social loss. The project develops a hybrid twin that operates in parallel with the real world at real-time resolution, leveraging machine learning and edge computing, to monitor surrounding traffic, send safety warnings to connected vulnerable users, and provide learning-based controls to traffic lights and automated vehicles. As such, the broader impacts include advancing the understanding of urban traffic modeling, computation, and simulation, and enriching transportation science with data science. The accompanying educational plan aims to broaden participation in computing and engineering by underrepresented minorities and women via outreach programs, including programs for Harlem public school teachers and K-12 students, as well as new graduate course development.

The project?s goal is to develop a hierarchical and distributed hybrid twin to support urban traffic management systems while leveraging Artificial Intelligence (AI), edge cloud computing, and next generation communication networks. A hybrid twin consists of a virtual (i.e., existing traffic simulation) and a digital twin, which integrate physics-based models and assimilate data acquired from infrastructure and in-vehicle sensors for traffic modeling, prediction, and management. The foundational research contributions are data analytics and machine learning including real-time learning for control. The traffic management system will be validated and evaluated via computer simulation and experimentation in the NSF PAWR COSMOS city-scale wireless testbed that is being deployed in West Harlem next to the Columbia campus. This unique urban testbed will provide a realistic environment for the system design and evaluation process, and will also serve as a platform for local community outreach.
SCC-IRG Track 2: Digital Twin City for Age-friendly Communities - Crowd-biosensing of Environmental Distress for Older Adults
Older adults bear a high risk of multiple chronic diseases and face high healthcare cost expenditures. Physical activity such as walking is considered the most effective preventive strategy to reduce health risks and burdens. Strong evidence shows that built environmental factors such as walkability are strongly linked with older adults? walking and overall physical activity, with frail older adults being more susceptible to environmental barriers/challenges. However, traditional urban planning/design practices that target the ?average person? have failed to meet the special needs of older adults experiencing age-related physiological and psychosocial conditions. As a result, neighborhood environments in most communities can be the source of significant physical and emotional distress to older adults, thereby inhibiting their mobility and outdoor physical activity. This project thus aims to (1) create a digital twin city (DTC) model that reveals older adults? collective distress and associated environmental conditions, and (2) leverage the DTC model to develop and implement technological and environmental interventions that alleviate such distress and promote older adults? independent mobility and physical activity. The DTC model will be constructed by matching or twinning crowdsourced biosignals (i.e., physiological sensing data from unobtrusive wearable devices worn by older adults) with street-level visual data from participatory sensing and Google Street View, enabling the establishment of a city?s affective map. Thus, this project will leverage the DTC model to design, implement, and/or evaluate stress-responsive interventions, in collaboration with local stakeholders and older adults in an underserved neighborhood in Houston, TX. The empirical outcomes of our work will quantify the experience of community members on a daily basis, which will promote productive conversations and targeted interventions at both the individual and the community levels. While this research leverages Houston, TX as a testbed, research findings are expected to generalize to other communities in the U.S. and artifacts from this work (e.g., DTC model) will be made widely available to the general public.

This project will make methodological advancements in (1) constructing group-specific saliency models from reliable physiological measures, (2) developing personalized distress prediction using multimodal data, and (3) formulating distress-responsive routing algorithms. The research team with community partners will incorporate these outcomes into designing, implementing, and evaluation three technology/environmental stress-responsive interventions: (1) providing community affective maps to enhance older adults? awareness of environmental stressors in their communities; (2) offering stress-responsive routing plans that optimize older adults? route plans responding to individuals? health and mobility conditions; and (3) implementing environmental interventions (i.e., promoting age-friendly Privately Owned Public Space (POPS) redesigning/retrofitting neighborhood infrastructure/facilities) to alleviate location-based distress. Additionally, the community engagement activities contained within this project will critically enhance older adults? understanding of safety risks and of mobility challenges in the community environment and will contribute to the community?s capacity building in promoting health and reducing distress.
NSF Convergence Accelerator Track E: A Globally Coordinated, Universally-Accessible Digital Twin Network for the Coral Reef Blue Economy

OIA - 2137882 NSF Convergence Accelerator Track E: A Globally Coordinated, Universally-Accessible Digital Twin Network for the Coral Reef Blue EconomyThis project proposes to develop a ?digital twin? technology to improve stewardship of coral reef ecosystems. Digital twins are virtual replicas and the use of this technology is growing in many sectors, providing opportunities to collaborate virtually, visualize entire systems, intake sensor data and update system status in real time, design what-if scenarios, predict results of proposed interventions, and create strategies to improve the real-world features that the twin represents virtually. This technology has not yet been applied to analysis and stewardship of coral reefs but has the potential to facilitate the collaboration among diverse interest groups that is needed to preserve these crucial ecosystems. The team has identified three critical gaps that limit the utility of scientific knowledge in the management, conservation, and restoration of coral reef ecosystems which they are confident can be addressed by the digital twin approach: i) lack of a whole systems approach, ii) absence of a global platform for data integration, analysis and visualization, and iii) lack of universal access to data and knowledge, which in turn prohibits sharing and collaboration. The end goal of the effort is a global-scale, interconnected network of digital reefs with the potential to transform the management, conservation, restoration, and sustainable harvest of coral ecosystems for the 21st century blue economy.

Coral reef ecosystems play a central role in the global blue economy. In the US, coral reefs contribute billions of dollars to the blue economy each year, create jobs, and protect coastal infrastructure. However, coral reefs everywhere are declining at a pace and scale unprecedented in human history. This project incorporates valuable diversity and expertise, including includes the University of Guam, an accredited Asian American, native American, Pacific Islander-serving institution; the Marshall Islands Conservation Society (MICS), with strong stakeholder interests in coral reef sustainability; and the Nature Conservancy, whose coral reef program is established in over thirty countries around the world. All products generated as part of this research will be made publicly available via a project-specific website and existing portals as well as other media such as film, gamification, and collaborations with large public aquariums in the US. This strong network will help ensure co-development with a broad range of stakeholders and global utilization of the tools developed.

The team will develop the prototype Coral Reef digital twin on Palmyra Atoll, a US territory in the Pacific, and then during Phase 2 will expand the digital twin model to priority sites identified by collaborators from federal agencies and conservation organizations. The 3-dimensional virtual replica of a living reef will facilitate the integration, analysis and accessibility of a diversity of geological, physical, chemical, biological and socio-economic data and models from anywhere in the world. The data will be incorporated into a holistic representation of the living reef that can be visualized in 3-D, analyzed at any point in space and time, and simulated under different, future scenarios. Connection between the physical reef and its digital replica, via sensors, robotics and satellites, will allow the digital twin to receive and integrate updates on coral reef status in near-real time, providing critical information to managers, restoration practitioners, and stakeholders, including tourism operators, fishermen and coastal communities.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Coral reefs support the livelihoods of 1 billion people globally, and hundreds of thousands of jobs in the United States alone. Yet these valuable ecosystems are threatened by climate change, fueling an urgent, world-wide push for novel ways to ensure their survival. Here, a unique team of ocean scientists, software engineers, conservation organizations and stakeholders, drawing on use inspired research, laid the groundwork for the first Coral Reef Digital Twin, a novel software platform delivering state-of-the-art scientific data to decision-makers everywhere, as intuitive, immersive, interactive 4-dimensional visualizations.  Digital Reefs? empowers users to test the efficacy of various management scenarios before they are enacted, to easily share visualizations with stakeholders and investors, and to continually update, upload and geo-locate new information into each digital reef twin.
Academic scientists, post-doctoral researchers and graduate students gained critical new skills in convergence research, prototype development, logo design, marketing, communication and storytelling, pitching and use inspired research while partners in the technology industry found unique opportunities in collaborating with scientists and ocean conservationists to develop solutions to environmental challenges. A low-fidelity prototype was developed, based on the hydrodynamic model of Palmyra Atoll, a US coral reef territory in the central Pacific, managed by the US Fish and Wildlife Service (USFW). Scenarios based on actual USFW management decisions for Palmyra were developed and tested in the prototype and shared with USFW personnel. A Digital Reefs? logo and tagline was designed and copyrighted, and a website was established.
Digital Reefs? received enthusiastic support and valuable input from a wide range of potential users including NOAA?s Pacific Islands Fisheries Science Office, the US Fish and Wildlife Service, the National Academy for Marine Resources in Taiwan and The Nature Conservancy.  Digital Reefs? also garnered substantial media attention, featuring in Curiosity Stream?s ?Reefs of Hope? and The Monocle Companions ?50 Essays for a Brighter Future?.  Finally, Digital Reefs? was nominated to Fast Companies? annual list of World Changing Ideas (outcome pending).
SCC-CIVIC-FA Track B UNUM: Unification for Underground Resilience Measures
The importance of extreme events from climate change, weather, and accidents continue to have critical impacts on infrastructure. Underground infrastructures are interconnected within one another and with above ground infrastructure. These interdependencies often have cascading impacts on electric power, water, transportation and communication with extreme social and economic impacts. Increasing community resilience depends upon identifying underground infrastructure data and data sharing. Such underground infrastructure data are not readily known, available, or interoperable. Furthermore, they are typically under multiple jurisdictions. The UNUM project provides a collaborative mechanism for data sharing among government, industry, utilities, and community groups with different data security requirements. Two pilot areas are used within New York City, midtown East (Manhattan) and Sunset Park (Brooklyn), contrasting in the types of underground conditions, stakeholders, and demographic and economic diversity. In Stage 1, commitments to participate in data sharing were obtained from over 40 stakeholders through a data repository managed by a City agency that provide the foundation for Stage 2. This Stage 2 project has broader impacts in providing a roadmap and two testbeds for a data model, data needs, sharing, and security protocols for underground infrastructures transferable to other areas to improve their resilience.

Goals and Scope of Research and Methods and Approaches: UNUM Phase 2 expects to develop a methodology to standardize underground infrastructure and geological data sharing so that all subsurface utility information is made interoperable: capable of being brought together on a common basemap for integration and analysis. This will better enable (1) to identify hazard and disaster vulnerabilities, (2) devise strategies to reduce accidents and emergencies, and (3) to harden infrastructure in case of natural disasters and other large-scale threats like sea level rise and global warming. Based on relationships and agreements with utilities established in UNUM Stage 1, UNUM Stage 2 will collect infrastructure data related to water and sewer networks (NYC DEP), transit (MTA), electric power (Con Edison and National Grid), etc. Subsurface utility data will be transformed into a common format, registered to NYC?s photogrammetric basemap, and securely stored. The data will conform to the MUDDI data model now in advanced development by the Open Geospatial Consortium, and will be closely related to underground infrastructure ASCE 38 and 75 standards for engineering quality and accuracy. The UNUM team will co-develop with community groups art and engineering activities to improve community-based infrastructure literacy.

This project is part of the CIVIC Innovation Challenge which is a collaboration of NSF, Department of Energy Vehicle Technology Office, Department of Homeland Security Science and Technology Directorate and Federal Emergency Management Agency.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Overview:
The UNification for Underground resilience Measures (UNUM) project began as a once in a generation opportunity to begin to overcome New York City's siloed and non-interoperable subsurface utility data challenges to enhance community resiliency and improve its infrastructure delivery, management, and renewal in the face of persistent and often catastrophic natural hazards. Through the academic leadership of New York University multii-disciplinary faculty and unflagging dedication and expertise from UNUM's non-academic community leaders, the project's industry-based advisory board, and more than 50 community, not-for-profit, municipal, and industrial partners, UNUM has not only achieved that but impacted the national dialog and helped revolutionize expectations for large parts of the geospatial community by demonstrating the criticality and economic necessity of including detailed and accurate subsurface documentation in all governmental geospatial systems. Fundamental to these efforts has been a sustained and tiered set of engagements with (1) local residents and community leadership; (2) private utility owners, municipal and state-level utility providers, and elected officials; and (3) national and international thought leaders in business, government, and professional societies.
 Intellectual Merit:
 
In order to advance knowledge in these areas, interactions to support community engagement included nearly 400 meetings, presentations, and events reaching a cumulative audience of 7,800. This included , 2 focus groups and accompanying scripts, 3 surveys, 2 multi-stakeholder information sessions, 2 major technology demonstrations, a 100+ person professional workshop, 3 community workshops, 1 community-wide event, 1 month-long public art installation, 1 week-long utility-inspector workshop, 1 new data exchange model and accompanying international standard, 4 non-disclosure templates, 1 new security protocol for transferring sensitive utility data, 1 pilot project, 5 keynote talks, 29 conference presentations and panels, 1 webinar, 1 podcast, 5 short films, and the creation of a community driven digital twin App available through Apple and Google App stores.  
 
Broader Impacts:
The UNUM project produced multiple, diverse social benefits through education, training, job opportunities, and program development initiatives. It  created youth employment for 7 local high school students, additional training opportunities for 2 other high school students, 2 masters' students, 2 doctoral students (1 NYU and 1 visiting from the University of Stuttgart), and 1 post-doctoral researcher. This was done through a combination of UNUM related summer and academic-year research opportunities including masters' level capstone projects. Additionally, information from the UNUM project became the basis for a new, permanent graduate level class and informed the reshaping of a second one affiliated with NYU's Urban Systems' doctoral program. 
 
UNUM also produced for public usage three-dimensional (3D) point cloud models for much of the one study neighborhood area, a high-resolution 3D model of a New York City (NYC) subway station, and a readily accessible georeferenced, neighborhood scale, data set related to subsurface utilities that is the basis for an expanding electronic community resource with nearly 100 unique layers. These initiatives provide transferable and scalable frameworks for other communities
 
Beyond the UNUM pilot project that was successfully transferred to NYC's Mayor's Office of Operation along with an accompanying $10 million dollar investment for its extension, UNUM has transformed the relationship between NYC's municipal agencies and the local, private utility companies. Based on the trust and common vision forged through the UNUM project, there has been a memorandum of understanding signed by two major professional societies, and at least 3 on-going municipal initiatives among NYC public and private utility providers in the areas of (i) subsurface value engineering, (ii) municipal electrification, and (iii) the extension of the UNUM pilot to other neighborhoods. Prior to UNUM, these entities lacked knowledge of and coordination with the other public and private players. In doing so, UNUM has created a successful model that benefits all participating organizations and transferable beyond the study area. This includes overcoming many historical obstacles related to a general lack of trust, inadequate coordinated leadership, insufficient funding, security concerns, data incompatibility issues, and an absence of a common vision.
SCC-PG: Improving healthcare access in marginalized communities through smart connected technologies
This planning grant aims to deliver better healthcare to and encourage health-promotion behavioral changes among seniors from marginalized communities, by strengthening access to health care, services, and resources. The project envisions a Smart Health Access and Resource Portal (SHARP), which offers both an interface through which individuals gain access to care and resources in health systems, and an engine that designs, recommends, and operationalizes virtual and physical access. The results from this project will accumulate knowledge about the existing healthcare barriers faced by seniors from marginalized communities, reveal their health-related preferences and choice behaviors, and prescribe potential solutions to strengthening their physical and virtual access while maintaining privacy to health care, services, and resources. They will also lay the foundation for a future integrative research proposal that will implement, deploy, and evaluate SHARP. If succeeded, the seniors who participate in the SHARP pilot study will see tangible health benefits gained from better access to and engagement with healthcare systems. Improving the health of seniors contributes to the overall wellbeing of the society and helps lower healthcare costs. In the long run, SHARP can be deployed in other marginalized/underserved communities to help build a healthier and more equitable society. Research results will be broadly disseminated and adopted through publications/presentations, collaborations with industry partners, as well as engagement with various community stakeholders.


This integrative research spans across social science and technology domains. Central to the promise of the proposed solution is a holistic approach to integrating virtual and physical access. The project assembles a team of experts from Northwestern University (NU), Kaizen Health (KH), and Center for Neighborhood Technology (CNT), with expertise in transportation and mobility systems, computer engineering, behavioral science, social epidemiology, human-computer interaction and informatic, healthcare logistics, and community engagement. The team will work with a range of community partners led by CNT to identify and engage seniors living in selected low-income neighborhoods located in the southside of Chicago. The project will bring together academic, industry and community stakeholders to (i) understand needs, priorities and barriers related to healthcare and mobility via dialogue and discussions; (ii) assess the needs for an integrated digital tool that addresses the identified transportation and healthcare access challenges, and (iii) build partnerships for a meaningful future local pilot study. To fulfill these community engagement objectives, the project will conduct community-centered activities such as Steering Committee Meeting, Expert Panel, Listening Session, and Survey. The project will mine various data collected in this research and acquired from other sources. The findings from data mining will lay the foundation for new socio-behavioral theory that explores the complex relationship between mobility and health and between healthcare accessibility and broad health-related behaviors (e.g., health monitoring and management). They will also inspire new human-computer interaction methods that aim to reveal how seniors currently interact with digital tools, establish the correlations between such interactions and health-related behaviors, and assess their needs for an integrated health tool like SHARP. Taken together, the project will deepen our understanding of (i) the barrier to healthcare access among seniors from marginalized communities, and (ii) institutional and technological challenges in integrating conventional and emerging mobility services from both private and public sectors. The project will also lead to the creation of a suite of analytical and computational tools that form the core intelligence of SHARP. These include (i) a human digital twin model and a novel resource allocation model that address the choice between virtual and physical access at the individual and the system level, respectively; and (ii) mathematical models and solution algorithms supporting the design and operation of an integrated transportation and health logistics system.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This Smart and Connected Community Planning Grant project aims to deliver better healthcare to and encourage health-promotion behavioral changes among seniors from marginalized communities, by strengthening access to healthcare, services, and resources. Central to the promise of the proposed solution is a holistic approach to integrating virtual and physical access.  The project lays the foundation for the design, implementation, and evaluation of a Smart Health Assistance and Resource Portal (SHARP), envisioned as both an interface through which individuals gain access to care and resources in health systems, and an engine that designs, recommends, and operationalizes virtual and physical access.   
 
We organized two virtual expert panel sessions (January and May of 2022) to map out the connections between health, access to transportation, and digital technology. The participants represent public transportation agencies, medical professionals, health logistics providers, advocacy groups, and community organizations.  We also organized two in-person community listening sessions in Belmont Cragin (June 2022) and Washington Park (October 2022) including 40 total participants to hear directly from community members on qualitative experiences of (barriers to) health access. Listening session transcripts were analyzed by researchers using qualitative concept coding. Seven key findings about older adults’ experience with health were extracted from coding results: 1) the promise and pitfalls of technology, 2) coping and advocacy strategies, 3) communication challenges with older adults, 4) subtext and invisible topics, 5) contextual ‘outside’ understanding of code, 6) interface challenges between systems, 7) negative bent to discussion and preference for in-person care. The analysis highlighted the need to center the lived experiences of vulnerable populations to understand healthcare access barriers faced by low-income older adults. The analysis resulted in the paper ‘“I’m pissed off with the whole system”: A qualitative analysis of barriers to healthcare access faced by low-income older adults in Chicago” submitted to the journal Transport and Health and currently under review.
 
We analyzed smartphone-based location intelligence data acquired from a Europe-based data vendor to determine the suitability of using such data to understand the ground truth of healthcare accessibility and related mobility issues in the study area (five communities located on the West and South sides of Chicago as the study area of the project, including Belmont Cragin, Hyde Park, Kenwood, Washington Park, and Woodlawn).  We found a compelling paradox in the data consistent across multiple years of analysis: namely a negative relationship between income and representation in the data, as well as a negative relationship between density and representation.  Overall, our analysis, summarized in two publications, highlights the potential and pitfalls of utilizing this data source in our study, and sheds light on the ways by which the limitations can be mitigated.
 
As an attempt to alleviate the healthcare access barrier by improving public transportation, we studied transit design methods that place accessibility and equity at the center of the tradeoff. By incorporating ethical theories as a guiding principle, the method promises to improve vertical equity through ethics-awareness. We found the egalitarian design has a prominent equity-enhancing effect, whereas the utilitarian design often exacerbates inequality, especially when the initial access to opportunities is unequal and the planner is given greater flexibility to spatially differentiate services.
 
We obtained IRB approval and conducted an online survey to improve our understanding of the linkage between health and transportation access in a national sample of older US adults.
 
We actively reached out to several telehealth startups, as well as health establishments, to secure a partner willing to work with us on a pilot study that directly engages with patients.  These include Hoy Health Inc., Elation Health, Epic, and Northwestern Medicine.  We reached a tentative agreement with a team at Northwestern Medicine about collaborating on the full proposal.  Importantly, we settled on a plan to leverage, expand, and integrate the functionalities of the care-management tools currently in use at Northwestern Medicine and the health logistics tool developed by Kaizen Health for the implementation of SHARP.
 
The project provided training opportunities for several graduate students who learned about data-cleaning and processing, cell phone data analytics and validation, qualitative community research methods, as well as research design. Students were exposed to cross-disciplinary research, worked with a diversity of data types, and were provided close mentoring to become active members of the research project. This is made evident by the fact that Ph.D. students are the first authors of all the papers generated by the team.
SCC-PG: SmartCurb: Building Smart Urban Curb Environments
U.S. cities are witnessing an era of transformative innovations in electric vehicles (EVs), autonomous vehicles, on-vehicle electronics, Global Position System, mobile devices, digital maps, and numerous apps that assist driving and parking. However, the advance in curb environments where vehicles operate has not kept pace. Curb environments serve as a unique nexus that connects on-road traffic and pedestrian sidewalks across urban communities, but are burdened in urban cores due to space competition for pick-ups and drop-offs, freight loading, EVs charging, bicycle, and scooter parking. This NSF Smart & Connected Community planning grant studies curbside environments at the downtown and University of Florida (UF) campus communities in the City of Gainesville, Florida. It focuses on how to integrate vehicles, people, mobile devices, physical and cyber infrastructures to coordinate curb space uses. Collectively, these innovations will maximize equitable and convenient access while minimizing greenhouse gas emissions for healthy and sustainable communities, relieving congestion at curb spaces, and boosting livability for community residents. The project explores important, emerging challenges faced by cities across America. The knowledge learned will be shared with local communities, who will benefit in the long term, to prepare city curbs for future burgeoning technology and mobility innovations.

The project seeks to understand curb space uses of urban communities and to develop strategic management to adapt increasingly diverse and conflicting curb space uses in response to emerging vehicular technologies and mobility innovations. Several key interdisciplinary research questions are addressed, including (i) how sensor data from curb environments, vehicles, and human mobile devices can be collected, curated, and correlated jointly by separate transportation and parking management entities? (ii) how to design, plan, and manage curb environments to address congestion, safety, and accessibility issues collectively across cyberinfrastructure in the urban communities? (iii) how to precisely predict the evolvement of curb uses across time and space in the future? (iv) how to creatively coordinate various curb uses in real time? (v) what are the potential privacy issues to vehicles and people at curb environments when surveillance, sensing, and data analyses are performed, and how to design innovative technologies to mitigate privacy threats? The project team will form novel research problems from socio-technical perspectives in the context of campus-downtown settings, develop academic and community partnerships with the necessary knowledge to address the problems, and prepare testbed ?Smart Curb? environments to be included in a future SCC-IRG proposal.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The planning grant advances the understanding of different curb environments, dynamic and diverse curb uses, and their relations that shape strategic management and techniques. This understanding is critical for adapting to increasingly diverse and conflicting curb space uses in response to emerging vehicular technologies and mobility innovations. The project pilots the downtown and University of Florida (UF) campus communities in the City of Gainesville, Florida as the study site, engaging with a spectrum of community stakeholders to discern challenges in various curb environments. 
A primary facet of the project involves comprehensive data collection to identify curb use problems and facilitate the preliminary development of a SmartCurb digital twin. This entails gathering data from open data portals, the local stakeholders, and commercial platforms for the study site, including parking transaction records of and on-street parking violation citations from the City of Gainesville transportation office; dynamic curb access demands; geospatial data delineating the curb facilities and environments. Stakeholder interviews, surveys, and analysis of pertinent reports contribute additional dimensions to the understanding of residents' on-street parking experiences. 
Our empirical studies, approached from an urban planning perspective, leverage multi-sourced big data to model diverse uses. For instance, one study analyzes downtown curb parking and violation datasets, identifying potential curbs suitable for Pick-Up and Drop-Off zones. Another study employs a MultiGCN-LSTM deep learning model to predict the spatiotemporal dynamics of diverse curb uses in Gainesville under varying curb management practices and built-environment changes. These studies offer valuable insights to inform evidence-based urban curb management and planning.
The project extends its focus to the realm of electric vehicles (EVs), using them as a proxy for curb use.  A dynamic coordinated joint routing and en-route charging station selection mechanism (DcRC) is developed to provide guidance for a group of EVs mixed with internal combustion engine (ICE) vehicles. The approach effectively mitigates the overreaction effect without compromising individual vehicles’ selfish interests. Efficient computation is ensured through a distributed ADMM solution approach combined with a customized branch and bound algorithm. Moreover, the project also delves into a robust planning model that explores the optimal settings and capacity range planning of charging stations so that they cover the maximum EV demands with a high Level of Service while minimizing the traffic impact. 
In addition, the project explores edge-cloud computing for video analysis of vehicle traffic, data streaming algorithms, and privacy-preserving AI model training in the cloud. A novel edge-cloud computation model is experimented with, offering improved efficiency in processing videos recorded from smart curbs. This involves executing a vehicle identification and tracking application program between edge devices and the cloud. Privacy concerns are addressed by transforming data with matrix masking, enabling the outsourcing of privacy-sensitive data to the cloud in a masked form for AI model training.

Broader Impact
Well-coordinated curb space uses can help reduce on-road traffic congestion, GHG emission, achieve traffic safety objectives, improve accessibility and well-being, and further benefit the living environment and commercial development in college cities and downtown communities. The project actively addresses emerging challenges faced by downtowns across the U.S., sharing acquired knowledge with local communities to aid in their preparation of curb environments for evolving technologies and mobility innovations. Deep engagement with local stakeholders and dissemination of research findings in various forums contribute to the project's broader impact. We have also developed data-driven cyber-infrastructure for digital data collection across platforms and data modalities. The project has contributed to the education of six STEM graduate students from urban planning, computer science and transportation engineering. The research and engagement opportunities have also enriched the research agenda of the junior scholars involved in the project and promoted truly interdisciplinary research across backgrounds.
RAPID: Collaborative Research: A Modeling Based Investigation in Support of Pioneer Array Relocation Design in the Southern Mid-Atlantic Bight
Through a series of NSF sponsored Pioneer Array Innovations Labs, the new location for the NSF Ocean Observatories Initiative (OOI) Pioneer Coastal Array will be relocated to the Southern Mid-Atlantic Bight (S-MAB), in the region between Cape Hatteras, NC, and Norfolk Canyon, VA. In order to inform specific array design decisions, such as mooring siting, spacing, and instrument/sensor selection and configuration it is important to perform ocean numerical modeling analysis of the complex and dynamic ocean circulation to provide guidance to the OOI-CGSN Pioneer Array system design and build group that meets their published milestones/constraints for the design and build-out timeline. The results of this modelling effort would strengthen future model-based co-design of NSF-supported field programs, including the next Pioneer redeployment in another five (5) years. This project offers opportunities to develop strong partnerships with stakeholders in academia, government, and industry sectors, including marine fisheries and offshore renewable energy (wind and hydrokinetic).


The PI's proposes to perform a suite of numerical modeling investigations and analyses to deliver actionable guidance to the OOI-CGSN Pioneer Array system design and build group that meets their published milestones/constraints for the design and build-out timeline. Skillful ocean modeling and analysis will offer a virtual ocean framework ? a ?digital twin? ocean ? that can be used to inform specific array design decisions, such as mooring siting, spacing, and instrument/sensor selection and configuration.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The NSF Ocean Observatories Initiative (OOI) Pioneer Coastal Array is a relocatable observing network. Following an NSF-sponsored Pioneer Array Innovations Labs community workshop it was decided to recover the array from the New England Shelf and redeploy it to the Southern Mid-Atlantic Bight (S-MAB) with geographic footprint in the general area between Cape Hatteras, NC, and Norfolk Canyon, VA. In this region ocean observations will be made to inform science questions that include:
•    What are the roles of land-ocean interactions, shelf-sea/deep-ocean interactions, and intrinsic variability, in driving the export of S-MAB waters into the Slope Sea?
•    How do cross-shelf transports impact regional biogeochemical cycling and marine ecosystem dynamics?
To address these questions, the array's overall location, and the sites within the array of individual profiling moorings need to be selected to maximize the likelihood of achieving an independent yet complementary network of observations that capture dominant shelf water export pathways. In collaboration with North Carolina State University (NCSU) colleagues we used data assimilative ocean model reanalyses to evaluate the potential sites of the array moorings with respect to the science objectives.
In the course of the study, NC State and Rutgers PIs held numerous consultations with the OOI Coastal and Global Scale Node team (OOI-CGSN) at WHOI who were responsible for the S-MAB Pioneer Array design, construction and site characterization. In addition to the central latitude of the array, various T-shaped versus W-shaped mooring site arrangements, with greater or lesser along-shelf extent and/or density of moorings in shallow water were considered as candidate designs.
Leveraging existing models and archived model outputs from the NSF PEACH project, NC State conducted Lagrangian particle simulations to visualize transports pathways originating on the inner shelf and from within Chesapeake Bay, charting the routes to ultimate cross-shelf export to the Slope Sea. These results confirmed that the most probable across shelf pathways would fluctuate across the central latitude of the final T-shaped design, as hoped. They further guided the choice of alongshelf spread of the deep mooring sites on the continental shelfbreak so that substantial export north of the array would be improbable.
In complementary work, the Rutgers group used adjoint passive tracer simulations to trace pathways backward in time from the proposed mooring sites to the source regions several months previously. This confirmed the array would sample the full across-shelf extent of southwestward flow on the shelf, corroborating the NC State conclusions but extending consideration to source waters of the outer MAB shelf. Furthermore, the analysis highlighted that waters originating south of Cape Hatteras would make relatively frequent excursions into the vicinity of the Array, which will inform future experimental designs by users in their analysis of Pioneer observations. 
This model-based analysis represents possibly the first time that data-informed ocean models have been used to assess, and indeed modify, the design of a fixed mooring array before the array was deployed.
SBIR Phase I: A software-based tool for beyond visual line of sight (BVLOS) drone's connection reliability enhancement
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to improve standards for safety-critical connectivity of drones flying Beyond Visual Line of Sight (BVLOS). The drone communication market requires superior link reliability to BVLOS drone applications at an affordable price. The proposed technologies seek to control BVLOS operations enabling improved safety and increased trust in autonomous airspace systems from both regulatory authorities and society. Network-based solutions to improve airborne links require costly enhancements to the network and a large number of drone devices to be profitable. The team is focused on solving aerial command and control problems using the current mobile network; The technology seeks full integration with the 5G network and compatibility with subsequent network generations.

This Small Business Innovation Research (SBIR) Phase I project aims to develop and de-risk the connection reliability enhancement algorithms for Beyond Visual Line of Sight (BVLOS) drones, enhancing connection reliability. The technological improvements allow use of any network generation (2G/3G/4G/5G), delivering connection reliability to 95% of the global population. The team has three enabling innovations: 1) voice channel communication redundancy which translates commands into audio signals transmitted over the voice channel, 2) an algorithm for handover (HO) optimization which minimizes ping-pong HO, radio link failures (RLF) and connection losses, and 3) safe route planning which allows drone operators to select the best route for reliable connectivity given the network circumstances.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Existing solutions for control of Beyond Visual Line of Sight (BVLOS) drones are based on expensive satellite connections, or mobile networks modem-based products, which suffer from limited range, frequent interruptions, and capacity overloads. Mobile networks were not designed to control Unmanned Aerial Vehicles (UAVs), and their limited capabilities prevent safe BVLOS UAV applications. Base station (BS) antennas are down-tilted toward ground users which causes the UAVs to be served by the weak antenna side lobes. Such BS antenna characteristics create scattered coverage patterns in the sky which, in turn, makes mobility management of cellular-connected UAVs unreliable. UAVs also suffer from higher interference than ground users due to high line-of-sight probabilities with neighboring BSs. Changes of the network infrastructure for UAV use cases is not economically viable, given that this market is still emerging. Additionally, hardware-based solutions are costly and not scalable. Current solutions require large and heavy custom hardware and can't be integrated using certified Off-The-Shelf (OTS) modems, which hinders their adoption in this highly regulated industry sector. To address the limitations in mobile network reliability for BVLOS drones, Voltela developed an integrated software solution VolTech to enhance connection reliability of Long-Term Evolution (LTE) wireless standard based BVLOS drones. VolTech is a hardware-independent solution that can be operated with any OTS modem to translate narrowband telemetry data into audio signals for reliable and low-latency transmission over the voice channel.
Voltela built a database collecting connectivity metadata based on real-world drone flights over different areas (rural/suburban areas, metropolis areas) to investigate the circumstances of connection failures such as handover issues, interference, and radio link failure in representative air-space scenarios. The collected data contained features such as Global Navigation Satellite System (GNSS) position, altitude, receive signal strength of all visible BSs, cell ID, signal-to-interference-plus-noise ratio (SINR). 
VolTech includes a solution for mapping network coverage with high accuracy.  The critical component in predicting network availability in airspace is the network propagation behavior, which is unique compared to ground propagation due to the side lobes characteristics and line of sight visibility to many base stations. The network coverage map provides a visualization of aerial links and helps the drone to select the optimal route and set of BSs to minimize connection failures. The network mapping simulator was validated with Reference Signal Received Power (RSRP) values for all measurement points. In almost all points, the RSRP was predicted with low error thus establishing the high accuracy of the simulator.
To enhance the connection reliability and minimize connectivity failures of LTE-based BVLOS drones, data-over-voice technology (i.e. combining Voice and UDP packets) and dual modem solutions were implemented. The connection discontinuities were distinguished depending on their length as there exist a correlation between the duration of discontinuity and the safety risk due to the connection failure. VolTech reduced the number of discontinuities (connectivity failures). The reliability performance (for latency < 500 ms) improved from 90 % to 99.9 % for the combined measurement of train ride in very rural/remote areas and drone flight, whereas drone flight measurements alone yielded far better results of 99.99% reliability. Considering data from drone flights only, VolTech is not far from the targeted 99.999% reliability (for latency < 500 ms). Additionally, Voltela developed the following new solutions: 1) Link predictions based on digital twin, 2) Base station / Frequency band selection, and 3) Guaranteed link reliability, evaluated their potential with respect to the connection failure causes observed, and identified paths for a full implementation. With the implementation of these solutions, the connection reliability performance can be improved beyond 99.999%. Thus, Voltela's integrated software-based solution VolTech demonstrate superior reliability and improved performance for LTE based BVLOS drones using the voice channel redundancy of the existing mobile network. Safe BVLOS will bring more reliability and trust in autonomous systems and expand commercial drone applications multifold.

SBIR Phase I: Patient Digital Twin and Performance Capture System for Scoliosis Physiotherapy
The broader impact /commercial potential of this Small Business Innovation Research (SBIR) Phase I project focuses on scoliosis physiotherepy. Physiotherapy Scoliosis Specific Exercises (PSSEs) have been proved effective in treating scoliosis patients; However, PSSE effectiveness can be significantly reduced due to inappropriate planning, unclear requirements, and poor compliance. This project seeks to develop a hardware-software integrated system which employs a synthetic, immersive environment for physiotherapists and patients to conduct PSSEs effectively. The technology may save costs for patients, expand services for clinics, and improve the outcomes of scoliosis physiotherapy. The commercialization of the technology will be conducted via business to business and business to customer mechanisms.

This Small Business Innovation Research (SBIR) Phase I project focuses on the development a hardware-software integrated system which employs a patient digital twin (PDT) and a performance capture system to create a synthetic, immersive environment for physiotherapists and patients to utilize telehealth communication between physiotherapists and scoliosis patients. The technology seeks to improve the accuracy and completeness of diagnosis and assessment, optimize physiotherapy design and planning, facilitate effective patient communication and education, and help patients to perform physiotherapeutic exercises according to instructions and compliance requirements. By using PDT, the diagnosis of scoliosis and assessment of spine deformity can be performed in 3D space thus improving the completeness of assessment and the design of PSSEs. By building a light-weight performance capture system, body shape, pose, and motion can be captured in 3D space in real time. The teams seeks to represent the prescribed PSSEs in 3D games by animating the PDT with motion captured during clinic practice. In this way, the actual poses and motions of a patient during home exercise can be monitored, evaluated, and used to instruct with adjusted 3D games as reference. The goal of Phase I is to demonstrate the technical and commercial feasibility of the proposed innovation, which will be accomplished through a technical feasibility study, initial prototyping, and a commercial feasibility demonstration.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Scoliosis is an abnormal curvature of the spine that affects the quality of life of approximately seven million people in the United States. Physiotherapy Scoliosis Specific Exercises (PSSEs) have been proved effective for treating scoliosis patients with mild and moderate curves and beneficial for patients in bracing with moderate to severe curves and in post-surgery rehabilitation. However, due to the limitations in PSSE design, instruction, and practice at home, the effectiveness of PSSEs can be severely diminished. Therefore, Innovision proposed to develop a hardware-software integrated system, Ufit-Exercise, which provides an immersive, instructive, and interactive environment for physiotherapists (PTs) and Scoliosis patients to conduct PSSEs effectively. 
In Phase I, Innovision team has conducted a series of research activities, which include performance capture system construction, patient digital twin creation, PSSE design and modeling, performance monitoring and evaluation, software development, and test and evaluation. The outcomes of Phase I work are described as follows.
1.Two performance capture systems have been built.  One is based on RealSenseTM, and the other is based on Azure KinectTM. Both systems, supported by software developed in Phase I, can be used to capture body motion and back surface deformation and to conduct back surface deformation analysis.
2.The protocol, algorithm, and codes have been developed to create Scoliosis patient digital twin. The PDT provides unified, three dimensional, and dynamic information of body shape and muscular skeleton, which are useful to Scoliosis assessment, exercise design and training, patient education, PSSE modeling and monitoring, and treatment progress tracking.
3.3D games have been created to replicate PSSEs from 2D videos to provide references for PSSE design and training. The full body muscle model has been created with detailed description of intrinsic and extrinsic muscle groups relevant to Scoliosis physiotherapy, which provides an effective tool for a PT to explain the cause-effect of muscle activation.
4.A streamlined process has been established to use the motion data captured from a patient to animate the PDT, which in turn is used as the reference for the patient?s home exercises. A two-step procedure has been developed to monitor and evaluate patient?s motion and back surface deformation, so as to help a patient to improve exercise compliance.
5.An initial prototype of Ufit-Exercise software system has been constructed as a SaaS, with the frontal end for users? services and back end for core technologies integration and tasks handling. It provides basic capabilities and demonstrates the workflow and dataflow.
6.The initial prototype of Ufit-Exercise has been evaluated by an SME. The solution provided by Ufit-Exercise is (a) feasible, as the core technologies used to construct it have been developed, tested, and proved; (b) suitable, as it assists PTs in multiple ways in their workflow and can be easily used by patients at home; and (c) beneficial, as it helps a PT to better teach, communicate, train, and document 3D models and exercise concepts to patients and plays a role as a virtual PT for a patient to improve home exercise quality and compliance.    
SBIR Phase I: Composing Digital-Twins from Disparate Data Sources
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project relates to the creation of a "digital twin," a real-world system projected into spatially-computed environment such as virtual and augmented reality. This technology creates the infrastructure necessary for the application of virtual and augmented reality in industrial workplaces, at scale. By making the full-scale roll out of these technologies possible, the technology seeks to impact human health and safety, operational efficiencies, and environmental risk reduction for process operations facilities, such as oil refineries and chemical plants. The long term impacts of the technology may also enable automation and optimization, improving their efficiency, security, and safety. Such facilities are critical infrastructure and play a significant role in the national economy. The availability of this product may also enhance market opportunities for other businesses in the scanning, spatial computing, and training markets. The impact may be further broadened by adapting the process for digital twin production to new domains unrelated to the industrial market.

This Small Business Innovation Research (SBIR) Phase I project is advancing knowledge and understanding in both machine learning and spatial computing. This project focuses on a method for digitizing a complex, real-world system, in an efficient manner, sufficient to recreate the captured reality as an interactive digital twin. The primary technical hurdle is the combining of different data sources that describe aspects of a particular real-world system into a single complete description. The initial physical systems being modelled are industrial process operations, but the core methods could apply to other types of systems, including natural systems, such as a rainforest. For industrial process operations, the goal is to encode the entire process operations facilities at the component level, with sub-cm accuracy, at 10% of the current time and cost requirements. To achieve this, this project will combine physical scans with engineering documentation and relational probabilities. Once combined, the model will be used as the basis for a digital twin of the real-world system projected into spatial computed environments, such as virtual and augmented reality. These techniques replace a tedious and impractical static scan and human labor workflow with rapid scans, computer vision, and a combination of procedural and trained algorithms.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Our SBIR Phase I, Composing Digital-Twins from Disparate Data Sources, researched and developed a novel, end-to-end process for the rapid, automated creation of industrial digital-twins. Specifically, the focus was on digital-twins of chemical barges. This Phase I focus is being used as a proof-of-concept to support future R&D that will enable rapid, automated creation of digital-twins for process operations facilities, such as chemcial manufacturing plants and oil and gas refineries. There is no currently viable solution for producing a realistic, immersive, digital-twin of a complete, complex, as-built facility.
Chemical barges and, even more so, the larger process operations facilities are critical national infrastructure supporting nearly every other industry in the nation. Their safe and efficient operation is critical to the country and the communities that surround them. Emerging technologies, such as virtual-reality training, augmented-reality live-guided procedures, and system simulations, require accurate, full-scale digital-twins to transform operations in these industrial envrionments. The impacts of this transformation include reducing risk to human health and safety, reducing risk for envrionmental catastrophe, improving the nations economy, and improving the national defense.
During Phase I, we innovated in the fields of computer vision, machine learning, and graph theory. We advanced the domain of spatial-computing, and have generated the foundations for three new potential inventions to be covered under future patents. Our results enable the digital-twin modeling of a chemical barge at approximately 1/10th of the previous time and cost requirements, with significant improvements in accuracy and realism.  We anticipate continued research and development to yield similar results for larger facilities.

US-European Workshop: Grid at the Edge-towards the zero-carbon power grid with improved visibility, safety and reliability at Split, Croatia on May 23-24, 2022.

This proposal requests funding to provide travel subsidies for university professors to attend the NSF Joint US-European Workshop to define the research directions for the Grid Edge. The workshop to be held in Split, Croatia on May 23-24, 2022 will be focusing on unresolved problems, technology requirements and research directions for the time horizon of 5-10 years. This workshop intends to bring together the travel grant recipients, as well as industry and government stakeholders from the US and Europe to address the Grid Edge concepts, fundamentals and vision holistically. The participants will discuss and coordinate a research strategy and related action plan to meet industry needs for new solutions in response to the new developments affecting the grid. The grid edge is considered a crucial development for the future of the electricity grids and requires contributions from academics, engineers, policy makers, economists, social scientists, government officials, and technology leaders in delivering the next generation grid solutions for the benefits of the society.

The fundamental concepts needed for advancing the grid Edge require an interdisciplinary approach involving basic research, applied engineering, technology innovation, behavioral studies, market economics, and policy insights. The grid edge is defined in this context through multiple dimensions: a) the asset ownership, which entails the utility owned and non-utility owned assets, b) the edge stakeholder role, which encompasses the utility-centric and third party-centric view, and the view that focuses on the need to closely collaborate across the edge boundaries, and c) the structured operational layers covering physical grid components, control and protection, the technology, cyber-physical security and privacy, markets, regulatory framework, and business models. This workshop will continue the NSF past and current efforts undertaken through a recent series of timely workshops to offer additional research insight and a perspective focusing on the cyber and physical aspects of energy generation and consumption at the grid edge.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Overview: This proposal requests funding to provide travel and registration subsidies for invited participants to attend the NSF Joint US-European Workshop to define the Research Directions for the Grid at the Edge, an emerging research area exploring how the utility operated grid will interface with emerging grids owned and operated by third parties. The Joint Workshop will be covering unresolved research challenges, and research directions to achieve needed technology developments for the Next 5-10 year time horizon. The Workshop is going to be held in Split, Crioatia on March 23-24, 2022. The event will last for two days with 4-hour sessions each day. The following topics will be covered:
#1: The grid and DER monitoring, control and protection challenges to meet the net zero-carbon goals; (Day 1)
#2: Data analytics, ML/AI methodology, and digital twin modeling for future grid performance visibility; (Day 1)
#3: Control and communication architectures requirements for increased grid safety and reliability; (Day 2)
#4: The market challenges in integrating DERs into the net zero-carbon grid of the future; (Day 2)
Intellectual Merit: The fundamental concepts needed for the interfacing the prosumer (consumer with generation and energy storage capability) flexibility at the grid edge requires an interdisciplinary approach involving academics, engineers, policy makers, economists, social scientists, government officials, and technology leaders. This Workshop intends to bring them together to address the Grid at the Edge concepts, fundamentals and vision holistically.
Problem to be addressed: Interfacing of the distributed energy resources (DER) to the legacy grid to achieve a net zero-carbon grid of the future requires innovative fundamental research that spans across multiple disciplines: control, communications, cyberphysical resilience, policy, economics, behavioral and social sciences, and environmental impacts. This Workshop is aimed at defining a holistic view considered by many in the industry and research community as largely undefined.
Focus:  The Grid at the Edge is defined in this context through multiple dimensions: a) The asset ownership, which splits into the utility owned and non-utility owned assets, b) The edge stakeholder role, which encompasses the utility-centric and third party-centric view, and the view that focuses on the need to closely collaborate across the edge boundaries, and c) the structured operational layers covering physical grid components,  control and protection, the technology, cyber-physical security and privacy, markets, regulatory framework, and business models.                                          
Objective: The planning and development of the future grid requires additional research resulting in fundamental concepts that can be utilized within the next 5-10 years. Many organizations such as Academia, NSF, DOE, EPRI, ARPA-E, and NIST in the US and European Union, entso-e and many university consortia in Europe have focused on different aspects of the research ranging from a short-term, two to three-year horizon, to a long term, strategic, 20-30 year horizon. This joint Workshop aims at exchanging research directions for the research efforts in the time horizon of 5-10 years where tangible and innovative commercial solutions could be deployed.  This proposed workshop will focus on a research strategy and related action plan to meet industry and societal needs for new solutions in response to the new developments affecting the grid.
Goal. The world of energy supply and consumption has changed significantly since the 2009 NSF Workshop  titled Research Recommendations for Future Energy Cyber-Physical Systems.  As identified in the last NSF Workshop focused on the Grid at the Edge held in 2021, the renewable sources are more prevalent, and energy obtained from natural gas and other fossil fuel resources is being carefully managed and downsized to make the U.S. economy more energy independent and reach the net zero-carbon grid of the future. The proposed workshop will continue the NSF past and current efforts undertaken through a recent series of timely workshops to offer additional research insight by exchanging the research vision with the European research community. Such joint discussion will explore aligned societal goals how to address the social equity and meet diverse workforce expectations.   
Broader Impacts: The grid edge is considered a crucial development for the future of the electricity grids. Defining the fundamental concepts will engage and affect academics, engineers, policy makers, economists, social scientists, government officials, and technology leaders in delivering solutions for the next generation electricity grid for the benefits of the society. 

SBIR Phase I: A Digital Platform to Assess Water Quality at Urban-Watershed Interfaces
The broader impact of this SBIR Phase I project is to help manage water quality at the boundaries of cities and watersheds. The proposed work develops a financial system to help cities and public utilities build infrastructure projects. This system integrates physical data with artificial intelligence and advanced monitoring systems. It will serve as a scalable analytics platform using environmental, economic, and social data for financing projects in water quality management.

The proposed project develops analytics and a financial instrument for environmental adaptation and water restoration projects. It uses open-source data management, sensors and interfaces, and mathematical models in a system with state-of-the-art artificial intelligence-enabled digital twin technology. The environmental data will be used in an integrated watershed model using principles of uncertainty analysis and neural network-based learning. The econometric model combines uncertainty analysis with reinforcement learning where accuracy in prognostics is incentivized.

CAREER: Going Beyond Linear Models for Attack Detection and Defense in Control Systems

This Faculty Early Career Development (CAREER) project will contribute new knowledge related to the security of automation and control systems. Modern control systems enable automation across industries, from manufacturing to chemical processing, and regulate and monitor critical infrastructure, such as power, water, fuel, and transportation. However, these systems are susceptible to cyber-based attacks that target the computers that operate them. Cyber-based attacks could damage physical infrastructure, compromise safety, and harm the environment. Attack mitigation techniques rely on detecting discrepancies between the reported behavior of a physical process and the behavior predicted based on a model, or digital twin, of the process. However, existing models are often insufficient to capture the complex behavior of real-world applications. This award supports fundamental research for the development of detectors that aptly recognize, deter, and mitigate attacks. It will thus benefit the U.S. economy and society through increased resilience to cyber-attacks. Further, to enable a diversified and integrated workforce capable of meeting the security needs of the nation, this award will develop connections between university and community college students. It will support outreach activities and live demonstrations at local schools and community events to positively impact engineering education and to broaden participation of underrepresented groups in technical fields of study.

The sensitivity of model-based detectors is intimately connected with the quality of the model used. To date, work has focused on linear dynamical models, which do not capture the complex behavior of real-world applications. To fill this gap, this award will develop a systematic framework for the security analysis of systems with uncertainty and of switched dynamical models. This research will mathematically characterize the relationship between different sources of uncertainty and detector sensitivity. It will parameterize the impact of a potential attack in terms of the system and detector uncertainty, establishing a vulnerability index based on system and uncertainty characteristics. The research team will act as both attacker and defender to develop new attack vectors that leverage the uncertainty present in systems that switch between different modes of operation.

I-Corps: Artificial Intelligence (AI)-Enabled and Digital Twin Interactive Robots for Facility Hygiene and Human Health
The broader impact/commercial potential of this I-Corps project is the development of a mobile disinfection device. The COVID-19 pandemic, seasonal epidemics of flu, and other infections placed heavy burdens on society. This proposed technology may improve current cleaning and disinfection practices with an intelligent robotic system. The proposed solution is designed to improve cleaning and disinfection efficacy, enhance personal hygiene and public health, and relieve the high workload and exposure risks facing cleaning and disinfection workers. These benefits may reduce the facility operational costs as well as opportunity costs from infection within facilities, and generate positive societal and economic impacts. The proposed service business model may facilitate the adoption of disinfection and service robots as a flexible choice, creating new opportunities and serving broader areas in the post-pandemic age.

This I-Corps project is based on the development of an artificial intelligence (AI)-enabled and digital-twin interactive robot that adapts to ambient environments, social processes, and building dynamics for automated, continuous, precise, and socially-friendly disinfection. The key innovations include using a digital-twin based simulation and deep learning environment to train the robots to predict and perceive contaminated areas for precision disinfection at both the building and room levels. This research improves surface disinfection efficiency and effectiveness. Pathogen infection risks derived from data-driven modeling and simulation may provide a new stream of information for robot navigation to prioritize disinfection routes. In addition to the precision disinfection that specifically targets contaminated areas, the robots will learn to understand the social contexts (e.g., human activity and privacy within a room) to operate in the presence of humans without interrupting normal activities and exposing harmful substances such as ultraviolet lights to humans, enabling safe and socially-friendly disinfection. Augmented by high-fidelity virtual simulations, imitation learning will be developed to train the robots to learn the social and physical constraints and adapt their disinfection maneuvers with respect to the context and object geometry enabling complete, efficient, and safe disinfection. The technology may enable a single smart solution to maintain environmental hygiene and human health.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The resurgence of COVID-19 cases, flu epidemics, and emerging diseases, alongside heightened awareness of environmental hygiene and public health, has emphasized the urgent need for automated, intelligent, and precise disinfection robots. These robots are particularly crucial for areas with large gatherings, such as hotels, healthcare facilities, commercial buildings, airports, and schools. This I-Corps project aimed to assess the commercialization potential of an artificial intelligence (AI)-enabled disinfection robot. Developed by the team, this robot is designed to integrate with building digital twins for optimal planning, adapt to various environments, understand social contexts for safe human interactions, and utilize multimodal and optimized maneuvers to achieve maximum disinfection efficiency. Through this project, extensive customer discovery was conducted to understand the market needs for such a service robot, entrepreneurship and leadership skills were developed among the project team, and the commercialization potential of the technology was evaluated. The customer discovery process revealed a significant demand for automated disinfection robots that are cost-effective, addressing workforce shortages and improving operational efficiency in commercial buildings and healthcare facilities. Findings were shared with a broad range of stakeholders, including end-users, potential customers, investors, and industry collaborators. The feedback highlighted the relevance and applicability of the proposed AI-based disinfection robotic technologies, promising to significantly impact environmental hygiene and public health.
 
The COVID-19 pandemic, seasonal flu epidemics, and routine infections place heavy burdens on societies and economies. The proposed product and service introduce a new paradigm for precise and efficient disinfection practices, aiming to improve environmental hygiene, enhance public health, and reduce significant costs, thereby contributing to economic growth. Implementing intelligent robotic disinfection techniques will greatly improve cleaning and disinfection efficacy, reducing the reliance on labor that is both costly and at high risk of infection and exposure to harmful substances. Additionally, the anticipated improvement in environmental quality is expected to increase human work productivity, yielding substantial economic benefits. Furthermore, the entrepreneurial lead and a Ph.D. student played pivotal roles, underscoring the project's commitment to training and professional development. These activities equipped the team with vital skills in customer discovery, market analysis, and the commercialization of technological innovations, accelerating the process of technology transfer and commercialization. The project provided a crucial platform for developing leadership and entrepreneurial skills, preparing participants to transform innovations into marketable commercial solutions. By emphasizing key entrepreneurship concepts, effective communication, and teamwork, the project successfully positioned team members for future success.
I-Corps: Context-specific scientific simulation models to mitigate wildfire risks
The broader impact/commercial potential of this I-Corps project is the potential development of dynamic and community-oriented wildfire preparedness solutions for the Wildland-Urban Interface (WUI) areas, which constitute a third of US households. Current wildfire evacuation plans rely heavily on static zoning and often lack a regional or system-level perspective. These plans fall short when it comes to understanding and addressing practical challenges, such as the unpredictability of the wildfire progression, the general lack of awareness/collaboration by the residents, and the difficulty in communication and resource mobilization in emergency situations. By incorporating the dynamic elements as well as community efforts in the planning process, the proposed solution (software and service) aims at supporting government agencies at various jurisdiction levels (e.g., fire department, planning department, and offices of emergency services) in potentially safeguarding the WUI residents? safety and properties, as well as possibly creating positive community images. Private sector companies such as insurance providers or large resorts also may benefit from the proposed solution to develop better services/products for their customers and increase company returns. Once validated in the wildfire sector, the proposed solution may be used to mitigate the losses of other natural disasters, such as hurricanes and earthquakes.

This I-Corps project is based on the development of two core innovations: a suite of scientific simulation models that capture the holistic process of wildfire evacuations, as well as an advanced visualization platform that stimulates awareness-raising and bottom-up collaborations through hands-on experiences. On the modeling side, the integrated simulation framework captures the dynamics in wildfire evacuation through the inclusion of the wildfire progression, the organizational communication flow, and traffic evacuation sub-modules. The models are developed to be efficient and highly scalable, which helps to identify the evacuation bottlenecks beyond the boundary limits of a small jurisdiction or a single infrastructure type. On the visualization side, the results from the simulation module are converted to 3D animations, including the fire flames, traffic congestion, and communication disruptions. The simulation-visualization procedure enables endless scenarios to be generated and presented to the users, effectively facilitating the enhancements of the residents? situational awareness and ensuring smoother two-way communications between the local community members and the wildfire management experts. The models and visualizations have been prototyped at various case study locations, showing the potential of possible improved preparedness through scientific and social innovations.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
"How do I get out?" and "Where do I go?" These two questions are repeatedly asked by Wildland Urban Interface (WUI) residents in WUI-Go!'s I-Corps interviews. Due to the proliferation of technology in daily lives, residents are accustomed to applications or services that provide real-time information and personalized guidance (for example, navigation apps), which emergency response agencies are unable to provide in wildfire scenarios given time and workforce constraints. WUI-Go!'s broader impact seeks to bridge this gap by providing real-time and customized information to help residents evacuate safely and quickly. By steering residents to safe areas faster and further from fire hazards, WUI-Go! aims to reduce the anxiety and trauma in evacuation while allowing government agencies to focus on critical activities such as firefighting, traffic management, search and rescue for the most vulnerable population, etc. 
The major outcome of the I-Corps was the creation of WUI-GO, LLC, a company that is competing for technology innovation and local grants to demonstrate the feasibility of using dynamic socio-technical Digital Twin (DT) simulations to expedite WUI evacuation. Current wildfire preparedness relies heavily on static planning tools. Some available dynamic tools are still not fast enough to be useful in real time scenarios. WUI-Go!'s intellectual merit is to combine DT evacuation simulation, pre-simulation database, and local knowledge to overcome the speed and accuracy barriers. This will be a paradigm shift for evacuation preparedness for wildfires, transferable to other climate-related hazards that present high spatial behavior uncertainty (i.e., flash floods, landslides, debris-flow, and tornadoes). The design involves a deep empirical exploration of the DT concept. Case studies based on real community inputs are planned in this one-year project to verify the efficiency of the proposed tool.
At the start of the award the wildfire evacuation technology for planning purposes was broadly defined, with little knowledge of the customers profiles and what their major needs are. The customer discovery experience through I-Corps allowed for a better understanding of the pains and needs of our customer and a definition of the value proposition together with an approximation of a minimum viable product. The team decided to move the technology forward to market by establishing our small business and apply for technology innovation grants while also looking for local funders.

I-Corps: Cyber Physical System for Telehealth for Seniors with Cognitive Impairments
The broader impact/commercial potential of this I-Corps project is the development of a technology to better reintroduce the human touch in telehealth and improve the general quality of care. Most telehealth companies have focused on providing convenient logistics. Less effort has been devoted to consultation accuracy. This proposed technology uses Internet of Things (IoT) sensing to enable a more precise medical diagnosis, with measurement of temperature, blood pressure, and specific inputs depending on the specialty. Diagnosis and monitoring of neuromuscular disorders rely on a nuanced physical examination. Most neurologists are unfamiliar with telemedicine's potential use for stroke and movement disorders. This proposed technology may provide improved assessments of variable weaknesses, increase patient monitoring and education, reduce the burden and cost of clinic visits, and increase patient access.

This I-Corps project is based on the development of a cyber-physical and community system that address the use of telemedicine for neuromuscular disorders using computer vision, robotics, and non-invasive cognitive assessment tools. This technology includes a telehealth platform augmented by a digital twin of the patient that assists with diagnostic testing. This system lets doctors focus on taking notes, interacting with the patient, and getting more accurate diagnosis for their patients.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
 Intellectual Merit:
Using AI capabilities, we deliver Inteleclinic, an accurate and quantitative patient assessment through telehealth, which is unique to the market today. Our platform is focused on improving the patient/doctor experience by assisting the physician with diagnostic assessments. Using a simple video-call (such as Zoom), the physician can have accurate data on the patient. Data are collected without any contact, leveraging our patented solution.
Unlike many telehealth companies that focus on software and video technology, we are unique because our approach is rooted in the understanding of the diseases we target. Inteleclinic is designed to provide comprehensive, non-invasive, real-time analytical patient data that improves the management of the disease, by taking the subjectivity out of a diagnostic exam. Using off-the-shelf hardware and advanced computer vision, the platform is cost effective and leverages currently available video-conferencing tools.
Our competitive advantage is that, for pharmaceutical companies, our system is uniquely designed to advance clinical trials by:
Improving data collection by providing unbiased core exam data through AI, computer vision, voice analysis, robotics, and pervasive capture of vital signs
Increasing enrollment through increased patient access
Diversifying subject enrollment which increases the validity of the studies and leads to better scientific discoveries
Reducing the workload on staff through increased automated tasks
Providing a digital twin framework for the individual patient to advance the field of precision medicine
Broader Impact:
We provide solutions for clinical researchers who want to increase diversity in their enrollment and have quantitative diagnostic capability in their telehealth platforms.
Our beachhead market is to provide standard primary and new secondary end-point of clinical trial in neurological disease to our customer: the industry pharmacy sponsor of the clinical trial.
Our grand vision is to improve patients' quality of life with a telehealth platform that answers individual special needs and anticipates the problem before it happens, saving lives while decreasing the cost of care, fighting health disparities in the U.S.
Overall, telehealth should develop and improve medicine as medical imaging did in the last 30 years. We will do that by building telehealth portals that advance the field of telehealth and medicine. 
Outcome of the award:
Market discovery has been initiated during the NSF I-Corp:  we continue the market discovery steadily to validate the Minimum Viable Product spec. We received useful feedback from the profession on the new avatar proof-of-concept demo we have achieved.
The startup of the project, Care Constitution Delaware C-Corp, was funded by IndieBio, a program of SOSV, a global venture capital firm providing multi-stage investment to support big ideas
We have been able to refine our hardware-software solution, and our prototype is collecting new data in the Department of Neurology at The George Washington University: We are currently running two new clinical studies: one for Myasthenia Gravis; a second for Amyotrophic Lateral Sclerosis (ALS).
We have validated our digital tool with the MG-Net clinical data set (51 patients and 15 controls) and in the next few months will submit several manuscripts describing, respectively, the validation of our method, the impact of human factor on neurology examination reproducibility, the method we developed to have a fully automatic software that provides unbiased score, including an avatar proof-of-concept version.
We have recently developed an effective computing module for ALS patients to improve care, that extends our capability in telehealth.
 

CAREER: Unraveling Mechanisms of Mechanical Degeneration in Elastin

This Faculty Early Career Development (CAREER) grant will support fundamental research to understand complex changes to elastin that occur in aging and disease. Elastin is a key structural protein that gives biological tissues their elasticity. For example, skin, lungs, and arteries can stretch and relax thanks to elastin. During aging or disease, problems with elastin can cause tissue degeneration, which inhibits normal function. This is a challenging problem to study because the physiochemical stressors that can lead to degeneration are linked together. Many of these stressors occur at the nanoscale. This gap in knowledge prevents new therapies from being developed. The objective of this research program is to pioneer novel nanoscale insight into mechanical degeneration of elastic tissues. Specifically, this work will use a computer modeling framework, which will be calibrated from measured experimental results. Understanding the various sources of damage to elastin may facilitate new therapies aimed at maintaining biomechanical function of elastic tissues to prevent or delay complications in aging and disease. The research program will also engage and support undergraduate and graduate students, especially from underrepresented groups, through diverse research experiences and the Women in STEM Frontiers in Research Expo. This will lead to a new interdisciplinary curriculum and promote a new local network of computational biophysicists.

This research program will establish a new high-fidelity modeling framework for healthy and degenerated elastin as a tool to resolve the impacts of pathological physicochemical stressors on mechanics at the nanoscale and identify specific drivers to the loss of mechanical function of elastic tissues. The PI?s recent development of the first all-atom model of the elastin precursor tropoelastin lays a foundation for systematically probing deleterious stimuli independently and in combination. This research will establish a multiscale digital twin of healthy and degenerated elastin to elucidate how key physicochemical stressors, ? specifically glycation and non-enzymatic crosslinking, ectopic calcification, enzymatic proteolysis, oxidative damage, racemization, lipid peroxidation, and carbamylation ? contribute to structural change, impact mechanical function independently and cooperatively, and disrupt tightly coupled hydration water dynamics with unstructured elastin. This work will provide: 1) validated computational tools to characterize the multiscale structure and mechanical response of elastin, with applications to other heterogeneous, hierarchical disordered molecular systems; 2) fundamental insight into the role of hydration water in such systems; 3) mechanistic understanding of likely specific paths to loss of function in elastin during aging and disease; and 4) an educational program to engage and retain diverse students.

Workshop: Mid-scale RI-EW Concepts for a Tornado-Downburst-Gust Testing Facility to Study Wind/Debris Impact on Civil Infrastructure; Chicago, Illinois; 20-21 October 2022

Non-synoptic windstorms, tornadoes, downbursts, and gust fronts annually cause billions of dollars of property damage and inflict numerous fatalities and injuries in the United States. Major tornadoes, with an intensity of Enhanced Fujita (EF) 3 or greater (136 mph or greater, three-second gust), have struck large population centers in the past causing multiple fatalities and significant damage to structures. Property damage from downbursts and gust fronts are equally devastating. Damage to civil infrastructure will only increase in the future with growing urbanization and increased intensification/frequency of such windstorms because of climate change. To improve the resilience of civil infrastructure in these extreme wind events and reduce their economic and social impacts on society, an advanced research and testing facility is needed in the United States that can simulate realistic wind fields and a weather-like environment with wind speeds up to 250 mph. This award will conduct an engineering workshop that will identify the concepts for a university-based national testing facility that will be capable of generating wind fields associated with EF1 to EF5 tornadoes (86-250 mph), intense downbursts (150-175 mph), and gust fronts with wind speeds up to 100 mph. Such a facility, with capabilities that exceed those of existing facilities, would enable investigation of the impacts of non-synoptic windstorms on civil infrastructure at mid-to-large length scales (1/10-1/2) for studying the near-ground wind field, and wind loading, debris impact, and damage mechanisms for a range of structures with small to large footprints and heights.

The two-day workshop, organized by Iowa State University, will be held in Chicago, Illinois, on October 20-21, 2022. It will bring together 25-30 participants from universities, government agencies, and industry to discuss the scientific needs and requirements and conceptual design of the mid-scale research infrastructure facility. The workshop will facilitate (a) advancement of knowledge of the transient characteristics of the non-synoptic winds and the hazards they pose that cause structural damage, (b) understanding of the mechanics of the wind-structure and debris-structure interactions in non-synoptic winds, (c) need for new experimental data and its integration with computational mechanics to improve structural modeling and damage prediction, (d) discussion of facility components and instrumentation, and (e) development of a Project Execution Plan (PEP) for the design of a new facility. The workshop outcomes will be synthesized in a report that will be disseminated to the engineering research community via the Natural Hazards Engineering Research Infrastructure (NHERI) Data Depot (https://www.DesignSafe-ci.org).
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
A two-day planning workshop, MsRI-EW Concepts for a Tornado-Downburst-Gust Testing Facility to Study Wind/Debris Impact on Civil Infrastructure, was held in Chicago, Illinois, on October 20-21, 2022. This report summarizes the details and outcomes of the workshop.
The workshop, sponsored by NSF and organized by Iowa State University, brought together 30 participants from universities, government agencies, and industry to discuss the scientific needs and requirements and conceptual design of a mid-scale research infrastructure (MsRI) facility in the United States. The proposed university-based national testing facility was envisioned to have capabilities that would exceed those of the existing facilities by generating realistic wind fields (speeds up to 225 mph) in non-synoptic windstorm (NSW) events (tornado, downburst, gust-front) to enable physical testing of their loading and damaging effects on civil infrastructure at mid-to-full model scales.
The specific objectives of this workshop were to facilitate the (a) advancement of the current knowledge of the transient characteristics of NSW hazards from the extreme wind and wind-borne debris, (b) advancement of the understanding of the mechanics of the wind- and debris- structure interactions in NSW events, such as load- distribution and paths, and component stresses in structures, (c) identification of the need for new experimental data and its integration with computational mechanics to improve structural modeling and damage predictions, (d) discussion of facility components and instrumentation, and (e) initial Project Execution Plan (PEP) for the design of a new facility.
The workshop was held in a hybrid mode with 26 in-person and 4 remote participants with areas of expertise in wind engineering, structural engineering, atmospheric sciences, building codes, and large research infrastructure management. They were from 14 academic institutes, 2 national agencies, and 2 industries from geographically diverse regions of the US and included 7 women and 2 young researchers. It included a keynote talk to set the tone of the workshop, a talk by the NSF program director highlighting the MsRI program, 6 panel discussions led by experts on the topic, 19 technical presentations spread over 7 Sessions, and 4 group discussions enabled by breakout sessions, all targeting to achieve the goals of the workshop. The topics of the technical sessions were: 1) Spatio-temporal characteristics of near-ground wind fields and hazards they pose on infrastructure in NSW; 2) Current modeling methods/tools for assessing wind fields, wind loads and damage to infrastructure in NSW – Part I and Part II; 3) Mechanics of wind-structure and wind-borne debris impacts of NSW and mitigation strategies; 4) Identify MsRI Facility design criteria, components, equipment and instrumentation, 5) Integrating research, social science, education and outreach to achieve NSW resilient and sustainable communities, and 6) Need, concept and design criteria for the envisioned MsRI Facility and discussion on the implementation of its plans/construction.
The workshop participants collectively agreed that the proposed MsRI facility is critically needed to fill key scientific gaps that remain in addressing NSW hazards due to the limited capabilities of the current facilities. Physical models that can be tested in the existing facilities are limited to the size of a single-residential building at small model scales because of the relatively small size of the tornado or microburst core in the current simulators to avoid blockage effects, and thus, structures with large footprints or group of structures cannot be tested accurately. The proposed facility will be designed to better meet the requirements found lacking in the current NSW facilities.
Another workshop outcome was to establish the required characteristics/capabilities in the proposed MsRI facility, where the physical model must be able to simulate realistic Reynolds numbers and swirl ratios of the wind flow, resolve spatial and temporal scales, and incorporate effects of windstorm’s translation. It should generate wind fields associated with EF1 to EF5 tornadoes (86-225 mph), moderately intense downbursts (100-125 mph), and gust fronts (80-100 mph) to enable investigation of the impacts of NSW events on civil infrastructure at mid-to-large length scales (1/10-1/1) for studying the near-ground wind field, and wind loading, debris impact, and damage mechanisms for a range of structures with small to large footprints and heights. It was envisioned that a digital twin of the physical facility must be formulated to assist in the design of the new facility. Parameters that are important for structural loading in NSW should be identified and replicated in the proposed facility. The facility should be designed for a wide range of buildings and other structures.
This MsRI facility would be useful to serve broader impacts by transferring knowledge to the research community, public, engineering professionals, and the construction industry by improving the current understanding of wind loading in NSW events and enable practitioners to improve design guidelines.
Details of the workshop and its outcomes are synthesized in the proceedings of the workshop, available to the research community at the Natural Hazards Engineering Research Infrastructure (NHERI) Data Depot:
https://www.designsafe-ci.org/data/browser/public/designsafe.storage.published/PRJ-4399

Fast-Track Consensus Study on Foundational Research Gaps and Future Directions for Digital Twins

The National Academies of Sciences, Engineering, and Medicine is undertaking a study to identify needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. A digital twin is a computer model that changes over time to represent the structure or behavior of a unique physical entity, such as a manufacturing process, piece of equipment, or even a person. Based on data inputs, the digital twin can be used to gain insight into present and future states of the physical twin. The exploration and use of digital twins is growing across domains, but many state-of-the-art digital twins are largely the result of custom implementations that require considerable deployment resources and a high level of expertise. Due to the individualized nature of many digital twin implementations, the relative maturity of digital twins varies significantly across problem spaces. Moving from one-off digital twins to digital twin implementations at scale will involve addressing foundational mathematical, statistical, and computational gaps. This study aims to highlight these critical research gaps and provide options to address them with the goal of advancing the use of digital twins across disciplinary communities.


The proposed study by the National Academies of Sciences, Engineering, and Medicine, will highlight needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. A digital twin assimilates observational data and uses this information to continually update its internal models so that they reflect the evolving physical system. The digital twin is therefore continuously improving and provides a dynamic digital history of the physical entity. These core functionalities can be augmented with feedback control and artificial intelligence, combined with ensembles of similar twins, or used in tandem with other predictive tools to analyze and diagnose operational states and to optimize performance under real-world conditions. Utilization of digital twins varies across disciplines. This study will address the following: (1) diverging definitions of digital twins and domain-inspired use cases; (2) foundational mathematical, statistical, and computational gaps for the continued development of digital twins; (3) best practices for digital twin development and use; and (4) opportunities to move the community and state of practice forward. Three domain-specific workshops will be held to explore the methods, practices, use cases, and challenges for the development and use of digital twins?focus areas include biomedical domains, Earth and environmental systems, and aerospace engineering. Four reports will be released by the National Academies during the course of this 18-month study: three short summaries of the domain-specific workshops and a consensus report focused on the cross-cutting foundational research gaps and future directions for digital twins.

Collaborative Research: NeTS: JUNO3: Towards an Internet of Federated Digital Twins (IoFDT) for Society 5.0: Fundamentals and Experimentation

The concept of a digital twin (DT) that enables the creation of a programmable, digital representation of physical systems (e.g., smart vehicles) will revolutionize future industries. DTs lie at the heart of the vision of a future smart society, dubbed Society 5.0 by Japan, in which a high integration between cyber and physical spaces is exploited to bring forth economic and societal advancement across industries ranging from intelligent transportation to robotics. To realize this vision of a new DT-driven Society 5.0, this project envisions a novel concept of an Internet of Federated Digital Twins (IoFDT) that holistically integrates heterogeneous and physically separated DTs representing different Society 5.0 services within a single system. The goal of the research is therefore to create the scientific foundations of the IoFDT through a close collaboration between a team of US and Japanese researchers with complementary expertise in wireless networking, DTs, and artificial intelligence (AI). These foundations include a new, fundamental framework for allowing effective design, analysis, and optimization of an IoFDT system, in presence of heterogeneous applications. The proposed research will have a tangible societal impact since it contributes towards enabling diverse Society 5.0 services (from transportation to factory automation) thus potentially improving the quality of life. The research is coupled with collaborative US-Japan DT-centric education, outreach, and dissemination plans that will help broaden the impact of the research.

To design the proposed framework, this joint US-Japan project will merge ideas from wireless communications, machine learning, and programmable networking to contribute several innovations: 1) A cross-layer wireless and computing framework that introduces new methods for dynamic network slicing for the IoFDT system, 2) Efficient methods for DT-centric proactive resource optimization as well as collaborative computing and learning techniques that can deal with unprecedented dynamics of the cyber (wireless) and physical systems of an IoFDT, 3) A holistic AI framework, based on continual learning, that can faithfully build programmable DTs from a continuous stream of data while exploiting complex relationships and patterns across the physical systems and their coordinated twins, and 4) Realistic implementation in a programmable IoFDT platform that provides a meaningful proof-of-concept of an IoFDT system deployment in Japan.

MRI:Acquisition of a Network Emulator for Cyber Security Research of Electric Power Grids

This NSF MRI project aims to seek funding for purchasing network communication and cyber-attack co-simulation tool to increase the power grid?s cybersecurity research capacity at the University of New Mexico (UNM) and its collaborators. The project will bring transformative change to secure the national defense by designing and creating cyber-attack detection and mitigation algorithms and techniques for power grids. This will be achieved by utilizing the requested co-simulation tool which facilitates high-fidelity and scalable modeling of cyber systems. The intellectual merits of the project include performing cybersecurity studies on the distributed control architectures, protection systems, advanced metering infrastructure, and phasor measurement units. Moreover, the requested co-simulation tool will be utilized to study the societal impacts of power grid cyber-attacks. The broader impacts of the project include enhanced cybersecurity research capability and competitiveness at UNM and its collaborators, enhancement of existing and development of new courses, research involvement of underrepresented students, and K-12 outreach.

Modern power systems are highly exposed to cyber threats due to the deployment of communication and control technologies for different applications like microgrid distributed control, energy management systems, as well as wide-area protection, control, and monitoring. To perform research on the cybersecurity of a large-scale cyber-physical system (CPS) like a power grid, it is impossible to apply, study, and analyze cyber-attacks on a real system. To effectively study and analyze cyber threats in power grids, a real-time hardware-in-the-loop co-simulation platform that can simulate both the physical (i.e., power system) and cyber (i.e., communication network) layers is of paramount value. The requested co-simulation tool will provide us with a scalable solution to emulate the cyber layer of power grids in a digital twin platform and conduct a variety of different cybersecurity studies. The co-simulation tool will set the foundation for extending collaborative research activities with industry, national laboratories, and other universities in the area of power grid cybersecurity. This tool will be utilized by multiple faculties and their students and postdoctoral fellows across different institutions for education and research activities.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
This project resulted in purchasing Exata-CPS software which is a network communication and cyber-attack co-simulation tool that can accommodate the co-simulation of cyber studies in power grids. This tool was successfully integrated into the already existing Opal-RT real-time digital simulator at the University of New Mexico (UNM). This new addition to the Power System Emerging Technologies (PSET) laboratory at UNM will significantly increase the research competitiveness of UNM in smart grid and cyber security research areas. With the requested co-simulation tool, researchers from different universities with expertise in engineering and economics disciplines will be able to perform cyber security studies on technical and societal aspects of power grids. This tool has been already utilized to perform cyber security research in microgrid control systems and is currently being utilized for cyber security studies of adaptive protection systems.   
Intellectual Merit:
Due to the deployment of communication and control technologies, modern power systems are highly exposed to cyber threats. With the acquired tool, the project team has performed research on the cyber security of large-scale power grids in a real-time hardware-in-the-loop (HIL) co-simulation platform that can simulate both the physical (i.e., power system) and cyber (i.e., communication network) layers. The requested co-simulation tool provides a scalable solution to emulate the cyber layer of power grids in a digital twin platform and conduct a variety of different cyber security studies on distributed control architectures, protection systems, advanced measuring infrastructure, and phasor measurement units. Moreover, the requested co-simulation tool can be utilized to study the societal impacts of power grid cyber-attacks.
Broader Impacts:
The purchased tool significantly helps with the cyber security research in electric power grids which promises a more resilient and secure grid of the future to ensure a reliable supply of power to customers. The purchased tool not only is beneficial for the cyber security studies of electric power grids but can also be used for research in other fields of engineering like computer science, control, aerospace, etc. The acquired co-simulation tool has set the foundation for extending collaborative research activities with industry, national laboratories, and other universities in the area of power grid cyber security. The purchased tool has helped with workforce development and training of students to acquire skills required for employment in the U.S. power grid and cyber security sectors.
 
From Limited Data to the Deformation Field in Metals: A Machine Learning Driven Approach

Having warning before a catastrophic failure of a material occurs can save lives and reduce costs. Prior to failure a material may undergo internal changes which generate high-frequency stress waves, referred to as acoustic emissions. Measurements of acoustic emissions in metals provide a unique approach for quantifying defects and their movements. However, interpreting acoustic emissions is a longstanding challenge. This research will address this challenge by identifying and decoding the distinct acoustic emission signatures of each deformation mechanism with a combination of experiments and machine learning tools. This research will result in a unique method that endows experiments with a window into the fundamental deformation mechanisms that are not currently accessible from surface measurements alone. This will enable new basic knowledge of the behavior of metals during deformation. This research is also integrated with education and outreach. Results from this work will be integrated into a new course on machine learning for solid mechanics and materials engineering. Maryland high-school students from under-served/under-represented groups will also be engaged in research internship opportunities.

We will utilize integrated physics-based modeling, machine learning, and experiments to: (1) develop a ?digital twin?? of acoustic emission experiments to forward predict the acoustic emission surface waves associated with complex slip avalanches during the deformation of single crystal Ni micropillars; (2) definitively assess/scrutinize existing phenomenological acoustic emission models in literature, and develop new physics-based theoretical models that identify the interconnections between dislocation-based plasticity and acoustic emission signals; (3) predict the true experimentally observed slip localization in the 3D volume from the surface acoustic emission measurements; (4) train deep operator networks (DeepONets) for forward predictions of acoustic emission and inverse predictions of the underlying deformation mechanisms; and (5) validate the forward and inverse predictions through coupled in situ scanning electron microscopy microcompression experiments and acoustic emission measurements on single-crystal Ni microcrystals. To close the loop between the developed models and the experiments, we will also utilize the trained DeepONets on the experimental results to gain fundamental understanding of the underlying deformation mechanisms during dislocation avalanches in micro-compression experiments, which are currently difficult to interpret based on surface measurements and load-displacement measurements alone.
NSF Convergence Accelerator Track E: Digital Reefs: A Globally Coordinated, Universally Accessible Digital Twin Network for the Coral Reef Blue Economy

Coral Reefs support almost one billion people worldwide, yet development of a thriving Coral Reef Blue Economy is threatened by unprecedented global climate change and lack of universal access to the data and tools needed to address 21st century challenges. Digital Reefs leverages Digital Twin Technology, Gaming Engine Platforms and Cloud Analytics to transform the way humans? access, interact with, and use scientific data and models to solve societies? most pressing environmental problems. Digital Reefs delivers intuitive, immersive 4-dimensional visualizations of each coral reef ecosystem and a suite of interactive user-inspired tools to empower millions of stakeholders around the world with the most effective decision-making tools in a rapidly changing ocean. Within 5 years, Digital Reefs, scaled to the globally interconnected Digital Reefs Network, will be the go-to tool for effective management, conservation, and restoration of coral reefs in the 21st century, and for communicating and sharing data, knowledge and experience. Ultimately, the Digital Reefs technology framework will provide the blueprint for expansion of Digital Twin technology oceanwide.

Occupying less than 1% of the ocean surface, coral reef ecosystems play a disproportionately large role in the global economy, with an estimated annual value of $1 trillion. In the US, 4 million acres of coral reef support hundreds of thousands of jobs and protect vast expanses of coastal property, including strategic military infrastructure. Yet the sustainable growth of the Coral Reef Blue Economy is threatened by unprecedented climate change and a lack of access to intuitive, actionable data that can help to solve problems. The project bridges this gap, leveraging 21st century Digital Twin technologies to build digital replicas of actual coral reef ecosystems, continuously updated in near-real time by sensor and satellite data transmitted from the physical reef. The Digital Reefs platform employs Visualization softwares and Gaming Engines to create intuitive, immersive, interactive 4-D visualizations of each reef, and a suite of user-inspired decision-making tools via an easily navigable user-interface. Our fully functional, interactive Digital Reefs prototype of Palmyra Atoll, delivered in two years, will include 4 Data Layers (3-D reef bathymetry, temperature, currents, and benthic communities and changes through time) and 4 User Tools that facilitate climate risk assessment, ecosystem service optimization, effective restoration designs and invasive species and plastics monitoring. A modular platform design will allow easy integration of additional data and tools, guided by user-inspired research conducted throughout the project. Application of gaming engines to create immersive, Virtual Reality experiences with scientific data will deliver living coral reef ecosystems into homes, classrooms and boardrooms alike, to transform our relationship with the ocean and the Blue Economy.
SBIR Phase I: R&D for Water Efficiency, Management and Markets Utilizing Distributed Ledger Technology

The broader impact of this Small Business Innovation Research (SBIR) project will provide a significant improvement in economic incentives for water rights holders to conserve and efficiently deploy their water. One of the greatest struggles facing the Western States is the disincentive for water rights holders to efficiently manage their water. If holders don?t use the entirety of their water each year, they risk losing the unused portion back to the State. Commercially, despite a billion-dollar demand for seasonal leasing of water?which would satisfy annual use requirements?water holders have no place and little ability to market and lease their unused water. This approach using the proposed distributed ledger technology (DLT), allows holders to track and log their water rights information on its platform. In turn, water users will finally have access to information that traditionally informs other property transactions?title history, water usage, comparable transactions, and localized pricing. With that information, our customers populate a marketplace where short-term and long-term transactions can be quickly and easily transacted. In sum, by creating the tools and interfaces to digitize water rights and enable simple-to-execute water transactions, DLT will power water markets and drive efficient water use throughout the West.

This SBIR Phase I project proposes to research and develop a unique combination of distributed ledger (DL) tokenization and a decentralized distributed file system (DDFS) to produce a robust digital twin of both physical water rights and usage, as well as the associated legal materials and data. DL tokens represent the legal quantity of water that has been granted to the user by the government. DDFS allows the storage of key information pertaining to the water rights?including title and use history?in a public, immutable manner, anchored in the DL transactions. This approach provides a level of trust, transparency and provenance for water that is currently absent in the United States. The current state of the market?makes it impossible for the average water owner and manager to have any insight into the value of their water assets. Water-specific marketplaces backed by DL and DDFS allow both buyers and sellers the ability to view, vet, and transact water assets efficiently and with minimal cost. This first phase of research and development will establish the requisite infrastructure to populate those marketplaces.
This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
In Phase I, WETx successfully validated the proof of concept of their solution by developing a minimum viable product (MVP) and conducted extensive testing of three distinct water transactions utilizing a Distributed Ledger Technology (DLT) solution and Digital Document Filing System (DDFS). This initial phase provided invaluable insights as WETx engaged with clients and gathered crucial information to further enhance the development of the solution.
The technical accomplishments of Phase I demonstrate the feasibility of the WETx technology to:
1.Create a DLT mechanism to store and display key information about water rights and water shares utilizing DDFS technology that also provides an interactive visual representation of that data without needing any server or database technology;
2.Create DLT-based tokens that encompass the DDFS data representing several water rights including the total allocation of water described by water rights and water shares;
3.Create DLT-based tokens that encompass the DDFS data representing a series of water shares including the total number of defined shares and the water rights those shares represent; and
4.Demonstrate ownership of water shares and rights by distributing the water tokens to user accounts residing within the DLT, including DDFS data detailing each transaction.
Building upon the achievements of Phase I, Phase II aims to enhance and accelerate the evolution of our digital water bank solution.
Broader impacts: Through its transparent and accountable platform for water rights trading, WETx enables efficient water allocation and management, promotes fair competition, encourages market participation (buy/sell, lease, trade and finance of water), and unlocks opportunities for businesses to optimize their efficient water usage. This utilization of water contributes to improved public health and welfare by providing access to clean water and promoting sustainable water practices and enhances the overall competitiveness of water-dependent industries such as agriculture, energy, and manufacturing. If deployed broadly in the USA we estimate WETx could save billions gallons of water per day.

CIVIC-PG Track B Digital Twin-based Framework for Development of Schools as Smart & Connected Community Resilience Hubs

As the most recent United Nations Intergovernmental Panel on Climate Change (IPCC) report describes, climate change and the growing frequency and severity of disasters could soon outpace humanity's capacity to adapt. The severity of a disaster can be amplified by inadequate infrastructure investment, social marginalization, and a reduced capacity to adapt when a vulnerable community is impacted. Strengthening vulnerable communities? resilience and capacity to adapt is, therefore, a key challenge for emergency managers and community leaders. In this NSF CIVIC-PG project, we develop a community-led framework for adapting a public school to become a pilot Community Resilience Hub (CRH) for a community in the city of Portland, OR. CRHs are trusted, community-led facilities to support citizens before, during, or after a disaster event. This research and demonstration project will improve community self-sufficiency reaching more community members in need before, during and after a disaster event (e.g., wildfire, earthquake, flood) while also establishing the value of developing a network of CRHs in conjunction with community and city leaders and first responders. The research will elucidate strategies and logic for development of a network of CRHs that can be replicated in other communities across the United States. The findings of this project will be disseminated through collaboration with a NIST-sponsored Global City Teams Challenge SuperCluster and will be instrumental to the future development of CRHs.

This NSF CIVIC-PG project will take critical steps toward developing a transformative vision of how local CRHs can connect to form a Community Resilience Network (CRN) leveraging the CRH sociotechnical infrastructure framework and serving communities across a larger geographic area during a disaster event. We lack an in-depth understanding of the dynamic characteristics of disruptions and the emergency response needs that evolve during a multi-hazard disaster event. The project?s primary goal is to investigate how ?digital twin? technology can facilitate establishment of community-led CRH sociotechnical infrastructure that integrates evolving community needs, resources, and conditions to facilitate emergency response, timely information-sharing, resilient connectivity, and resource distribution across a network of CRHs in response to disaster events. This will lay the foundation for establishment of a CRN that leverages updated city management and community-led strategies to expand CRH resource resilience capacity, distributed disaster response (across vulnerable communities and CRHs), and data-driven decision-making on disaster response.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
In response to the lack of whole community participation, preparedness, and resilience to disasters, and in partnership with the National Institute of Standards and Technology (NIST)'s Global Community Technology Challenge (GCTC) SuperClusters (Smart Buildings and Mobility), this civic-academic project team collaborated to assess, understand, conceptualize, and plan the implementation of a community-led sociotechnical framework for Community Resilience Hubs (CRHs) within a Pilot School setting in Portland, OR. Leveraging digital twin technology, this effort aimed to strengthen community resilience against disasters such as wildfires, earthquakes, and floods by enhancing communication and information sharing, and guiding collaborative disaster decision-making and response.
We conducted three Community Resilience Workshops to unify stakeholders, understand resilience needs, and assess disaster scenarios, including a preliminary multi-hazard disaster risk and vulnerability assessment. We collaborated with community leaders, first responders, local public school officials, residents, and resource and service providers to identify gaps and needs of vulnerable communities with limited capacity to access and manage critical resources and services (connectivity, energy, mobility). Our efforts led to: (1) the identification of critical communication gaps that must be resolved to maximize the effectiveness of CRHs; (2) the recognition of the essential need for improved coordination among community groups and emergency management authorities; and (3) the emphasis on the critical role of workforce development, training, and sustained stakeholder engagement to enhance disasters preparedness and empower community-led responses.
The project team explored the novel application of digital twin technology in community resilience. We co-created an innovative framework for integrating digital twin technology, which creates virtual models of physical systems and provides real-time data and simulations to support informed decision-making, into the infrastructure of CRHs. This framework aims to transform communities' capacity for disaster resilience by integrating three key components:
1. Promoting community knowledge (Information Hub)
2. Reshaping connectivity between local communities (Communication Hub)
3. Building resilience capacity and access to resources and services (Service Hub)
Our community engagement involved diverse stakeholders, including school officials, community members, and emergency management professionals, to ensure that the framework addresses real-world needs and challenges. We identified priority Key Performance Indicators (KPIs) such as Coordination with External Agencies, Community Inclusivity, Trustworthiness, Information Sharing, and Community Participation as benchmarks for assessing and improving the resilience capacity of CRHs. Designed to facilitate disaster communications and community engagement, this approach bridges the gap between technological innovation and social infrastructure, paving the way for new methodologies in disaster preparedness and response.
The project raised awareness about the importance of community-led response efforts and the role of CRHs in disaster preparedness. It fostered interdisciplinary collaboration among community stakeholders, first responders, and academic partners, ensuring diverse perspectives were considered in planning and execution. This planning grant has established a foundation for an inclusive, community-led, data-driven disaster decision-making through innovative technology integration and stakeholder collaboration. These efforts aim to enhance response strategies by promoting community knowledge, improving connectivity, and building resilience capacity and access to resources and services. The project's outcomes provide a path for implementing and scaling these initiatives, ultimately contributing to more resilient and sustainable communities.

Collaborative Research: Visual Tactile Neural Fields for Active Digital Twin Generation
Robots will perform better at everyday activities when they can quickly combine their sensory data into a model of their environment, just like how humans instinctively use all their senses and knowledge to accomplish daily tasks. Robots, however, must be programmed to create these models that humans do intuitively, effortlessly, and robustly. This robotics project explores a novel algorithmic approach that combines visual and tactile sensory data with a knowledge of physics and a capability to learn that makes robot planning and reasoning more effective, efficient, and adaptable. The project includes the development and testing of research prototypes, preparation of new curriculum, and outreach to high school students and teachers and to the general public.

This project introduces a new data representation, called a Visual Tactile Neural Field (VTNF), that allows robots to combine data from visual and tactile sensors to create a unified model of an object. The VTNF is designed to be used in a closed-loop manner, where a robot may use data from its physical interactions with an object to create or improve a model and may use its current understanding of a model to inform how best to interact with a physical object. Towards this end, the investigators create the mathematical techniques, computational tools, and robot hardware necessary to generate a VTNF model. The investigators also develop techniques to quantify the uncertainty about an object and use this uncertainty to learn search policies that allow robots to generate accurate models as quickly as possible. The VTNF, which allows for the easy addition of new properties about an object, provides a flexible representational foundation for other researchers and practitioners to use to enable robots to learn faster by having a more detailed understanding of both the surrounding environment and their interactions with it.

This project is supported by the cross-directorate Foundational Research program in Robotics and the National Robotics Initiative, jointly managed and funded by the Directorates for Engineering (ENG) and Computer and Information Science and Engineering (CISE).

Collaborative Research: FW-HTF-R: Future of Construction Workplace Health Monitoring
Given the disproportionate rate of fatalities and injuries in the construction industry and the potential of ambiguous health and hazardous situations with respect to the impending technological revolution and climate change, it is crucial to improve the health and safety of the future workforce. However, there is a lack of an effective, objective, and continuous approach for assessing construction workers' health status at jobsites. Although there have been important innovations in wearable physiological sensing technologies and artificial intelligence for objective assessment of construction workers' health parameters, there remain fundamental challenges for establishing a worker-centered holistic health monitoring approach with promising preventive potentials. These challenges stem from: a) lack of a scalable and feasible wearable sensor for continuous elicitation of workers' diverse bodily responses to stressors in the field; b) lack of a robust interpretive data-driven framework to process the elicited signals for automatic early detection of physical fatigue, mental stress, and exposure to heat stress; and c) lack of effective representation of health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. By establishing a real-time and context-aware holistic health monitoring approach, this project will play a fundamental role in improving the safety of close to 7 million workers in the U.S. construction sector. The developed intelligent health monitoring system is expected to produce changes in the quality of work and workforce policies, resulting in reduced conflicts and enhanced quality of life. It can also be used to address workplace health issues in other hazardous industries such as manufacturing, firefighting, and agriculture.

The overarching goal of this research is to improve construction workforce health and safety by integrating multi-disciplinary research in flexible, wearable sensor fabrication, artificial intelligence, and privacy-aware information visualization to provide near-real-time and projected future context-aware health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. The intellectual significance of this project lies in fulfilling the goal by generating and expanding new knowledge on three fronts. First, the project will design and fabricate a flexible wearable sensor for continuous and noninvasive measurement of workers' bioelectric signals and electrochemical responses at construction sites. The use of a single, flexible wearable sensing device instead of multiple off-the-shelf sensors will facilitate the scalability and feasibility of the proposed health sensing system in the construction workplace. Second, the project will develop robust machine learning algorithms and frameworks for continuous and objective assessment of workers' health conditions in the field based on physiological, contextual, and environmental data. For this purpose, this project will address fundamental challenges related to traditional machine learning algorithms by developing a novel interpretive data-driven approach robust to inter- and intra-individual variability while ensuring data security and privacy. Third, this research will generate a digital twin model (health and safety maps) of the construction sites through an array of collective health analyses and develop an automated feedback module for providing personal health-related information and corresponding mitigation strategies to field workers. The insights into the collective health and safety information can profoundly assist the workers and safety managers in making a sound, far-sighted decision about the execution of field-oriented construction operations in near real-time. This research effort will open new doors in improving proactive health and safety management in the field through collective visualization of workers' real-time health and safety information.

CIVIC-PG Track B: Co-Creating a Community Platform to Improve Services for People on the Homelessness Continuum

Homelessness encompasses a wide range of lived experiences, from housing instability to reintegration from incarceration, couch-surfing to street homelessness, and episodic to chronic, with some people having multiple experiences, changing over time. As a challenge facing communities nationwide, it is vital to ensure that policymakers and community members have high-quality, up-to-date, data and information about homelessness and the people who are experiencing it. This project will bring together people experiencing homelessness, policymakers, and other community organizations to design a platform for data sharing and collaboration. The platform will help to ensure that people experiencing homelessness can access services and resources, while also providing the best evidence possible for how social service organizations can best leverage their resources to serve this community.

Our government-nonprofit-university collaboration proposes to co-create and pilot a community platform supporting data integration efforts allowing for more timely and tailored provision of wrap-around services when, where, and by whom they are most needed. The City of Austin previously has invested in more effectively using its data ? a key example is the Affordable Housing Search Tool, which resulted from the 2019 Code for America Summit. The Downtown Austin Community Court (DACC) innovative Intensive Case Management program provides services to people on the homelessness continuum, including substance abuse treatment, counseling, assistance with public benefits applications, job search assistance, and temporary housing. Good Systems, a UT Grand Challenge, has partnered with DACC and other City of Austin departments to improve services for people on the homelessness continuum, including over 60 interviews with case managers and service providers, and over 100 surveys from people on the homelessness continuum. This platform will include a public-facing dashboard providing information at the Austin City Council District level creating a shared interface enabling service providers, users, and community members to obtain and share information about resource availability and needs. The back-end of the system will serve as a digital twin for the City of Austin, allowing for more accurate prediction of when and where resources will be needed, which will only be accessible to trusted organizational partners who complete a memorandum of understanding and a data sharing agreement. The system will empower users to make evidence-based predictions about future service and resource needs. Service providers will be enabled to provide more timely and tailored services to proactively address these community needs. Hence, the platform will bridge the gap between essential resources and services and community needs.

This project is in response to the Civic Innovation Challenge program?Track B. Bridging the gap between essential resources and services & community needs?and is a collaboration between NSF, the Department of Homeland Security, and the Department of Energy.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
We convened two community co-creation workshops. First, we convened a workshop in Austin, with 55 participants, including representatives from local, state, and federal government; 18 nonprofits; and members of the community served. To explore the scalability and transferability of this research, we convened a second workshop, which we held online with 41 participants from 12 states and territories. The workshops revealed the main data collection concerns as lack of collection accuracy, consistency across organizations, and collectors’ limited resources. There was a shared grievance that organizations generally have their own data model for storage, making cross-organizational querying difficult. Many participants referenced the need for an easy-to-use interface with concise language and policies, and video interpretations, to help users comprehend the platform. Participants in both workshops emphasized the need for simplified reporting and data assessment with a data system accessible to people with different data literacy levels. Participants were not only concerned with the technical features of the platform, but they also provided recommendations for governance, ongoing collaboration, and concerns about social and organizational sustainability.
A key outcome was that local workshop findings have the potential to be transferred to a broader context, and that similar concerns exist in data collection, use, and management for social service provision despite significant regional variations in resources, governance, and approach. During the workshop, representatives from the Ending Community Homelessness Coalition (ECHO) heard from homelessness service providers that are unable to pay the Homelessness Management Information System (HMIS) licensing fees, which creates a barrier to getting access to data, including data that they entered into the system. Since our workshop, ECHO informed our team that they have waived the HMIS licensing fees for all service providers in the Austin/Travis County Continuum of Care (CoC). This is a vital step in ensuring equitable data access given the important role that ECHO plays in managing data for the community.

SCC-IRG Track 1 Designing Smart, Sustainable Risk Reduction in Hazard-Prone Communities: Modeling Risk Across Scales of Time and Space
The exponential increase in extreme events over the last decade compels new methods of managing risk in communities exposed to recurring natural hazards. This project advances the National Science Foundation?s goal ?Growing Convergence Research? to enable smart and connected communities by initiating and expanding collective learning capacity through integrating digital twin technologies and social games. This project proposes to engage decision makers across sectors and scales of jurisdiction in managing risk by reallocating attention, time, resources and overcoming barriers to act collectively as hazardscapes change. This project will use the threat of wildfire across two communities in northern California as community engagement study sites. Working with thirteen community partners, the project will develop an innovative sociotechnical digital twin of the San Francisco Bay Area that integrates virtual models of physical infrastructure systems, social/commercial networks, and insurance mechanisms that distribute risk over space and time. Serious games will be designed to activate learning processes inherent in play to engage community?s awareness and commitment to collective action.

This project will use a complex systems approach to hazard reduction across multiple scales of risk by developing a new generation of socio-technical digital twin that integrates models of physical infrastructure systems and virtual networks of communication with social games to engage community stakeholders? awareness and commitment to collective action. Using a conceptual framework of complex adaptive systems, this project will investigate whether community learning processes that focus on cognition and action will mitigate wildfire risk in the short-term and lead to sustainable adaptation to recurring risk conditions in the long-term. This inquiry advances risk management theory by testing a prototype sociotechnical framework for developing shared knowledge to support decision making by multiple actors at different scales to reduce hazard risk. The sociotechnical digital twin provides a macro view of risk at the regional scale, as well as detailed views of interactions at the micro scale, essential to manage operations. Translating risk information into formats that are easily understood by different groups and embedding learning processes in gaming scenarios to advance risk reduction is transformative. A major goal is to shift the perspective from reaction to extreme events after they occur to anticipation of risk and mitigation of potential losses before hazards occur. Using serious games, a process of iterative learning for diverse community actors increases the level of shared cognition of risk and commitment to action. The project will engage under-represented minorities in affected regions and support decision-makers in vulnerable communities.

This project is in response to the Smart & Connected Communities program. It is co-funded by the Advancing Informal STEM Learning program which seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments.

MRI: Acquisition of the Kentucky Research Informatics Composable Cloud (KyRICC)
Scientific discovery today is driven by computation and data-intensive research that exploits the growing amounts of available data. However, the wide variety and size of emerging datasets often make analysis challenging on current high-performance computing (HPC) infrastructures because system configurations cannot be customized to process the data efficiently. This project will acquire and deploy a dynamically composable computer infrastructure called the Kentucky Research Informatics Composable Cloud (KyRICC). This KyRICC architecture will support complex data analysis pipelines with highly heterogeneous hardware requirements not currently supported by current HPC infrastructures. As a result, this project will enable and support a wide range of new research activities. KyRICC will be used by hundreds of University of Kentucky (UK) researchers (faculty, staff, and students) and by other computational research collaborators at institutions across the region including Centre College, Morehead State University, Eastern Kentucky University, University of Louisville, Northern Kentucky University, and Kentucky State University. As a leading-edge system, KyRICC and the exciting projects it makes possible will help recruit students, including students from groups underrepresented in STEM, to computational research. The system will also enhance the research training of many undergraduates, graduate students, and postdocs in Kentucky colleges and universities.

The KyRICC architecture will support complex data analysis pipelines with highly heterogeneous hardware requirements across individual data analysis steps. Specifically, KyRICC will integrate four subsystems that will enable dynamically composable cloud infrastructure: (1) A cluster of peripheral-composable compute nodes, allowing for up to 10?s of GPUs and 10?s of TB of main memory on a single node. Groups of nodes can be dynamically allocated to allow the training and inference of very large deep learning models and datasets; (2) A next-generation high-speed NVMe-based storage cluster capable of efficiently serving large volumes of data to multi-GPU nodes. Unlike traditional clustered storage systems, this composable filesystem allows the partitioning of storage on the project-level, allowing us to isolate data and better manage system performance; (3) A Peta-scale storage system provided by UK?s current research storage infrastructure, providing a total of 2.2 PB of storage; and (4) An innovative workload management system for dynamic infrastructure composition, workload profiling, model and infrastructure tuning, supporting common pipelines and machine and deep learning models through templated projects. KyRICC will be a regional computational resource and will also be made available to the broader national computational research infrastructure through the NSF-supported ACCESS projects. Areas of expected breakthroughs in KyRICC-enabled research include deep learning and computer vision; natural language processing and multimodal embedding; computational modeling and simulation with data analytics; and omics analysis and systemic integration.

This project is jointly funded by the Major Research Instrumentation (MRI) program, the Established Program to Stimulate Competitive Research (EPSCoR), and the Computer & Information Science & Engineering (CISE) Directorate.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Scientific discovery today is driven by computation and data-intensive research that exploits the growing amounts of available data. However, the wide variety and size of emerging datasets often make analysis challenging on current high-performance computing (HPC) infrastructures because system configurations cannot be customized to process the data efficiently. This project will acquire and deploy a dynamically composable computer infrastructure called the Kentucky Research Informatics Composable Cloud (KyRICC). This KyRICC architecture will support complex data analysis pipelines with highly heterogeneous hardware requirements not currently supported by current HPC infrastructures. As a result, this project will enable and support a wide range of new research activities.
While GPUs have been part of HPC deployments for many years, recent advancements in the application of accelerators across machine learning, omic analysis, and simulation, have greatly increased demand.  Accelerator workload diversity has also increased, with large language models requiring multi-node scale for the training and inference of single models.  Conversely, reinforcement models, like those used in digital twin simulation, might require many GPU instances, but in a much smaller capacity.  It is increasingly important that HPC systems implement features to better adapt available resources to specific workloads.
The Kentucky Research Informatics Composable Cloud (KyRICC) was created to provide flexible yet specialized infrastructure in support modern computational research.  KyRICC is composed of five densely connected (IB 4x400Gb/sec + Eth 2x100Gb/sec) nodes NVIDIA DGX nodes, each with 8 X H100 80G GPUs.  The total cluster provides 40 x H100 GPUs (3200GB vRAM), 10TB of CPU RAM, 174TB of onboard NVMe storage, and 560 CPU cores.  Accompanying computational infrastructure are management and login nodes.  While deployed in a secure environment, the cluster is equipped with dual purpose login/data transfer nodes with 100G connectivity to Internet2.  Fractions of a node can be isolated for computation or the entire cluster, supporting a wide range of computational needs.            
The compositional features of KyRICC provide a foundation for process, resource, and data isolation.  Building on these features KyRICC supports workloads with protected and sensitive data.  While the system can be used for general purpose computation, optimized pipelines have been developed and validated for accelerator-dependent workloads, such as large multimodal models, accelerated omics, and digital twin / time-series analysis.

Collaborative Research: FW-HTF-R: Future of Construction Workplace Health Monitoring
Given the disproportionate rate of fatalities and injuries in the construction industry and the potential of ambiguous health and hazardous situations with respect to the impending technological revolution and climate change, it is crucial to improve the health and safety of the future workforce. However, there is a lack of an effective, objective, and continuous approach for assessing construction workers' health status at jobsites. Although there have been important innovations in wearable physiological sensing technologies and artificial intelligence for objective assessment of construction workers' health parameters, there remain fundamental challenges for establishing a worker-centered holistic health monitoring approach with promising preventive potentials. These challenges stem from: a) lack of a scalable and feasible wearable sensor for continuous elicitation of workers' diverse bodily responses to stressors in the field; b) lack of a robust interpretive data-driven framework to process the elicited signals for automatic early detection of physical fatigue, mental stress, and exposure to heat stress; and c) lack of effective representation of health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. By establishing a real-time and context-aware holistic health monitoring approach, this project will play a fundamental role in improving the safety of close to 7 million workers in the U.S. construction sector. The developed intelligent health monitoring system is expected to produce changes in the quality of work and workforce policies, resulting in reduced conflicts and enhanced quality of life. It can also be used to address workplace health issues in other hazardous industries such as manufacturing, firefighting, and agriculture.

The overarching goal of this research is to improve construction workforce health and safety by integrating multi-disciplinary research in flexible, wearable sensor fabrication, artificial intelligence, and privacy-aware information visualization to provide near-real-time and projected future context-aware health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. The intellectual significance of this project lies in fulfilling the goal by generating and expanding new knowledge on three fronts. First, the project will design and fabricate a flexible wearable sensor for continuous and noninvasive measurement of workers' bioelectric signals and electrochemical responses at construction sites. The use of a single, flexible wearable sensing device instead of multiple off-the-shelf sensors will facilitate the scalability and feasibility of the proposed health sensing system in the construction workplace. Second, the project will develop robust machine learning algorithms and frameworks for continuous and objective assessment of workers' health conditions in the field based on physiological, contextual, and environmental data. For this purpose, this project will address fundamental challenges related to traditional machine learning algorithms by developing a novel interpretive data-driven approach robust to inter- and intra-individual variability while ensuring data security and privacy. Third, this research will generate a digital twin model (health and safety maps) of the construction sites through an array of collective health analyses and develop an automated feedback module for providing personal health-related information and corresponding mitigation strategies to field workers. The insights into the collective health and safety information can profoundly assist the workers and safety managers in making a sound, far-sighted decision about the execution of field-oriented construction operations in near real-time. This research effort will open new doors in improving proactive health and safety management in the field through collective visualization of workers' real-time health and safety information.

Collaborative Research: Visual Tactile Neural Fields for Active Digital Twin Generation
Robots will perform better at everyday activities when they can quickly combine their sensory data into a model of their environment, just like how humans instinctively use all their senses and knowledge to accomplish daily tasks. Robots, however, must be programmed to create these models that humans do intuitively, effortlessly, and robustly. This robotics project explores a novel algorithmic approach that combines visual and tactile sensory data with a knowledge of physics and a capability to learn that makes robot planning and reasoning more effective, efficient, and adaptable. The project includes the development and testing of research prototypes, preparation of new curriculum, and outreach to high school students and teachers and to the general public.

This project introduces a new data representation, called a Visual Tactile Neural Field (VTNF), that allows robots to combine data from visual and tactile sensors to create a unified model of an object. The VTNF is designed to be used in a closed-loop manner, where a robot may use data from its physical interactions with an object to create or improve a model and may use its current understanding of a model to inform how best to interact with a physical object. Towards this end, the investigators create the mathematical techniques, computational tools, and robot hardware necessary to generate a VTNF model. The investigators also develop techniques to quantify the uncertainty about an object and use this uncertainty to learn search policies that allow robots to generate accurate models as quickly as possible. The VTNF, which allows for the easy addition of new properties about an object, provides a flexible representational foundation for other researchers and practitioners to use to enable robots to learn faster by having a more detailed understanding of both the surrounding environment and their interactions with it.

Collaborative Research: Adaptive Gaussian Markov Random Fields for Large-scale Discrete Optimization via Simulation
Major federal agencies, including the Department of Veterans Affairs, Department of Defense, Department of Homeland Security, Federal Aviation Administration, Department of the Treasury, Internal Revenue Service, Centers for Medicare and Medicaid Services, Department of Health and Human Services, and others, seek government and non-government assistance with the application of scientific, data-driven methods to help them execute effectively on their critical missions. Because their mandate is typically large-scale, complex, and involves inherent uncertainty, computer simulation is often the only tool for representing their problems in a comprehensive way. Similar problems occur in the private sector, especially in health care delivery, computer networks, warehousing and distribution, and transportation systems. Unfortunately, "large-scale, complex, and involving inherent uncertainty" are the features that make "optimizing" a simulated system hard, particularly when the decisions are how to allocate discrete units of resources such as personnel, vehicles and facilities. The proposed research marries high-performance computing, smart numerical methods, and state-of-the-art statistical methodology to significantly increase the size and complexity of simulated systems that can be optimized. As a result, agencies such as those listed above will be able to more fully solve their "system of systems" resource-allocation problems using computer simulation.

The proposed research tackles statistical and computational challenges that arise in solving large-scale stochastic optimization problems when the objective function may only be evaluated by executing a stochastic simulation. Such optimization problems are often with respect to a high-dimensional, discrete-valued decision variable in a large solution space. The modeling flexibility of simulation comes at a cost: arbitrarily complex stochastic simulations may not be optimized using tools from mathematical programming. As a result, the scale of problems that can currently be solved by simulation with an optimality gap guarantee is limited. The investigators propose to create theory, algorithms and software for large-scale discrete-decision-variable simulation optimization that converge to the global optimum asymptotically, and provide optimality-gap inference when terminated. The proposed methods are based on inferential optimization, which models the unknown objective function by a Gaussian Markov Random Field (GMRF), a type of Gaussian Process defined by a graph on the discrete solution space; the investigators have shown that GMRFs provide better inference for a discrete problems than Gaussian processes defined on a continuous domain. The conditional distribution of a GMRF provides inference for selecting solutions to simulate and for search termination when the inferred optimality gap is small. However, the computational cost of numerical linear algebra increases faster than the number of feasible solutions. To facilitate the solution of large-scale problems, three core topics are proposed: exploiting high-performance computing; creating a restricted search scheme and tailored computational linear algebra that significantly reduces the computations in GMRF updates; and attacking limits on dimensionality via an adaptive multi-resolution GMRF and projections to lower dimensions. This award will provide support of graduate student training through research.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Stochastic simulation is a tool for the design, analysis and improvement of large-scale systems that are subject to risk and uncertainty. Application areas are broad and diverse, including manufacturing, supply chains, healthcare delivery, transportation, digital twin technology and service systems. Simulation optimization is the key technology employed by industrial engineering, operations research and management science to attain the best possible system performance using stochastic simulation. In many simulation optimization problems the underlying decision variables over which the system can be optimized are discrete valued; examples include the number of units of each product to stock in a sports equipment supply chain, the number of beds to allocate to mental health patients in a hospital, and the number of vans to include in a dial-a-ride fleet. However, solution technology for discrete-decision-variable problems has been greatly limited in the size (number of feasible solutions) and dimension (number of quantities to be set) of the decision variables of problems that are practically solvable. This research project pushed these limits back substantially, thereby allowing richer real-world problems to be solved.
The intellectual merit of this project was extending Bayesian optimization to discrete-decision-variable problems by employing an innovative prior distribution---discrete Gaussian Markov Random Fields---smart computational linear algebra for posterior updating, projection and decomposition techniques to reduce dimension, a new Bayesian acquisition function for guiding the search, innovations in hyperparameter estimation, and parallel computing to greatly extend the size and dimension of practical problems that can be solved. Problem classes addressed include simulation optimization over integer-ordered decision variables, over integer-ordered decision variables with a priori objective function information, over categorical decision variables, and over categorical decision variables in the presence of real-time covariate information. Specific applications included future mobility simulations for General Motors, air traffic recovery from weather and equipment disruptions, and real-time assortment optimization using publically available data from Yahoo! In addition, the grant supported related work in simulation analytics (i.e., deeper analysis of stochastic simulation outputs).  
The research outcomes, including publically available, open-source software, have broad impacts across the application domains noted above. Further, the methodological breakthroughs have value in Bayesian optimization more broadly, including the design and analysis of deterministic computer experiments. One Ph.D. student at Penn State University and one Ph.D. student at Georgia Institute of Technology were supported, and publication and presentation of the work was disseminated widely via journals, conference proceedings and highly visible workshops.

Collaborative Research: Visual Tactile Neural Fields for Active Digital Twin Generation
Robots will perform better at everyday activities when they can quickly combine their sensory data into a model of their environment, just like how humans instinctively use all their senses and knowledge to accomplish daily tasks. Robots, however, must be programmed to create these models that humans do intuitively, effortlessly, and robustly. This robotics project explores a novel algorithmic approach that combines visual and tactile sensory data with a knowledge of physics and a capability to learn that makes robot planning and reasoning more effective, efficient, and adaptable. The project includes the development and testing of research prototypes, preparation of new curriculum, and outreach to high school students and teachers and to the general public.

This project introduces a new data representation, called a Visual Tactile Neural Field (VTNF), that allows robots to combine data from visual and tactile sensors to create a unified model of an object. The VTNF is designed to be used in a closed-loop manner, where a robot may use data from its physical interactions with an object to create or improve a model and may use its current understanding of a model to inform how best to interact with a physical object. Towards this end, the investigators create the mathematical techniques, computational tools, and robot hardware necessary to generate a VTNF model. The investigators also develop techniques to quantify the uncertainty about an object and use this uncertainty to learn search policies that allow robots to generate accurate models as quickly as possible. The VTNF, which allows for the easy addition of new properties about an object, provides a flexible representational foundation for other researchers and practitioners to use to enable robots to learn faster by having a more detailed understanding of both the surrounding environment and their interactions with it.

This project is supported by the cross-directorate Foundational Research program in Robotics and the National Robotics Initiative, jointly managed and funded by the Directorates for Engineering (ENG) and Computer and Information Science and Engineering (CISE).

Collaborative Research: FW-HTF-R: Future of Construction Workplace Health Monitoring
Given the disproportionate rate of fatalities and injuries in the construction industry and the potential of ambiguous health and hazardous situations with respect to the impending technological revolution and climate change, it is crucial to improve the health and safety of the future workforce. However, there is a lack of an effective, objective, and continuous approach for assessing construction workers' health status at jobsites. Although there have been important innovations in wearable physiological sensing technologies and artificial intelligence for objective assessment of construction workers' health parameters, there remain fundamental challenges for establishing a worker-centered holistic health monitoring approach with promising preventive potentials. These challenges stem from: a) lack of a scalable and feasible wearable sensor for continuous elicitation of workers' diverse bodily responses to stressors in the field; b) lack of a robust interpretive data-driven framework to process the elicited signals for automatic early detection of physical fatigue, mental stress, and exposure to heat stress; and c) lack of effective representation of health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. By establishing a real-time and context-aware holistic health monitoring approach, this project will play a fundamental role in improving the safety of close to 7 million workers in the U.S. construction sector. The developed intelligent health monitoring system is expected to produce changes in the quality of work and workforce policies, resulting in reduced conflicts and enhanced quality of life. It can also be used to address workplace health issues in other hazardous industries such as manufacturing, firefighting, and agriculture.

The overarching goal of this research is to improve construction workforce health and safety by integrating multi-disciplinary research in flexible, wearable sensor fabrication, artificial intelligence, and privacy-aware information visualization to provide near-real-time and projected future context-aware health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. The intellectual significance of this project lies in fulfilling the goal by generating and expanding new knowledge on three fronts. First, the project will design and fabricate a flexible wearable sensor for continuous and noninvasive measurement of workers' bioelectric signals and electrochemical responses at construction sites. The use of a single, flexible wearable sensing device instead of multiple off-the-shelf sensors will facilitate the scalability and feasibility of the proposed health sensing system in the construction workplace. Second, the project will develop robust machine learning algorithms and frameworks for continuous and objective assessment of workers' health conditions in the field based on physiological, contextual, and environmental data. For this purpose, this project will address fundamental challenges related to traditional machine learning algorithms by developing a novel interpretive data-driven approach robust to inter- and intra-individual variability while ensuring data security and privacy. Third, this research will generate a digital twin model (health and safety maps) of the construction sites through an array of collective health analyses and develop an automated feedback module for providing personal health-related information and corresponding mitigation strategies to field workers. The insights into the collective health and safety information can profoundly assist the workers and safety managers in making a sound, far-sighted decision about the execution of field-oriented construction operations in near real-time. This research effort will open new doors in improving proactive health and safety management in the field through collective visualization of workers' real-time health and safety information.

FMRG: Eco: Cyber Enabled Transformation to Circular Supply Chains for Sustainable Pharmaceutical Manufacturing Networks
Current pharmaceutical manufacturing systems follow the linear model of ?take, make, and dispose?. This model is neither economically nor environmentally sustainable. To enable future manufacturing to transition to a circular economy, making more efficient use of materials through recycling and repurposing, there is a growing emphasis on moving towards circular supply chains (CSCs). Many industries have begun to convert from centralized batch manufacturing to automation-driven continuous manufacturing that is based on actual, rather than predicted, levels of demand. Yet, while this reduces waste at the single-plant scale, it is not sufficient for the creation of a complete zero waste CSC integrated industrial ecosystem. Models do not exist that connect single-scale plants to the larger network, making it impossible to account for feedback interactions between single-scale process innovations and existing processes across the manufacturing network to optimize resource utilization. The goal of this project is to enable the design of zero waste future pharmaceutical manufacturing networks at a macroscale using an integrative, multidisciplinary approach focused on novel process chemistry and separation methods, macroscale modeling of manufacturing networks to integrate new processes at scale, and pricing optimization for collaborative decision making through dedicated cyberinfrastructure development. The cyberinfrastructure developed will aid in understanding the feedback loops between plant-scale automated manufacturing and macroscale manufacturing networks in making optimal decisions for adoption of recycling processes at scale and redesign of material flows towards CSCs.

Targeted key contributions of the project are: 1) creating novel modular recycling processes for waste medicines to enable CSCs and digital twin models for the facilities, 2) automating the integration of novel processes in an existing network by advancing algorithms for multi- scale integration and advancing cyberinfrastructure, and 3) creating a dual pricing mechanism and econometric model to transition the whole system to a CSC by facilitating waste exchange and use of recycled active ingredients. These advancements will provide a framework to drive innovations in manufacturing processes at the single scale in convergence with macroscale manufacturing networks to create a zero-waste future manufacturing system. The approach of integrating modular manufacturing processes at the right scale in the overall network for zero waste pharmaceutical manufacturing will be generalizable to other future manufacturing networks. Graduate education in engineering will be enhanced to include convergence thinking by developing a ?Computational Lab? to train interdisciplinary students in process design through macroscale network interactions for economic and environmental sustainability. Undergraduates and minority students will be engaged in research through Purdue's SURF program and existing Purdue partnerships with HBCUs and Purdue?s Women in Engineering program. For future workforce development in pharmaceutical manufacturing, the project team will partner with Purdue?s existing Biotechnology Innovation and Regulatory Science program and Office of Professional Practice to translate the research outcomes into industrial practice training through workshops for industrial stakeholders, student internships, and partnerships with Ivy Tech Community College. Integration through a web-based Hub of citizen data for use in driving network decisions will facilitate participation of citizens in sustainable future manufacturing.

NSF Convergence Accelerator Track J: Building a digital twin for national-scale field-level crop monitoring, prediction, and decision support

Crop production in the U.S. feeds not only the U.S. but also the world. During the 2020/2021 fiscal year, U.S. exports accounted for over 25% of total grain traded globally. A healthy crop cropping systems (CCS) is vital for achieving food and nutrient security of the U.S. and the world and enhancing the competitiveness of U.S. agriculture in the world market. Yet, crop production creates large environmental footprint. The USDA Agricultural Innovation Agenda calls for increasing U.S. agricultural production by 40% while cutting its environmental footprint in half by 2050. Sound crop management decision-making is a key in reaching this ambitious goal. Traditionally, crop management decisions are made by individuals based on their empirical judgment, which is often subjective and far from optimal. On the other hand, the science-based, data-driven approach for crop management decision-making relies on timely and accurate information on current and predicted future conditions of crop, soil, weather, and market to make optimal decisions. Studies demonstrated that the data-driven approach can overcome the inherent deficiencies in the empirical approach and bring significant economic and environmental benefits. However, it remains a challenge for stakeholders to utilize the data-driven approach because they don?t have full and effective access to the timely and accurate information and lack facilities or knowledge to process the information. This project will provide such timely information and decision support to stakeholders for enabling the data-driven optimal decision-making nationwide at field scales by developing the CropSmart Digital Twin (CSDT). CSDT will not only accurately represents the current conditions, but also predict, with acceptable confidence levels, future conditions of CCS with hypothetical ?what if? scenarios, resulting in actionable predictions. The project will provide significant help in reaching the USDA Innovation goal and greatly enhance food and nutrition security of the U.S. and the world.

Crop production is the foundation for food and nutrition security in the U.S. and the world. However, it also creates large environmental footprint. The USDA Agricultural Innovation Agenda calls for increasing U.S. agricultural production by 40% while cutting its environmental footprint in half by 2050. The data-driven approach for crop management decision-making, which relies on timely and accurate information on current and predicted future conditions of crop, soil, weather, and market to make optimal management decisions, has demonstrated its great potential to help USDA reach its ambitious goal. However, it remains a challenge for stakeholders to adopt the approach because they don?t have effective access to the decision-ready information (DRI) and lack facilities or knowledge to process the information. This project proposes to build the CropSmart Digital Twin (CSDT) with innovative Earth system DT technologies to facilitate the data-driven approach. The overarching goal is to ensure food and nutrition security by enhancing crop productivity and reducing environmental footprint in the U.S. through wide adoption of the data-driven approach enabled by CSDT. The major project activities include: (1) understanding stakeholders? requirements on DRI and decision support; (2) identifying existing data, technologies, and gaps for CSDT; (3) quickly prototyping CSDT by integrating existing technologies and developing gap-filling technologies; 4) broadening participation and impact by training agricultural workforce through comprehensive extension; and (5) establishing a community-based CSDT network for long-term sustainability. This project explores the convergent approach for quickly constructing an operational DT through integration of multi-disciplinary components and services with interoperability technology. It demonstrates the advantage of multi-disciplinary collaboration and feasibility, usability, and value of DT as a multi-disciplinary integration platform for enabling the data-driven approach. The project will help USDA reach its Innovation Agenda goal and enhance food and nutrition security of the U.S. and the world.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Crop production is the foundation for food and nutrition security in the U.S. and world. However, it also creates large climate and environmental footprint. The USDA Agricultural Innovation Agenda calls for increasing U.S. agricultural production by 40% while cutting the climate and environmental footprint of U.S. agriculture in half by 2050. Sound crop management decisions are the key for reaching this ambitious goal. Examples of such decisions are “when should I irrigate my corn field with how much water?”, and “when should I fertilize my wheat field?”.  The science-based, data-driven crop management decision making approach, which relies on timely and reliable information on current and predicted future conditions of crop, soil, weather, and market conditions to make optimal management decisions, can achieve significant economic and environmental benefits and cost savings. However, it remains a great challenge for decision makers to adopt the approach because they don’t have effective access to the timely and accurate information in a format that provides specific support for actions and decisions and lack facilities and/or knowledge to handle the information. 
 
The recent innovation in Earth Science Digital Twin (ESDT) provides us a great opportunity to facilitate the data-driven approach in crop management decision making. This project proposed to build, operate, and promote an ESDT, the CropSmart Digital Twin (CSDT), to facilitate the data-driven cropping decision making nationwide. The overarch goal is to ensure the food and nutrition security of the U.S. and the world by enhancing the crop productivity and reducing environmental footprints in the U.S. through wide adoption of the data-driven approach enabled by CSDT. The major goal of the phase 1 of this project, of which this project outcome report covers, is to demonstrate the feasibility and potentials of proposed CSDT to facilitate the data-driven cropping decision making. To realize Phase 1 goal, the project team has conducted the following major activities, including: 
(1)  conducting end-user interviews to understand stakeholders’ difficulties, needs, and requirements on cropping decision making. The project team interviewed 19 end users, including farmers, crop advisors, agri-business operators, researchers, extension experts, museum content providers, and state and federal agricultural officers, with 21 generic questions relevant to all end-users and 6 sets of end-user specific questions. In addition, the project team conducted two workshops with a total of 60 participants to solicit requirements and inputs
(2)  designing a preliminary CSDT architecture by identifying components, interfaces, services, data sources, existing technologies, and technology gaps for CSDT to functionally meet stakeholders’ requirements 
(3)  developing the low-fidelity CSDT prototype. After initial user interview, the CropSmart low-fidelity prototype was quickly implemented for corn and soybeans in Nebraska at up to field level (up to 10 meters) with a limited number of cropping parameters and decision-making scenarios by integrating and reusing components and services of existing operational systems to demonstrate the feasibility and potentials of proposed CSDT to meet stakeholders’ decision requirements
(4)  participating in the NSF Convergence Accelerator Curriculum to learn the use-inspired research and team-science principles. The project team drafted the team science collaboration agreement, the intellectual property management plan, and project sustainability plan and created project logo, tagline, short project description, market video, and initial, mid-term, and final project pitches. These materials will be finalized at beginning of Phase-2 and used to guide the Phase 2 implementation  
(5)  preparing the phase 2 proposal for securing NSF funding to develop, operate, and promote operational CSDT.
(6)  providing training and learning opportunities to the next-generation agricultural workforce through participating in this cutting-edge research
(7)  conducting comprehensive outreach and extension activities to broaden community awareness of digital innovation in agriculture
 
PIRE: Multi-Domain, Multi-Scale, Policy-Aware Digital Twin for Offshore Wind Energy Infrastructure
For the US to achieve the offshore wind goals of 30 Gigawatts by 2030, approximately 2000 offshore wind turbines (OWTs) need to be installed in the coming years. Only 7 are currently operating in US waters. For comparison, close to 6000 foundations are operating in European waters. The size, expense, and importance of OWTs to mitigating climate change necessitates to consider them as civil infrastructures. Such infrastructures must be built to last for over 50 or even 100 years. However, a conception of OWTs as infrastructure has not caught up with their rapid growth. Most OWTs are typically designed for a 25-to-35-year service life. In a few decades, the industry will face a consequential decision-making challenge as to whether to decommission, rebuild, or retrofit these assets. This challenge is not limited to the US. It has global implications considering the global expansion of offshore wind energy. Here, the team develops a joint modeling framework for decision making about OWT safety, operation and maintenance, life extension and design. International collaboration is essential because the developers, designers, and operators for offshore wind energy farms are almost entirely from Europe. Leveraging the European experience from their international collaborators, the researchers use both quantitative and qualitative data collected directly from OWTs, the workers who service them, and the decision makers who enable their development. Their goal is to find solutions to improve the resilience of offshore wind turbines, notably while confronted to more frequent extreme weather events. By improving OWT service life, this project paves the way to efficiently develop the US clean-energy infrastructures. The project also provides support and training to 1 postdoctoral associate, and graduate and undergraduate students notably from underrepresented groups in science and engineering.

More specifically, the team develops an extensible and customizable joint modeling framework for policy and safety aware digital twins. They use a physics-data-policy-safety co-modeling paradigm, integrated with the help of Agent Based Models. A digital twin is a computational model of an actual OWT (or systems of OWTs) that is maintained and updated based on measured data. The proposed framework is formulated and studied within the context of the Block Island Wind Farm in Rhode Island state waters, the Coastal Virginia Offshore Wind Pilot Project, in US federal waters, and the Levenmouth Demonstration Turbine in the United Kingdom. It enables short, medium, and long-term modeling, learning, and assessment of the offshore wind farms. It uses measured data and integrate with policy, labor, and safety aspects. The multi-domain, multi-scale nature of the renewables and related structural elements is modeled by a physics-based Bayesian Assimilation Framework. It is complemented by data-driven machine learning and transfer learning. The project includes national and international stakeholder engagement programs to facilitate a diverse and inclusive co-production of knowledge. The project has integral educational components including K-12 outreach, professional trainings, and development of graduate students within a transdisciplinary research environment to support the future workforce needed in the offshore wind industry.
FMSG: Cyber: Nanoscale Single Photon 3D Printing at Scale

This grant supports research that contributes new knowledge related to a nanomanufacturing process, promoting both the progress of science and advancing national prosperity and security. Three-dimensional printing or 3D printing is the process of creating three-dimensional (3D) objects from a digital computer model. It has been widely used for applications ranging from product visualization to making engineered parts. Nanoscale 3D printing is useful for making miniaturized structures and devices and can enable new product functions for many applications. However, the common 3D nanoprinting methods are slow and costly because they typically rely on serial point-by-point scanning of an expensive laser beam. This award supports fundamental research to provide the needed knowledge for the development of a fast nanoscale 3D printing method, which uses a low-cost compact light source, similar to that used in laser pointers, and digital light projection for high-throughput fabrication of 3D nanostructures. The single photon 3D nanoprinter?s potential use spans applications in information technology, communications, energy, healthcare, and biomedical industries, which benefits the U.S. economy and society. This multi-disciplinary research involves several disciplines including manufacturing, photochemistry, optics, and materials science. The project helps broaden the participation of women and underrepresented groups in research and training and impacts engineering education and development of the future workforce.

3D nanostructures have properties and functions exceeding those of bulk structures or even properties traditionally not possible. Printing of 3D nanostructures requires a nonlinear process to locally define high-resolution features. The state-of-the-art is femtosecond laser two-photon polymerization (2PP) process. However, the 2PP process is slow, costly and generates structures with micron-scale resolutions. This research is to fill the knowledge gap in the development of a high resolution, high throughput, low-cost 3D nanoprinter. The research team aims to develop a system that uses a low-cost diode laser and single-photon dosage nonlinearity to achieve 1000 times higher throughput, at least 10 times less cost and 50 nm or less resolutions than those possible with conventional 2PP. The team plans to investigate and understand the nanoscale nonlinear single-photon polymerization process, control the diffusion of inhibiting radicals to prevent unwanted polymerization to improve feature resolution, develop a parallel projection method to print entire nanolayers at a time to speed up the printing throughput, and create a machine learning (ML)-guided digital-twin database that connects the desired functionality of the manufactured parts with the build parameters for its future broad adoption as a cyber 3D nanomanufacturing platform.

This project is supported with co-funding from Civil, Mechanical and Manufacturing Innovation (CMMI) Division in the Engineering (ENG) Directorate, and the Chemistry (CHE) Division in the Directorate for Mathematical and Physical Sciences (MPS).

Collaborative Research: Calibrating Digital Twins in the Era of Big Data with Stochastic Optimization

This project will contribute to the national prosperity by providing new calibration methods to generate value-producing opportunities for digital twins in many applications, including energy, healthcare, and manufacturing. A digital twin is a digital representation of a complex physical system that can be useful for monitoring, forecasting, and testing the system in a virtual world. Parameter calibration of digital twins with observational data is one of the most important steps in enabling them to closely replicate a physical system. Today, advanced data sensing and collection technologies provide massive data points from many components of a complex system. The success of this project will provide a means of robust estimation by efficient sampling from these large datasets, thereby significantly reducing the computational burden of calibration. The outreach activities of the project will improve workforce preparation through engagement with industrial practitioners, broaden participation through involvement of underrepresented students in research, and provide opportunities for K-12 students to learn about the field of data science.

Quantitative methods established during this project for digital twin calibration will fully leverage the power of Big Data while addressing the research challenges brought forth by the size and complexity of the datasets. Specific research tasks include: development of stochastic optimization approaches reconciled with statistical theories that will optimally guide simulation experiments by identifying the best (smallest most informative) subsets of data for computational efficiency; extending the integrative optimization framework to be applicable for a wide range of calibration problems, including multi-dimensional, functional, and time-variant calibrations, with theoretical and practical implications; and seamless incorporation of input uncertainty with optimization to dramatically enhance the solution's robustness while maintaining computational tractability. The approach will be validated through real-word case studies in building energy systems and wind power systems.
BRITE Relaunch: Improving Structural Health by Advancing Interpretable Machine Learning for Nonlinear Dynamics

This Boosting Research Ideas for Transformative and Equitable Advances in Engineering (BRITE) Relaunch award will focus on advancing interpretable machine learning to match modeling accuracy with transparency. This will provide structural engineers with a superior and trustworthy tool to model nonlinear dynamical systems. Modeling the complex behaviors of structures and materials under various types of dynamic loads remains a major challenge for smart structures, structural control, nonlinear system identification, damage detection, and earthquake engineering. Machine learning is becoming a popular approach for meeting this challenge. However, there is a significant gap between knowledge of the physics of these systems, the engineering practice design of these systems, and the models produced from machine learning methods. Machine learning lacks interpretability and transparency. This research project will develop systematic solutions with reasoning based on the engineers? knowledge and training, physics-informed, and empowering the engineers? judgment. This research directly benefits society in terms of improving infrastructure health, mitigating the consequences of earthquake, wind hazards, and climate change.

This research project builds upon the project leader?s past work to advance ?nonlinear static function approximation using interpretable machine learning?, given its direct use in approximating nonlinear constitutive relations and its use in approximating nonlinear integrands in ordinary differential equations for nonlinear dynamics. To achieve interpretable and physics-informed machine learning methods, this research project will create new algorithms and implementation procedures. Neuromanifold theories in advanced applied mathematics will be employed to make the training process of sigmoidal neural networks interpretable. Graph theory will be leveraged to create knowledge graphs so that nonlinear static function approximation using interpretable machine learning can be automated during initialization to approximate nonlinear static functions and can be used for deep learning. In addition to extensive cross-validations, a major application of the project's approach will be investigated by using real-world data in a digital twin setting, the state-of-the-art system-level modeling framework in structural engineering. Also, a comprehensive laboratory demonstration and validation will be carried out using timber beam-column joints to generate broad interest in the broad relevance of nonlinear dynamics in structural engineering.

FMSG: Cyber: Perceptual and Cognitive Additive Manufacturing (PCAM)
This grant supports fundamental research on a radical transformation of additive manufacturing through digitally connecting machines, humans, and manufactured products. Additive manufacturing has enabled a new paradigm shift from conventional design for manufacturing approaches into manufacturing for design. A fundamental change in additive manufacturing is necessary as we enter a new era of intelligent future manufacturing beyond additive manufacturing. A promising solution is the convergence of wireless embedded sensors with artificial intelligence (AI) and machine learning (ML) data processes, which can transform the way people interact with manufacturing processes, factory operations, optimizing efficiency, and anomaly system detection that could provide critical information about evaluated components and systems. This project opens a new transitional door to perceptive and cognitive additive manufacturing, enabling true internet of things and digital twin, connecting devices and machines in factories with robots, computers, and humans, and every product we manufacture in factories. The grant will also support educational activities to upskill the manufacturing workforce, K-12, undergraduate and graduate students, and the public, significantly influencing diverse populations of all ages and backgrounds.

Transformation to cyber-physical production manufacturing demands advanced process monitoring through distributed sensing beyond the current state of digitally connected machines and robots collaborating with humans. This project seeks to enable unprecedented wireless fingerprinting and sensing of additively manufactured parts by embedding wireless sensors and performing predictive analysis and health monitoring using AI and ML techniques. This project proposes a holistic approach involving four core research tasks: 1) to study the effects of embedding sensors during additive manufacturing; 2) to design embeddable acoustic sensors and insert them during the manufacturing process to read physical parameters; 3) to prove that embedded passive sensor signals can be sensed wirelessly using millimeter-wave antennas, and 4) to quickly monitor and evaluate the state of manufactured products using ML algorithms. This project has the potential to enable next-generation cyber-physical production systems.

This Future Manufacturing award is supported by the Division of Electrical, Communications and Cyber Systems (ECCS) in the Directorate for Engineering (ENG) and the Division of Computer and Network Systems (CNS) of the Directorate for Computer and Information Science and Engineering (CISE).


Conference: Climate Change and Quantum Computing

The goal of this project is to organize and host a workshop to identify opportunities to address some of the most daunting climate change related problems using advances in quantum computing information science and technology. The core of this effort is to engage and bring together scientists and user communities together to envision how best to leverage the advances in quantum computing to accelerate development of solutions that can help some of the major challenges posed by events related to climate change. The particular focus of the workshop will be on the leveraging convergence between areas related to energy and power, greenhouse emissions, and climate change related simulations. The goal will be to bring experts from industry, academia, foundations, and government agencies to discuss and identify opportunities in which quantum computing as well as quantum technologies such as quantum sensors could offer a unique advantage. The proposed workshop is expected to help identify efforts to develop a track that will be based on using the power of quantum computing, using development of quantum computing algorithms and hardware, to address different areas including but not limited to energy, power grid, transportation, renewable energy transition. The insights gained from this workshop, if implemented into a track, could have transformative impacts on accelerating translational research aspects of quantum computing science and technology in the context of climate change related challenges. Hybrid approaches of classical and quantum computing will also be considered.

This workshop effort plans include online ideation, virtual meetings of stakeholders, and in-person synthesis. A particular focus will be on bringing together researchers and user communities from different segments including climate change, manufacturing, environmental monitoring, clean energy, power grid, transportation, to demonstrate that quantum is a strong enabler for combating climate change.

CAREER: Towards Reliable and Quantum-resistant Connected Vehicle Security
The number of people who die in motor vehicle accidents unfortunately increases every year with human error as a primary cause of serious crashes. To enhance roadway safety and efficiency through driver-assistance and automated-driving systems, cellular vehicle-to-everything (C-V2X) is the globally preeminent technology to support connected vehicle services, including vehicle-to-vehicle (V2V) communications, and is set for expansion toward 6G cellular technology. In particular, V2V communication that allows vehicles to directly exchange messages beyond the perception range of on-board sensors has many tangible benefits to society with the potential to prevent a significant percentage of crashes, increase transportation system efficiency (reducing travel time, pollution, etc.), and enhance mobility for disadvantaged communities. Despite long-standing classical security protocols designed for C-V2X, however, this technology is vulnerable to imminent quantum attacks from emerging quantum computers threatening the ?integrity? of safety-critical messages, and to denial-of-service attacks at the currently unprepared communication protocols hindering possible mitigations for those quantum attacks. Countering these attacks, separately or jointly, is challenging, a significant risk to the safety of the growing connected vehicle services and future autonomous ones, especially as current hardware (with 12-15 year life expectancy on vehicles) is inefficient at running post-quantum algorithms fast enough.

This CAREER project refines and refocuses existing V2V communication and security protocols, without an overhaul, to support post-quantum algorithms, given the hardware and other practical limitations in the automotive industry. With novel security protocols and backward-compatible communication techniques to prevent, mitigate, and evaluate quantum attacks, this project transforms the current C-V2X ecosystem to become crypto-agile for a staged transition over the next two decades into a quantum-resistant one. The research and education agenda is framed around three thrusts: (1) Establishing a roadmap and a transition plan in designing specific interim security protocols for different stages of the transition (e.g., before and after compatible hardware is available); (2) Supporting post-quantum algorithms by optimizing the resource scheduling techniques at the MAC layer to accommodate more messages, and developing novel message sieving techniques at the physical layer to reduce the integrity verification loads on the security modules; and (3) Developing a unique prototype of a digital twin infused with software-defined radios by converting an open-source V2V security testbed to an interactive platform with interfaces to large-scale (NSF-funded) testbeds. As an integral part of this project, a digital twin will be used (instead of costly urban experiments) for comprehensive evaluation of the proposed techniques and development of a novel game for gamifying connected vehicle security education. It cultivates new interdisciplinary research among game design, wireless security, and the automotive industry, providing for synergistic research and education outcomes, and will also help broaden the impacts through inclusive mobility initiatives for disadvantaged (deaf and hard-of-hearing) communities and innovative educational pathways to train next generation of under-represented cybersecurity experts. The PI will disseminate new course modules, hands-on labs, and the new games through open-access lectures, academic/public exhibits, and various collegiate and K-12 competitions. The research outcomes will be shared with stakeholders for supporting safe innovations, standardization bodies (NIST, IEEE) for security standards, and 3GPP and automotive industry for 6G C-V2X specifications.

STTR Phase I: Active Blade Morphing Control to Improve Efficiency and Reduce Loading for Wind Turbines
The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is expanding global deployment of wind turbines with increased production not obtainable with today?s fixed blades. An adaptable blade with advanced control capabilities helps solve technical and scientific challenges as wind projects accelerate their move offshore, extending the physics of the larger turbines needed for future wind farms. Developing a high-fidelity modeling tool to design morphing blades capable of boosting energy production, reducing wear and tear, dampening vibration, improving stability, and reducing load effectively achieves two crucial goals. These goals include accelerating the deployment of renewable energy with affordable electricity that is efficiently and economically extracted from wind and improving the loading and stability necessary for the development of floating wind farms capable of installation in challenging water depths and extreme weather. The technology proposed makes large turbines better at capturing wind in more locations to protect against volatile energy prices, generate jobs, and promote greater participation in the global energy transition for developed and developing countries alike. Furthermore, the blade technology resulting from this research provides opportunities for new manufacturing techniques and commercial applications in other industries such as aviation, automotive, and marine renewable energy.

This STTR Phase I project proposes to examine a high-fidelity model to support the design and control of an advanced wind turbine blade configuration with an adaptive twist angle distribution (TAD). Conventional control is generally applied to rotor torque to maximize wind capture, and thus production, below the rated wind speed. Above this speed, control shifts to the blade pitch angle to maintain full power. However, limitations in existing designs lead to trade-offs where wind capture or power production is relinquished to reduce loads, mitigate vibration, and improve stability. The actively adaptive TAD provides greater control capabilities and satisfies these objectives without trade-offs. A crucial goal of this research is the means to understand the complex aeroelastic and aerodynamic relationships with respect to the TAD. The technology is a high-fidelity model that simulates these dynamics and the aeroelastic performance in a reasonable amount of time. This model involves the development of a framework combining these dynamics and uses computational tools that leverage data analytics and machine learning. The technical result of the proposed work is the creation of a digital twin that enables effective design and robust control of highly sophisticated blades with adaptive TAD.

CAREER: Toward RIPEST Photopolymer Additive Manufacturing (PAM): A Cyber-Physical System of Novel Dual-wavelength Photoinhibition aided PAM
Photopolymer additive manufacturing (PAM) uses light to cure photosensitive materials and is one of the most versatile technologies to fabricate components with higher resolution compared to existing polymer manufacturing technologies. However, the state-of-the-art photopolymer additive manufacturing technologies still suffer an intractable issue of overcuring, which is detrimental to the structural integrity of printed parts. This Faculty Early Career Development (CAREER) grant supports research to elucidate the fundamental science of light propagation and photochemistry reactions for establishing a smart dual-wavelength photoinhibition aided PAM process. This new process allows to manipulate cure and curb exposures of the PAM processing so that the geometrical fidelity and functional integrity can be enhanced. If successful, this project will impact numerous applications including biochips, electrodes, soft robots, metamaterials, and others that demand precision manufacturing of parts with complex shapes and strengthened performance. This project will provide educational activities aimed to train future leaders to pursue manufacturing-related careers and improve the diversity of the STEM workforce by reaching out to historically underrepresented communities.

The overarching goal of this CAREER project is to realize Rapid, Intelligent, Precise, Extensive, Sustainable, and Transformative (RIPEST) PAM. The primary research objective is to establish a novel digital light processing method - DLP2Curb that capitalizes on photoinhibition induced by a second wavelength light to curb curing parts, via a holistic cyber-physical approach that combines physics-guided surrogate modeling with real-time sensing and control. Thrust 1 elaborates the DLP2Curb process dynamics and materials behavior through multiple physical regimes, temporal stages, and spatial scales to unravel the causes of overcuring and the curb reaction paths by constructing a digital twin. Thrust 2 aims to achieve non-contact, full-field operando characterization of PAM in real time. In-process part properties will be measured by deploying in-situ interferometric and ultrasonic monitoring systems along with physical sensor models and data analytics methods. Thrust 3 develops a real-time control method via deep reinforcement learning of model predictions and in-situ measurements feedback. With case studies such as micropillars and lattices, this project will demonstrate the potential ability of DLP2Curb to unlock efficient precision manufacturing of sophisticated parts with exact geometry and exquisite details.

Conference: NSF Industry Partnership Summit: A Workshop Proposal
The CHIPS and Science Act, signed into law on August 9, 2022, envisions new opportunities in accelerating progress in key technology areas, translating science and technology into solutions to society?s major challenges, and fostering partnerships with the private sector to enable this progress. This vision for the future encourages federal agencies such as NSF to take bold steps and experiment with new approaches for creating an ecosystem of partnerships and collaborations for use-inspired and translational research.

This workshop will convene a strategic set of corporate science and technology leaders, as well as NSF leadership representatives, to identify effective strategies as well as nontraditional approaches to enhance the next generation of public-private partnership models. Through co-investing in research, companies and federal agencies such as NSF can complement traditional R&D investments, leverage and expand available funding to achieve research goals, broaden engagement with new research partners, and fill the workforce pipeline. The workshop will take place in Spring 2023 in the Washington, DC, region and will be followed by a virtual event in June 2023 to broadly disseminate findings.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Innovative Partnerships for the Future: NSF Industry Partnership Summit
Event Report Executive Summary
Companies and federal agencies have historically worked independently to sponsor academic research in the United States while other nations have leveraged funding models that include partnerships among other interested parties. As the private and public sectors seek to address contemporary global challenges requiring science and technology solutions, these cross-sector, joint research collaborations can be leveraged to identify solutions to improve the human condition.
The National Science Foundation (NSF) has been at the forefront of piloting new approaches to multi-sector funding models. To date, each direct, jointly funded NSF partnership has been conceived, developed, and negotiated individually, and the process and timeline from idea to release of a solicitation are long and complex.
NSF and potential partners recognize the need to optimize the process of prioritizing timely and pertinent topics, identifying potential partners, and streamlining collaboration modalities to increase efficiency and maximize impact. In April 2023, UIDP (uidp.org), a nonprofit membership organization comprising global companies and universities that pursue research together, worked with NSF’s Technology, Innovation and Partnerships Directorate to convene 63 science and technology leaders, representing 49 leading companies, to engage in cross-sector discussions about best practices for public-private partnerships. Representatives from NSF’s directorates provided information on research areas of interest and facilitated discussion about partnership challenges and opportunities, key technology focus areas ripe for co-funding, and workforce development and training opportunities for growth.
There were three key objectives for the NSF Industry Partnership Summit:
1.        I.            Strategy Development. Shape effective strategies for partnering across research, workforce development, and infrastructure—explore opportunities to expand public-private partnerships and characteristics of new funding modalities.
2.       II.            Sector Interest. Identify several “coalitions of the willing,” or companies in a sector highly interested in pursuing deeper discussions on partnership opportunities in research or workforce development.
3.      III.            Greater Engagement. Expand industry awareness of the opportunities for public-private partnerships to accelerate research and workforce development.
Key Findings
Increase industry participation in NSF efforts. Expand the number and size of companies that co-invest with NSF in funding opportunities. Small to medium-sized companies and startups offer valuable use cases, facilities, and technical capabilities, but lack resources internally to support ongoing engagement, such as project managers and partnership officers. An NSF resource to assist with coaching and process simplification could make broader participation possible.
Develop new IP approaches. Funded research can lead to new discoveries, and the intellectual property (IP) emanating from research can create opportunities for potential development if the pathway to commercialization is transparent, rational, and not needlessly complex. New models are needed for companies to partner at different investment levels, including new IP management approaches. Models are also needed that recognize tangible, non-monetary contributions such as facilities, use cases, and other in-kind assets as co-investment options. Selecting from a menu of approaches and options from the start would simplify decision making for companies that desire collaboration.
Move faster. There is a need to reduce the timescale for NSF engagement (from idea to solicitation) and to reduce the paperwork burden so industry can more easily collaborate with NSF. One avenue is co-funding pilot programs with the option to scale and increase investment at the three-to-six-month mark.
Improve communication. Regular, industry-facing communication from NSF can inform companies about opportunities to meaningfully engage in research activities. There is also a need for an NSF web presence specifically for industry partners (and potential collaborators).
Facilitate greater connectivity. Regularly established meetings can bring industry representatives together to share challenges and identify solutions, segmented by technology interest area. Meeting with NSF program managers that align with research interest areas could engage industry representatives to help inform and shape future funding opportunities and build informed technology roadmaps.
Strengthen experiential learning opportunities. Connect NSF-supported researchers and Ph.D. students with experiential opportunities in industry. Upskilling through non-degree coursework, certifications, and practicum experience offers additional ways to expand the talent pool.
Fund work closer to industry research interests. To make the transition to translational research, support projects at the technology readiness level of three or higher.
Create onramps for new partners. Allow additional companies to join projects after initial funding rounds, perhaps at lower funding levels if offering in-kind value (use cases, test beds, etc.).
Advance soft-skill capabilities. Design thinking, communication, and multidisciplinary team dynamics should be emphasized alongside technical skills in both academic and experiential learning.
Raise researcher awareness. Support engagement with academic researchers so they understand the value of use-inspired and translational research and how it relates to economic growth.
Develop support tools. A co-investment model involving industry and NSF could be leveraged to develop tools for use across an industry sector, e.g., data standardization and platforms, digital twin capabilities, etc.
Cultivate multi-agency work. Engage in multi-agency, multinational funding for technology development in grand challenges to pool more resources and accelerate progress.

STTR Phase I: Registration of Below-Canopy, Above-Canopy, and Satellite Sensor Streams for Forest Inventories

The broader/commercial impact of this Small Business Technology Transfer (STTR) Phase I project is to increase the volume and improve the accuracy of data on the world?s forests. Presently, when collecting data on forests, surveyors must choose between slow, laborious methods, or quick but inaccurate ones. This project uses recent advances in sensors and machine learning to greatly improve data collection speed without sacrificing accuracy. The resulting rich datasets enable the construction of true ?digital twins? of forests and open the door for higher fidelity modeling of forest growth trajectories. This information is useful both for timber firms seeking to maximize the potential of their assets and environmental groups projecting how changes today could impact a forest?s performance as a carbon-sink over the long term. The impacts on United States citizens are widespread. Here are two examples: improved efficiency in the timber industry brings down the cost and improves the quality of raw materials and turning forests into denser carbon sinks helps meet climate change goals. The availability of such broad and deep data on forests could also drive a boom in research and understanding about the more complex and nuanced relationships that drive forest health and productivity, launching entirely new sub-industries around forestry.

The key technological innovations explored in this STTR Phase I project are in constructing the most high-fidelity forest model (digital twin) by combining disparate information sources, each with their own advantages and disadvantages. Light detecting and ranging (LiDAR) and camera sensors on backpacks provide high-quality inventory metrics nearly 1000 times faster than manual measurements, but still require someone in the forest to wear the backpack. Satellite imagery scales almost instantly to entire forests and also through time with historical data but is limited by the top-down nature of satellites and the resolution they offer, especially when historical and free data sources are considered. Drone-based imagery sits in-between, with advantages and disadvantages of both. In practice, combining information sources that measure in such different ways can be very difficult. In this project, the team explores how to express LiDAR-based metrics to best associate them with top-down imagery from satellites and drones. From these associations, one can then build powerful machine learning models and specialize them to individual forests. This ability may enable the company to provide forest inventories and forest management recommendations to timber companies at any scale: with satellite imagery only or with a combination of backpack-LiDAR and satellite for the highest accuracy over the entire forest.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Project outcomes report


Under the grant “Phase I, Registration of Below-Canopy, Above-Canopy, and Satellite Sensor Streams for Forest Inventories”, Gaia AI partnered with researchers at Carnegie Mellon University (CMU) to investigate three different kinds of data for forest inventories. A forest inventory includes measuring heights and diameters of subsets of trees to make forest management decisions such as thinning or timber price estimations. 


Gaia AI uses lidar technology, camera, and GPS all integrated within a convenient backpack to take these measurements and make a map of the forest. By allowing foresters to measure trees up to 1000 times faster than traditional methods, which involve hugging trees with tape measures, Gaia AI enables foresters to do their job faster and more efficiently. This grant funded R&D of the backpack hardware and the robotics code to make these inventories more accurate and scalable across different forest types in the United States. 


Researchers at CMU have experience with flying drones in forest environments. Under this grant, they developed and tested robotics mapping software to make three-dimensional models of the forest canopy, by flying drones high above the forest. They also compared the quality of these drone images with images from much higher above the forest, such as from planes or satellites. They found that image quality matters much more than image resolution if you want to identify individual trees.


Finally, with the combination of the backpack on the ground and satellite images far above, Gaia AI researched artificial intelligence models to learn about how the backpack and satellite information complement each other.

NSF Engines Development Award: Advancing space technologies (CO)

This NSF Regional Innovation Engines Development Award is focused on igniting technological innovation and workforce development to foster technology commercialization and technology transfer in space systems, space infrastructure, and space cybersecurity for the space economy. Based in Colorado Springs, this development award for the Resilient Space Infrastructures, Systems, and Economy (RISE) project has core partners in the Colorado Springs Chamber & EDC, Exponential Impact, National Cybersecurity Center, Pikes Peak State College, Space Foundation, Space Information Sharing and Analysis Center (ISAC), U.S. Air Force Academy (USAFA), and University of Colorado, Colorado Springs. Spanning private, public, and nonprofit sectors, each partner organization brings unique capabilities to inform and grow the vast space ecosystem in Southern Colorado. Through this development award, relationships among core partners will be formalized, and anticipated partner growth will expand beyond the Pikes Peak region further into Southern Colorado to include Pueblo, Trinidad, and Cañon City and possibly into surrounding states.

The RISE project will be propelled by and organized across three key pillars: 1) Technological Innovation, 2) Commercialization Acceleration, and 3) Community, Policy, and Workforce Building. The Technological Innovation pillar will act as the research and development arm of RISE. During the Type-1 period, this pillar will organize two workshops to leverage nationwide expertise to develop further specific goals of the pillar, which include the creation of a full stack of space resilience capabilities, creating, maintaining, and running a digital twin of space infrastructures and systems; space resilience standardization and management; and space resilience certification. The Commercialization Acceleration pillar will focus development activities on market research, exploring regional gaps and industry needs, and testing potential innovation pathways to support the development of a strategic plan and building blocks for the type-2 proposal. To ensure the scope and strategic plan are aligned with the market needs and regional culture, the pillar will run emerging tech workshops, focus groups, and human capital support projects. The Workforce Development pillar will leverage existing ties to the local, state, and national policymakers to develop a framework of existing policies and advance channels to create influence on policy decisions in later phases. Community and policy work through this pillar seeks to directly benefit the research and startups emerging from the engine and support existing space companies in the region. With a leadership team comprising representatives from each pillar, development activities will be synchronized across pillars leading to the development of a full NSF Engines proposal.

ERI: Hydraulic Cylinder Diagnostics Using Nonlinear Inverse Model Estimation and Frequency Domain Analysis
This Engineering Research Initiation (ERI) grant will fund research that enables reliable operation and cost-effective maintenance of machinery in which hydraulic cylinder actuators play a central role?of critical importance to the transportation, manufacturing, construction, and agricultural industries?thereby promoting the progress of science and advancing the national prosperity. Given its high power density and large force capacity, the hydraulic cylinder is a trusted workhorse across many industrial applications. By its design, it is also vulnerable to a variety of critical faults, such as seal failures, leakage, fluid contamination, and reduced load-carrying capacity, that may negatively affect performance and efficiency, and result in high costs from downtime, permanent damage, or operator injury. To ensure that such faults can be accurately detected and identified, this project relies on a combination of physics-based modeling and innovative frequency domain analysis to develop a novel diagnostic methodology that accounts for system nonlinearities and closed-loop operation. Such a diagnostic paradigm is also anticipated to find application in a variety of energy and power transmission systems, including electric vehicles. Efforts to disseminate research outcomes to local industry in Southeast Michigan, undergraduate research opportunities, and integration of modeling and systems diagnostics principles in coursework will produce broader societal impact.

This research aims to develop the foundations for a comprehensive systems diagnostics methodology for hydraulic cylinders that relies on physics-based modeling rather than statistics and machine learning techniques, as has been common in recent years. Such foundational contributions will be made through the design of a new residual generator that extracts faulty features from sensor measurements, as well as of a residual evaluator that makes a diagnostic decision by comparing the residual with the prescribed threshold under a certain operating condition. Several tasks will be pursued through a combination of theoretical modeling, numerical simulations, and physical experiments, including comprehensive fault analysis using a nonlinear, inverse, frequency domain representation of the system dynamics, and formulation and testing of a parameter estimation algorithm for such a nonlinear inverse model under closed-loop operation using an adaptive digital twin. Hardware-in-the-loop simulations will be performed to validate the methodology and investigate its robustness also in the presence of multiple simultaneous faults.

Workshop on Atmospheric and Urban Digital Twins (AUDT); Austin, Texas
This award is for a series of workshop activities that would result in the development of a white paper on the potential applications of combining atmospheric and urban digital twins in urban environments. Digital twins are virtual representations of physical objects, processes, and systems and have gained widespread use in the engineering community, and are currently being adopted in the atmospheric sciences community. The impact of the workshop series would be to engage the academic research community in the use of digital twins for enhancing understanding of interactions between natural, physical, cyber, and social systems within cities.

The leadership team will conduct a series of focused group meetings, invited talks, virtual meetings, and a session at a major conference over the course of a year that will result in the development of a white paper on the potential applications of combining atmospheric and urban digital twins in urban environments. Atmospheric digital twins (ADTs) are digital clones of the Earth?s atmosphere while Urban digital twins (UDTs) are digital replicas of urban spaces. The integration of ADTs and UDTs into an atmospheric and urban digital twin (AUDT) would result in real-time virtual representations of physical, environmental, and social systems by providing 3D (three-dimensional) visualization, AR (augmented reality), virtual city models, and prediction capabilities. The objectives of the workshop are to develop clear definitions and requirements of AUDT, develop exemplary AUDT demonstrators, and support in channeling the different digital twin developments for weather and climate studies.

CAREER: Digital Twins of Surgical Environments for Situational Awareness and Immersive Simulation

Developing new tools and procedures for robot-assisted surgery is difficult and expensive. This is because of the need for special-purpose tools and environments for building and testing prototype surgical robots, even before clinical testing can begin. This project seeks to address these problems through developing ?digital twins? of image-guided surgery. The high level idea is to develop virtual reality simulations and computational models that closely mimic real patients, surgical tools, and operating environments. These simulations will help designers correct possible problems with robot and procedure designs early on; this in turn will allow faster, cheaper design cycles that will make robot-assisted surgery design more practical and widely available. The simulations can also be used to support real operations; room-scale models of the operating environment will support algorithms to monitor the robotic systems for safer surgeries, as well as provide new opportunities for educating both surgeons and the general public about ways technology is used to advance surgery.

The technical aims of this project are divided into two thrusts: In the first thrust, the team will develop innovative methods that enable the creation of virtual immersive and interactive simulation environments that precisely recreate scenarios of the real world. To build these so-called digital twins, the team will create computer vision and machine learning techniques to create, update, and leverage digital twins of the surgical environment from multi-modal measurements. The second thrust will use digital twin models of unstructured, human-in-the-loop environments to explore how these immersive simulation models may be used to accelerate the design of intelligent surgical systems and provide complementary situational awareness to enhance decision making.

EAGER: Advanced Digital Twin Capability for Turbulent Wind Fields in the NHERI Boundary Layer Wind Tunnel at the University of Florida
This EArly-concept Grant for Exploratory Research (EAGER) will establish, validate, and disseminate an advanced digital twin capability for the National Science Foundation (NSF)-supported Natural Hazards Engineering Research Infrastructure (NHERI) boundary layer wind tunnel (BLWT) at the University of Florida (UF). Wind tunnel testing remains the most common approach for assessing wind loads on structures and informing wind resistant design to reduce the cost of damage. However, wind tunnel experiments have limitations, such as the measurement resolution and the challenge of obtaining simultaneous records of wind velocity and pressure fields. Numerical simulations, such as Large Eddy Simulation (LES), offer an opportunity to fill in these gaps, but such simulation capabilities are currently not optimally leveraged by the research community. An important barrier is that current numerical modeling capabilities are mostly tailored to stationary, standard neutral wind profiles; in contrast, wind tunnels such as the BLWT at UF are increasingly implementing advanced capabilities to reproduce more complex turbulent wind fields that cause structural damage. This research project will establish numerical simulation capabilities for these complex wind fields. To maximize the potential impact of the project, validation test cases and a corresponding digital twin tool set and tutorial for the simulation capabilities will be defined through structured interviews with the current UF BLWT user base. The resulting digital twin capability will make it possible to jointly leverage numerical and experimental models to improve understanding of the turbulent wind loads that drive damage to buildings and civil infrastructure and to advance wind resilient design. Simulation data and documented source codes will be archived and made publicly available in the NHERI Data Depot (https://www.DesignSafe-ci.org). This EAGER will contribute to the NSF role in the National Windstorm Impact Reduction Program.

The specific goal of the research is to establish and disseminate a numerical modeling strategy for reproducing complex turbulent wind fields generated in the UF BLWT. For standard neutral log-law wind fields, inflow boundary conditions commonly employ artificial turbulence generation methods. Since the velocity statistics of artificial turbulence evolve within the computational domain, some form of calibration is required to ensure that the target wind field is correctly reproduced. This calibration challenge is exacerbated when the objective is to model more complex turbulent wind fields, such as the boundary layer with a pronounced roughness sublayer that can be produced in the UF BLWT, which is important for low-rise buildings, where the building is immersed in the roughness sublayer (the roughness height of the boundary layer is on the order of the building height). The first objective of this project is to explore computationally efficient and accurate methods for numerically reproducing these roughness sublayers. Different combinations of artificial turbulence inflow generators, upstream roughness resolving simulations, source term forcing methods, and machine learning approaches will be investigated and validated against experimental data. The second objective of this project is to support broad dissemination of the resulting turbulence generation method by co-designing a digital twin tool set and a tutorial with the BLWT user base. The digital wind tunnel can also help identify optimal measurement locations for physical testing and potentially support data infilling where there are limits on the spatial resolution of physical measurements.

Conference: IACM 2nd Mechanistic Machine Learning & Digital Engineering for Computational Science, Engineering & Technology (MMLDE-CSET); El Paso, Texas; 24-27 September 2023
This award is to provide travel and registration support to 20 high school, 15 undergraduate and 20 graduate students, 5 post-doctoral fellows, and 3 high school teachers to participate in-person or virtually in the 2nd Mechanistic Machine Learning & Digital Engineering for Computational Science, Engineering & Technology (MMLDE-CSET) Conference to be held in El Paso, Texas from 24-27 September 2023. The objective of MMLDE-CSET is to provide a platform for intradisciplinary collaboration between experts in engineering, basic sciences, mathematics, and data and computer science from academia, industry, and federal entities, who have interests in developing and applying machine learning and digital twins solutions to complex technological problems based on mechanistic methods and computational engineering and science paradigms. Additionally, the conference strives to facilitate the development of the next generation of engineers, scientists, and researchers, through education and dissemination, with a strong focus on underrepresent groups.

The objective of this conference pertinent to this grant is to enable and expose the workforce of tomorrow to the exponentially growing fields of machine learning and digitals twins in the context of mechanistic based computational science and engineering methods. There are several activities dedicated or geared towards students. Undergraduate students will have an opportunity to present their research in an exclusive education track composed of oral and poster sessions. Two short one-day courses targeted towards graduate, undergraduate, and high school students will be offered on digital engineering, mechanistic data science, and basic application of ML as applied to STEM fields. Three lunchtime education panels on curriculum, teaching and learning approaches, and funding, respectively, will be held for general audiences. Besides these carefully designed activities, students will be able to attend plenary talks and general technical sessions and interact with industry in specifically themed sessions.

This project is supported by the Advanced Manufacturing (AM) Program, the Biomechanics and Mechanobiology (BMMB) Program, the Engineering for Civil Infrastructure (ECI) Program, and the Mechanics of Materials and Structures (MoMS) Program of the Civil, Mechanical and Manufacturing Innovation (CMMI) Division.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
The MMLDT-CSET 2023 Conference was organized to advance the primary technical themes of the event: 1) mechanistic machine learning, data-driven physics, and related methodologies, and 2) digital twins and their applications in digital engineering. The goal is to engage and prepare the next generation of the technical workforce, who will play a significant role in this evolving scientific ecosystem. This three-day conference includes technical tracks on emerging MML and digital twin research, special public lectures, a short course for high school teachers, and demonstrations of data science applications in STEM fields. The conference is in a hybrid format, featuring both on-site and virtual sessions. A total of 217 attendees participated in this conference, as shown in Table 2. Among them, 98 (45%) are students, of which 60 were sponsored by the NSF fellowships. The NSF Fellowship was used to promote awareness of mechanistic machine learning and digital twins and knowledge dissemination to all levels in education, including high school, undergraduate, and graduate levels. And 70% of the fellowship was reserved for underrepresented groups to promote equity, diversity, and inclusiveness. The NSF fellowship fund was used to cover the conference and short course registration fees. A total of 114 fellowship applications were received, and 60 of them were selected to receive the NSF Fellowship support as shown in Table 3. About 35% of the Fellowship awardees are female students, while 45% of the awarded undergraduate and graduate students are first generation college students, and the 15 high school students are Hispanic students from El Paso, TX.

SBIR Phase II: Building the digital twin of radiology operations
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to improve the utilization of medical imaging equipment and potentially increase access to medical imaging for the population. Today, medical imaging facilities operate expensive equipment but lack access to operational data and modern tools to monitor and use them more efficiently. The company is building a digital twin of radiology operations to continuously capture imaging operations, monitor them, suggest optimizations, and optimally schedule patients. Beyond reporting and scheduling capabilities, the models will allow the prediction of interventions in software for the evaluation and comparison of different scenarios in-silico without real-world experimentation. The more efficient use of scanners is expected, in turn, to potentially benefit the patient population as it will reduce the wait time for imaging, increase patient access, shorten imaging protocols, reduce sedation duration, reduce and predict delays, and ultimately improve the patient experience. The data unified in the digital twin will also open new avenues of research for radiologists and researchers.

The proposed project aims at developing and testing key technological innovations underpinning our digital twin vision. The company will develop a generic architecture to harmonize data across many sources, including from the scheduling system and the scanners themselves. The company will develop and test new artificial intelligence (AI) techniques to augment the data and unlock essential descriptors to manage operations. This solution will include AI to passively learn imaging protocols from the patients? exams and automatically detect protocol deviations. The company will also use AI to automatically characterize the content of images and enable them to be queried. The company will evaluate federated learning techniques to allow learning ?at the edge? on large datasets at scale, without sharing the data, alleviating data privacy challenges. The company will incorporate advances into a smart recommendation engine that continuously mines customer data to identify opportunities for improvement and proposes interventions. Finally, the company will develop and test key building blocks for implementing a smart scheduling assistant that uses retrospective data and digital simulations to optimally schedule exams while maximizing equipment utilization.

SBIR Phase I: Proximate Wind Forecasts: A New Machine Learning Approach to Increasing Wind Energy Production

The broader/commercial impact of this Small Business Innovation Research (SBIR) Phase I project will be to demonstrate the potential to increase (by 2%) wind-energy production from existing wind farms at very low cost. Combining networked, air-pressure sensors distributed on the landscape with artificial intelligence/machine learning (AI/ML), the technology will empower wind farm operators with advance alerts of oncoming winds and gusts to preemptively adjust settings like blade pitch and turbine yaw. These adjustments will result in more wind energy production and less turbine damage. This technology will significantly increase energy revenues and decrease costs. In 2022, US wind farms produced 380 terawatt hours (TWh) of energy. If serving just half of existing plants, this technology could yield an additional 3.8 TWh of renewable energy and over $150 million to US wind energy sales annually. In the competitive wind industry, these revenues can greatly increase operating margins and help accelerate the growth of the industry and clean energy jobs. Using government emissions figures, this deployment would also avert 2.4 gigatons of carbon dioxide (GTCO2) over 20 years. This wind alert technology could also benefit solar tracker safety and increase safety at aerial vehicle ports and lift-crane operations.

This Small Business Innovation Research (SBIR) Phase I project will show how wind can be measured and predicted 10?600 seconds in the future by combining a new sensor modality ? distributed pressure sensors ? with new machine learning (ML) models. Pressure sensors are far cheaper than wind sensors (e.g., Doppler LIDAR), but processing data from pressure sensors into predictions of the wind is complex. It is impossible to hand-code statistical models to predict turbine-height wind from ground-level pressure measurements. Instead, one may rely on learned ML models to make these predictions. Previous studies have used ML to model weather on regional or global scales, but this project is the first to create models for the much smaller and more demanding scales applicable to wind farm operation and to optimize for metrics important to wind farm operators. Because ML models have not yet been developed directly for combined pressure and wind data at this spatial and temporal scale, this project will combine advances in attention-based models (like Transformers) with advances in models that respect physical priors (like Hamiltonian Neural Networks) and will lead to a new form of sensing which will be far more accurate than was previously possible at this price point.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
Introduction 
Wind turbines play a vital role in clean energy production in the U.S., contributing 10.3% of total utility-scale generation in 2022. Enhancing their operations can bring significant benefits to the clean energy sector by reducing costs, mitigating climate change, and decreasing reliance on foreign fossil fuels. This report details a Windscape AI research project aimed at improving wind turbine operations through advanced sensing and modeling of wind and turbine interactions. The report focuses on two main research areas:
1.Wind Modeling Using Barometric Pressure Sensors: This section examines the use of pressure sensors deployed across wind farms to predict wind changes within the next 60 seconds. This predictive capability allows turbines to adjust yaw and blade pitch, enhancing efficiency and reducing component stress.
2.Wind Modeling Using SCADA Data for Yaw Calibration and Wake Steering: Building on the first section's findings and customer needs, this section explores machine learning models that use data directly from wind turbines to optimize performance.
Section 1: Wind Modeling Using Barometric Pressure Sensors
Theoretical Foundation
In short, wind is caused by pressure changes and wind causes pressure changes as it moves across the landscape. Pressure sensors even near the ground have the promise to provide observability of air movements at turbine height.
Sensor Deployment and Data Collection
Non-NSF funding supported the deployment of pressure sensors around a wind farm in Texas. Data collection involved:
Sensors placed at 20 locations within the wind farm.
A met tower at the center of the site capturing wind speed and direction data at turbine height.
Collection of data from both pressure sensors and the met tower over several measurement campaigns.
Creation of separate datasets for training and evaluation to ensure accurate model evaluation.
Machine Learning Modeling Setup
The machine learning models were set up to predict wind speed based on past wind data and pressure readings. The process involved:
Selecting 80-second windows of wind speed data, with the first 40 seconds representing the past and the remaining 40 seconds for prediction.
Aligning pressure data with wind data, focusing on pressure readings corresponding to the first 40 seconds of wind data.
Training various model types, including linear models, recurrent neural networks (RNNs), multilayer perceptrons (MLPs), and a Transformer-based model. These models aimed to predict future wind speed using past wind data, pressure data, or both.
Machine Learning Modeling Results
The results are presented first by time horizon (showing prediction errors for different future timeframes) and second aggregated by averaging errors across all 40 seconds of future predictions. Key findings include:
Pressure data alone had limited value in predicting future wind speed, with linear models performing poorly and neural networks offering only slight improvements.
Pressure data revealed general wind conditions but lacked visibility into specific near-term gust events.
Models using wind data (either linear models or neural networks) significantly outperformed those relying solely on pressure data.
Combining pressure data with wind data improved predictions, particularly for longer timeframes (10-40 seconds into the future).
Section 2: Wind Modeling Using SCADA Data for Yaw Calibration and Wake Steering
A major wind industry challenge is yaw misalignment, which occurs when a turbine's yaw (vertical axis rotation) is not set optimally, which leads to reduced energy production and increased wear.
The Neural Digital Twin Approach
Windscape AI developed a "Neural Digital Twin" ML model to address yaw misalignment and other optimization.
Data-Driven Recalibration Recommendation
This model leverages data from wind turbines' SCADA systems. The model simulates past weather conditions and compares simulated energy production with observed data. It then simulates production under various hypothetical recalibration angles to identify the angle associated with the highest total energy production.
Future Directions
The Neural Digital Twin approach shows promise for further wind farm optimization. Potential advancements include incorporating additional factors like general turbulence conditions and seasonality and enabling wake steering to minimize negative effects on downstream turbines, boosting overall farm production.
Conclusion
Pressure sensors offer some benefits for wind prediction, and the Neural Digital Twin approach using SCADA data proved effective for correcting yaw calibration. The approach holds further promise for wake steering.

Conference: Mathematical Opportunities in Digital Twins (MATH-DT)
Recent advances in physics-based modeling, data-science, sensor technology, and computational mathematics have made it feasible in many areas to produce a 'Digital Twin' of a complex real world system. Such twins have shown significant promise to better understand, monitor, predict, and control real-world systems, particularly in cases where not every aspect about the system can be observed or modeled. This can improve safety, comfort, maintenance and, also the health and well-being of humans. However, many fundamental questions and challenges remain, particularly regarding a rigorous (mathematical) foundation for this emerging field. This award provides support for a workshop titled ?Mathematical Opportunities in Digital Twins? to be held on December 11-13, 2023, in the George Mason University's campus in Arlington, VA. The workshop brings together key experts working in many aspects of mathematics, key application fields, and industry with the goal to determine the ways in which mathematics can contribute to the research on Digital Twins and how Digital Twins can open up new mathematical directions, as well as to identify connections, synergies, and organizational efforts within the mathematical community, and to/with other disciplines. Digital Twins can lead to new developments in many applications, such as: engineering by e.g., determining weaknesses in structures such as bridges, nuclear plants, or wind turbines; medicine, where Digital Twins of organs may lead to better cures and understanding; society, where Digital Twins of large-scale events like sport games can improve safety. A broad impact of the conference is facilitated by the conference website featuring videos and slides of talks and a technical report that will be shared with the entire scientific community. Students and early career researchers are invited to the workshop, with special attention given to groups traditionally underrepresented in STEM.

Mathematical models and computations have always played a significant role in simulating, understanding, and predicting physical phenomena. While traditionally, many models have been based on first principles via a rigorous mathematical foundation, such approaches face limitations: Not everything in a physical system can be captured using physical principles, and the available computing resources and algorithms may be unable to model an entire complex system, particularly in real-time environments. Significant advances in sensing technology have enabled the equipment of complex real-world systems with sensors and to employ the sensor data to inform the workings of the system. Moreover, recent developments in data science and machine learning have strengthened the interest and confidence in empirical methodologies. However, purely empirical approaches do not take advantage of the physical principles and may require measurements and data generation at a cost that is not feasible. This workshop focuses on 'Digital Twins', which aim to combine physics-based models with data-driven models, with the goal to leverage the best of both worlds. Digital Twins bring together several research areas in mathematics (including: modeling, analysis, control, optimization, numerical analysis, and scientific computing). This workshop is expected to stimulate new developments in these important areas and to initiate new collaborations, and strengthen the existing ones, among the researchers with diverse background on mathematics of Digital Twins.

Collaborative Research: Digital Twin Predictive Reliability Modeling of Solid-State Transformers
Solid state transformer (SST) is deemed as a revolutionary technology for future power systems. It is much more compact than the conventional electromagnetic transformer, with significant advantages of controllability both in power flow control and power quality regulation. However, one major technical barrier that constrains the practicality of SST is the low reliability compared to the conventional transformers. This is due to the large device count including semiconductor transistors, auxiliary circuits, and passive components. Currently, the reliability of SST has received little attention, which constrains their commercialization and adoption by industry. This project will develop data-driven digital twin models for SSTs that will facilitate prediction of component degradation and prevention of catastrophic failures. This is aimed to significantly improve the reliability of SSTs for safety-critical applications, such as future power systems and electrified transportation applications. The proposed modeling and design methods will result in new classes of power electronics design tools and will enable a fully integrated design process that will generate new topologies and save substantial design and implementation time. Further, these approaches will enhance reliability modeling where reliability can be accurately estimated from at design stage even for newly synthesized architectures. Regarding educational impact, this work presents an opportunity to apply artificial intelligence to power electronics engineering. Hence, the outcome of the project will upgrade power electronics teaching curricula and provide students with an effective skillset for future power engineering.

To address the challenge of reliability of SSTs, this project will develop a comprehensive systematic framework of online health monitoring for SSTs to significantly improve the reliability in the face of electric faults. The proposed health monitoring framework will include online prognosis and diagnosis of potential electrical faults that SSTs could be subject to, targeting common semiconductor switching faults and health degradation in high-frequency transformers. Specifically, a portfolio of critical SST parameters will be monitored through a smart gate driver that will be integrated with the power electronic building blocks, so degradation in the semiconductor modules can be predicted and diagnosed during the fault inception stage. A novel data-driven digital twin approach is proposed to predict the behavior of the SST converter modules and it will compute health performance indices to make the technique more computationally efficient compared to full physical model computations. Fast online diagnostic algorithm will be developed and embedded in the SST microcontroller, so a fault can be identified and characterized, to minimize downtime cost and avoid cascading failures.

Conference: US-UK Workshop on Transformation in Urban Underground Infrastructure; 28-29 September 2023
This award supports a workshop that will bring together leading experts from diverse, but complementary backgrounds in civil engineering, urban planning, disaster management, computer science, operations engineering, public policy, sensing, energy and other fields to identify gaps and opportunities for advancement and define a vision for supporting the transformation of underground urban infrastructure systems. Urban cities across the world have been a major driver of economic growth, technological innovation, and cultural vitality. However, their infrastructure systems are often patchworks of legacy and new components with incompatible standards, materials, and governance structures. As a result, the performance of such systems has been inherently unpredictable under normal conditions and more so when subject to extreme events. Without a paradigm shift in how infrastructure systems are engineered, constructed, and operated, the gap between the service they can deliver and the demand from citizens will continue to widen in the foreseeable future. Within urban settings, the challenges faced by underground infrastructures, such as water and wastewater, transportation, telecommunication, and power, are exacerbated due to difficulties in access and the harsh environment in which they reside. Early success in building digital models at city scales through smart city and digital twin concepts offers a promising direction that is now newly enabled by breakthroughs in sensing and computation. However, key knowledge gaps remain a barrier to wholesale changes.

The workshop will catalyze creative thinking and knowledge exchange with the purpose of devising a vision for transformation in underground infrastructure. It will expose relevant research areas, knowledge gaps, research opportunities, barriers, and fundamental research needs. It will aid in identifying fundamental theories and methods needed to answer fundamental, open questions, such as, ?How do we plan for and achieve decarbonization goals for and by way of underground infrastructure systems?? and ?How do we optimize day-to-day operations of underground infrastructures under normal, changing normal, and disaster conditions?? Through a joint workshop with the United Kingdom, this award will create an international research roadmap. It will also generate networking opportunities across nations, disciplines, and between academia and industry.

SBIR Phase I: Mission Planning Methods and Simulated Crisis Management Framework to teach STEM to Underserved Youth
The broader/ commercial impact of this Small Business Innovation Research (SBIR) Phase I project is in addressing the unemployment of youth, primarily in underserved communities, across the United States by providing an online, collaborative gaming environment to educate, train, and provide Science, Technology, Engineering and Mathematics (STEM)-based career opportunities. It is estimated that more than 5.5 million youth in the U.S. ages 16 to 24 years, are out-of-school and out-of- work. The unemployment rate in this age group is close to 20 percent. This project aims to improve young people?s skills and overall knowledge, and help them secure certifications in drone design, function, and operations related to industry and first responder communities. Implicit to the core learning are knowledge in areas such as critical thinking and planning, rapid prototyping, mission planning, data post-processing and analytics, and co-authoring of effective robotics designs to assist in industry and first responder areas of operation. The project empowers students to develop strong STEM skills that target career opportunities in advanced manufacturing, inspire budding entrepreneurs, and directly benefit industry and first responder communities with a relevantly trained workforce.


This Small Business Innovation Research Phase I project will develop a scalable template of a realistic environment for a multi-player game for education and training of youth, integrating autonomous robots using scenarios driven by industry and first responders. This technology will be accomplished through a collaborative mission readiness workflow application with a multi-player gaming engine, Computer Aided Design (CAD)-generated prototype drones, and geo-accurate areas of terrain for realistic and relevant environments. Intelligent improvements of the system will be accelerated as Machine Learning (ML) and Artificial Intelligence (AI) algorithms are added to the workflow application and gaming environment from data that is collected and analyzed from training, exercise, and lessons learned from live response events. The greatest technical obstacle is modeling the data in a way that preserves integrity and that can be adapted and ?trained? for machine learning. Teams of youth, first responders, and engineers in this e-sports gaming league will compete for the best time, procedures, and systems to win each mission. Drone design, physics reality, and machine learning algorithms for each vehicle will be outputs for the drone blueprints that will be actualized via 3D printing.

This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.
PHASE I PROJECT OUTCOMES REPORT
The United States is lagging in advanced manufacturing, as evidenced by the growing deficit in advanced technology products. United States Advanced Manufacturing Growth requires a dynamically skilled engineering workforce. With a projected shortage of 186,000 engineers by 2031, there is a high demand for professionals in machine learning, automation, and robotics. However, the US lacks enough students graduating with the necessary skills, primarily due to consistent career pathway education that largely begins with inadequate high school physics education… A foundational for engineering careers.
AALMV has designed a methodology to improve physics performance with the Drone Wolf Artificial Intelligence (DWA) portal prototype. The DWA prototype includes four AI-enhanced, 3D drone online interactive simulation labs that align with the standardized curriculum for high school physics. Students collaborate and compete to create the best robotics physics solutions for automating mission challenges. DWA leverages large language models (LLM) and ChatGPT to develop machine learning algorithms for virtual robotics automation. Students complete robotic simulated missions and compete in time-critical tasks to progress to more challenging levels. These tasks align with standardized learning outcomes, assessing and enhancing students' applied physics skills. Successful algorithms are downloaded to actual drones for real-world testing and competitions with first responders and advanced manufacturing engineers. This methodology, patented by AALMV's founders, evaluates performance against national and state standards, advancing physics knowledge equitably through intelligent robotics design in a safe digital twin environment.
Keywords: LLM-Powered Robotics, Physics Education, Engineering Education, Mission-Critical Gameplay, Algorithmic Robotics, Game AI Education
Phase I Topic Name: Mission Planning Methods and Simulated Crisis Management Framework to teach STEM to Underserved Youth
Subtopic: Learning and Cognition Technologies: Learning and Workforce Development (LC4)
INTELLECTUAL MERIT
This Small Business Innovation Research project addresses the national deficit in physics teachers and engaging learning resources that together hinders student engagement and learning. The research integrates Large Language Models and mission-critical, game-based learning to enhance physics engagement and comprehension using aerial robotics and automation simulations. The implementation of Pilot program competitions in 2-3 school districts across two states will assess student engagement and performance in physics. Anticipated outcomes include improved student engagement, enhanced understanding of complex concepts, and the application of machine learning algorithms to real-world drones, with feedback aligned with Next Generation Science Standards (NGSS).
BROADER IMPACTS
Market: The programmable robot industry is poised for disruption. The DWA platform aligns with U.S. education standards, including NGSS and Career Technical Education (CTE) requirements. By simplifying the teaching and assessment of complex subjects through an online platform, DWA can capture a significant share of the educational technology market, providing schools with affordable access to cutting-edge robotics and machine learning tools.
Educational: The DWA platform addresses challenges in physics education caused by teaching staff shortages, lack of student participation, and weak performance among students who do participate. DWA provides resources to engage students in physics and engineering, promoting collaboration and competition, while enhancing student learning outcomes.
Societal: By democratizing access to advanced robotics and AI education, DWA can bridge educational gaps and foster innovation. The platform's real-world testing capabilities allow students to apply theoretical knowledge practically, promoting problem-solving skills and creativity. This supports the development of a technologically proficient workforce essential for economic growth.

Bacterial-based Biosensor Digital Twin for Microbial Community Sensing (US-Ireland-R&D Partnership)
In the 21st century, the understanding of microbial ecology evolution is a critical challenge, especially as we are tackling new emerging threats to our health as well as the global environment. This collaborative US-Ireland R&D Partnership project aims to address this challenge by developing a system that can sense molecular communications from living bacteria in the natural environment, and feeding this information to its equivalent digital twin, in order to allow us to observe and predict their conversation and tap deep into their internal functionalities that control their behaviors. This will be realized through a detailed computer produced digital twin model of the bacteria?s intra and inter-cellular communication model, which is wirelessly connected to a novel integrated living cell biosensor with electrochemical nanosensors. The resulting impact of this system is a new monitoring tool to understand how bacteria evolve and transform their capabilities. This project will develop a cross-cultural Trans-Atlantic mentoring program, where undergraduates from the US and Ireland will be jointly mentored by a team of researchers with the aim of attracting women and underrepresented minorities to do multi-disciplinary breakthrough research. The project will also develop and deliver programming activities to middle-and high-school students as part of the outreach program.

In the 21st century, the understanding of microbial ecology evolution is an emerging challenge, especially as we are tackling new emerging threats to our health as well as the global environment. The aim of this US-Ireland R&D Partnership is to develop a novel system that senses communication molecules from living bacterial cells using a novel integrated engineered E.Coli biosensor with an electrochemical nanosensor and feeding this information to a digital twin that will enable prediction and observation of their complex communication patterns. The in silico digital twin will be constructed from a combination of 3D trans-omics data models with parameter inferencing techniques from wet lab experimental data integrated with biophysical computational models. This system will provide detailed information that correlates the intra and inter-cellular communications under a specific environmental condition. Solid state generator-collector electrochemical sensors will be designed to eliminate interference for deployment into the environment to increase the accuracy of information that is transmitted to the digital twin. The research in this project can lay the groundwork of an open platform for ?Bacterial Citizens on the Internet," where researchers can (i) develop new forms of bio-cyber environmental monitoring of ecological systems, (ii) provide collaborative initiatives for global bacterial tracking to understand how they evolve and spread globally, and (iii) design future bio-hybrid implantable devices for disease detection. A prototype demonstrator will be developed to show communication between wild-type bacteria to a computer, focusing on two case study scenarios, which are ?Bacterial conversation in chronic wounds? and ?Conversation enabling Horizontal Gene Transfer (HGT) in soil microbiome."
This project is jointly funded by the Biosensing Program and the Established Program to Stimulate Competitive Research (EPSCoR).

IUCRC Planning Grant University of Michigan: Center for Digital Twins for Consolidated Manufacturing Intelligence (DTCMI)
Digital Twins are purpose-driven software entities that collect run-time data from an operating manufacturing system, and bring that data together with models (representing both historical and theoretical behaviors) to create outputs and metrics that can be used to improve the overall performance of the manufacturing system. The development of Digital Twins requires significant subject matter expertise, where operators and engineers help determine the appropriate data to collect, modeling frameworks, and the most useful output metrics. This planning grant will build industry partnerships and develop a set of research directions in the area of Digital Twins for Consolidated Manufacturing Intelligence. Through a two-day workshop and multiple smaller conversations including the development of an industrial advisory board, industry partners will be engaged with university partners to develop a plan for a full IUCRC proposal. The resulting research directions have the potential to improve overall manufacturing operations, increasing productivity, leading to more profitability for manufacturers as well as lower costs and higher quality for consumers. The proposed center will engage both undergraduate and graduate students in the research projects, giving them experience in working on relevant industrial research projects.

The research that will be proposed in the full IUCRC will relate to the development of methodologies and solutions for Digital Twins in the manufacturing industries. Given the primary site at the University of Michigan, automotive manufacturers and tier suppliers will be engaged as partners. As the system-level challenges are quite common across industries (even with very different processes), semiconductor fabrication and chemical process companies will also be engaged. A common framework for Digital Twins will be used to extend and reuse manufacturing intelligence and provide practical solutions to support evolution from reactive to predictive, proactive and prescriptive solutions envisioned in smart manufacturing and industry 4.0/5.0. Contributions will be in defining research directions that allow the framework and its elements to be re-useable, interoperable, extensible, and maintainable, but also in defining the state-of-the-art and deriving a research and development roadmap for key Digital Twin enabled solutions such as predictive maintenance and virtual commissioning. The planning activities will make use of the SMART 4.0 testbed in the Ford Robotics Building at the University of Michigan, that includes both additive and subtractive manufacturing processes, mobile and fixed-base collaborative robots, and an integrated data and control platform.

Collaborative Research: Evaluating and parameterizing wind stress over ocean surface waves using integrated high-resolution imaging and numerical simulations
The small-scale dynamics above/below the ocean surface are crucial for wind-wave coupling and govern the air-sea exchanges of mass, momentum, heat, and energy. In particular, surface waves and the corresponding generation of turbulence, breaking waves, and bubbles profoundly affect the roughness of the surface and determine the wind stress at a given wind speed. Although many studies have investigated the impact of water waves on wind stress, the present parameterizations lag behind actual needs. In this project, an interdisciplinary team of investigators (with backgrounds in physical oceanography, air-sea interactions, and atmospheric sciences) will leverage recent advances in infrared imaging of waves and high-resolution modeling to develop a sea-state-dependent numerical algorithm that estimates the wind stress accurately for models that cannot explicitly resolve the air flow over waves. Such models are used for many research, engineering, and planning applications, including physical oceanography, meteorology, climatology, and coastal engineering, among others. The project is highly interdisciplinary and will give the participating students valuable experience interacting with researchers outside their core disciplines. It will also broaden participation in science through the recruitment of students from under-represented groups at the University of Texas at Dallas (UT Dallas) and Columbia University through established programs. The knowledge and data generated by this research will be incorporated into the educational programs of both institutions. For example, at UT Dallas, the PI will introduce new under- and graduate-level courses on Wave Dynamics and Air-Sea Interactions into the engineering curricula. Further, PIs will participate in the Skype a Scientist program that provides middle and high school students opportunities to talk about basic science-related topics. As part of this project, two graduate students will receive interdisciplinary training in turbulent air-sea interactions, and an early career scientist who leads the project will gain valuable experience working with two experienced scientists.

The wind stress at the ocean surface is a crucial parameter for ocean, atmosphere, and surface wave models. Although progress has been made in understanding how the air-sea fluxes are modified by different sea states, detailed investigations of the wind stress and drag coefficient above waves remain rare, and the specific processes governing wave-mediated transfers of momentum are not well understood. Most operational atmospheric models use a simple bulk parameterization based on the equivalent surface roughness. Using existing integrated laboratory measurements of surface stress (Co-PI Zappa) and high-fidelity digital twin simulations of turbulent flow above ocean waves (PI Yousefi and Co-PI Giometto), this collaborative research project is anticipated to lead to a sea-state-dependent parameterization of surface stress based on a dynamic reduced-order modeling. This integrated approach will allow the PIs to specifically (1) investigate the variability of wind stress and its partitioning (i.e., the skin friction and form drag) over ocean waves under a range of wind-wave regimes, (2) examine the scale-invariance and self-consistency arguments of the surface drag over water waves, and (3) develop a wall-layer model for LES of wind over ocean wavefield to investigate the air-sea fluxes in strongly forced conditions.

ASCENT: From sensors to multiscale digital twin to autonomous operation of resilient electric power grids
The modernization of power systems for clean energy by integrating multiple renewable resources is changing the dynamics of power grids at a fundamental level. There is a dire need to understand new phenomena and possible failure mechanisms to unlock the design of countermeasures so that operators can make electric grids more resilient. But the required degree of understanding must keep up with the pace of new technologies in generation and storage, sensing and communications, optimization and control, power electronics, machine learning, and data science. This NSF project aims to develop a unified framework towards this goal, starting from sensors to algorithms to real-time control. The project will bring transformative change by leveraging fundamental developments in control, power electronics, and machine learning, and fusing them with trusted power system models, significantly enhancing the ability to predict and control grid dynamics with a high share of renewable energy resources. Results will be verified by building a digital twin of a large-scale transmission grid. The intellectual merits of the project include a balanced solution between models of renewable-integrated power systems developed from first principles and those identified from data, and the convergence of advanced methods under development within otherwise disconnected research communities. The broader impacts of the project include addressing pressing research questions whose solution will enable the building blocks of a cleaner power grid. The project will also engage underrepresented groups in STEM.

A central problem hampering the pace at which one can integrate renewable energy sources into electric power grids is the insufficient understanding, at a systems level, of the dynamic interplay between existing assets and inverter-based resources (IBR) deployed at scale on a transmission grid of substantial size. This project will address this challenge by creating a unified modeling environment for bulk transmission grids that integrates data-driven yet analytical IBR models. The resulting framework lends itself seamlessly to a state-space form familiar to those working with dynamical systems. Thus, the proposed framework is inclusive beyond traditional disciplines in power systems modeling. The approach will be to leverage this inclusiveness by absorbing into a digital twin of a transmission grid the latest developments in tangential areas driving innovations in power systems.


PFI-RP: Fiber Optic Sensing System for Smart Infrastructure Monitoring
The broader impact/commercial potential of this Partnerships for Innovation - Research Partnerships (PFI-RP) project is to significantly enhance the capability in predicting and managing life expectancy of large civil infrastructure . The future infrastructure systems need smart and fast sensors to assess their performance against designed parameters and predictive models. The project will integrate smarter information gathering sensors that will focus on safety, security, lower life cycle costs, and post-disaster surveys needs in new and current infrastructure. The broader implication will be in realizing the 'smart city' approach for hazard prevention and future city management. This will be achieved by actively monitoring operational conditions using fiber optic sensing technology developed in this project. Training courses will be offered to develop future leaders in the area of infrastructure engineering. Students from underrepresented minority groups will participate in the research and work closely with the industrial partner organizations bringing together designers, engineers, operators and owners.

The proposed project makes a single low-cost fiber optic cable into thousands of strain gauges, thermocouples, or accelerometers. It utilizes a patented distributed fiber optic sensing system, which provides a very high density of data (every 2 cm) over a very long distance (5-10 km). The system competes with existing technologies primarily based on its low price, high data acquisition speed and delivery of processed engineering data. The project will produce mass-producible hardware and commercial quality user interface software by (i) developing digitization hardware for a low-cost, mass production system, (ii) integrating a cloud-based data process system for more efficient data acquisition and interpretation, and (iii) demonstrating online digital twin model for real-time engineering analysis. The ultimate goal is that this scalable distributed monitoring system is used for large infrastructure monitoring projects such as bridges, tunnels, foundations, dams, levees, deep wells, and surface or buried pipelines in partnership with an industrial leader and US Army. The intellectual merit of this technology, coupled with data from other sensing and data analytics tools, is to significantly enhance the capability in predicting and managing large infrastructure's life expectancy by actively monitoring operational processes.

Planning: Building a Digital Twin-Based Virtual Engineering Laboratory for Students with Disabilities
With support from the Improving Undergraduate STEM Education: Hispanic-Serving Institutions (HSI Program), this planning project aims to develop an immersive virtual engineering laboratory that caters to the needs of students with disabilities, providing them with a transformative learning experience within an inclusive framework. In STEM education, engineering focuses on design, problem-solving, and construction. Preparing students for engineering roles necessitates a balance between theoretical instruction and practical exploration. However, traditional engineering laboratories frequently overlook the specific needs of students with disabilities, limiting their active engagement and learning outcomes. This challenge is especially evident given the significant number of students with disabilities in the U.S., the distinct barriers they face in STEM education, and the notable presence of both Hispanic/Latinx students and those with disabilities at Florida International University (FIU). To address this challenge, the research team will collaborate with the Disability Resource Center, the Center for the Advancement of Teaching, and the Center for Diversity and Student Success at FIU to develop an immersive virtual engineering environment that caters to the needs of students with disabilities, providing them with a transformative learning experience within an inclusive framework. By incorporating digital twin models and extended reality (XR) technologies, the study aims to bridge the gap between current teaching pedagogy and student needs, ultimately enhancing educational accessibility and equity for students with disabilities. This project aims to overcome the common barriers faced by students with disabilities by creating an inclusive and immersive learning environment. This innovative fusion of technology holds the potential to significantly enhance the learning experience and outcomes of students with disabilities, enabling students with diverse backgrounds and learning needs to participate in their STEM education actively and effectively.

This project strives to provide actionable insights and solutions tailored to the needs of students with disabilities, aiming to enrich their academic experiences and boost their success in STEM education. By conducting surveys, ethnographic research, and interviews, the research team will delve into the unique needs and characteristics of students with disabilities in their STEM education experience. The creation of an inclusive virtual engineering laboratory, utilizing digital twin models combined with XR technologies, will redefine traditional learning methods, opening up novel avenues for student engagement. This technology-driven approach promises to transform the landscape of STEM education, empowering students with diverse learning needs to engage themselves actively and effectively in their engineering courses. The end goal is to champion inclusivity, elevate educational opportunities, and ensure academic achievements for students with disabilities. The HSI Program aims to enhance undergraduate STEM education and build capacity at HSIs. Projects supported by the HSI Program will also generate new knowledge on how to achieve these aims.

Planning: Digital Twin for Building Performance Simulation and Optimization in Adaptive Reuse Planning
The adaptive reuse of existing buildings presents an essential strategy for addressing societal challenges related to aging infrastructure, urban development, and environmental sustainability. However, current methodologies often fall short in predicting and optimizing building performance due to a lack of accurate data and robust tools. In the realm of sustainable and energy-efficient building design, the untapped potential of advanced technologies such as digital twin technology, integrated with rich building data transformative solutions. Incorporating digital twin technology into adaptive reuse planning not only contributes to efficient resource utilization and waste reduction but also significantly reduces environmental impact, thereby addressing critical sustainability issues. In this project, the Principal Investigator (PI) will utilize this planning grant to develop a proof-of-concept digital twin system, specifically designed for adaptive reuse planning. The PI will conduct workshops to promote knowledge exchange among faculty members from various HBCUs and engage with potential collaborators and industry partners, laying the groundwork for the research ahead.

The project involves developing a sophisticated digital twin model that incorporates diverse data sources, including architectural and structural parameters, historical data, environmental factors, and urban regulations. Leveraging the power of machine learning algorithms and high-performance computing, this model will perform real-time simulations and predictions of building performance. The project aims to enhance energy efficiency by predicting and optimizing building performance parameters, thus informing adaptive reuse planning. The PI will be developing a robust methodology for data collection, model development, performance simulation, and energy efficiency evaluation. This endeavor will result in a user-friendly framework that empowers non-experts to conduct complex simulations and make informed, energy-efficient decisions. This approach introduces a revolutionary, technology-driven method to adaptive reuse planning, potentially transforming the field by making the process more efficient and effective, ultimately fostering more sustainable cities. In this crucial planning stage, the focus is on strategic coordination, engaging potential collaborators, conducting preliminary investigations, and proof-of-concept studies, thereby laying a robust foundation for this ground-breaking research.

SBIR Phase I: A Fully Autonomous Prognostic Digital Twin for Smart Manufacturing
The broader/commercial impact of this Small Business Innovation Research (SBIR) Phase I project seeks to assist industries reduce their downtime for scheduled, preventative maintenance. Industries with high-value assets like manufacturing facilities, engines, satellites, reactors, etc., often incur significant expense due to a lack of usable insights into productivity optimization. The forecasting technology and the developments stemming from this project will have general applicability and enable the use of prescriptive prognostics (when and what to repair) in diverse markets. Additionally, the methods developed in the project for training deep learning systems on limited data would have broad application within the machine learning (ML) community. Frequently, projects are limited by access to and availability of data. The methods developed in this project could be applied to small sets of medical data or financial data, as they are entirely defined on time series variables and dynamics.

This SBIR Phase I project has two main goals. First, to develop a technology that will enable full autonomy in the extraction of meaningful feature sets from raw sensor data. An autonomous feature selection procedure developed in this project will exploit the combination of powerful control-theoretic results with modern ML tools to discover non-obvious linear and nonlinear features. This solution will provide a physics-informed architecture, allowing users to incorporate available physics knowledge with that emerging from the data, configuring a robust, flexible, and autonomous feature extraction mechanism. Second, the team will construct a robust, multi-modal, sensor emulator to address data insufficiency in order to train the ML components. This opportunity is in response to the limited availability of data in manufacturing sector, especially time-series sensor data in operational systems. The sensor emulator will be formed via combinations of modern ML-based generative tools in a manner that exploits their proven effectiveness while being able to work with high-dimensional signals and small training datasets.

CIVIC-FA Track A Smart Kids and Cool Seniors
This Civic Innovation Challenge Full Award (CIVIC-FA) project pilots innovative solutions to assist low-resource urban residents as they adapt to increasing heat stress and local air pollution, both outdoors and indoors. It also seeks to understand what enhances or diminishes adaptive capacity. The project takes place in an environmental justice community in the heart of a densely populated megalopolis that endures air pollution emanating from oil refineries, major highways, a large port, and an international airport. Residents also suffer from increasing heat stress during the summer months. The compounded effects of environmental hazards and poverty threaten the health and well-being of seniors living in public housing. Moreover, young people living in these locations often lack opportunities to learn and develop STEM skills. A community collaborative led by a local non-profit, a public housing authority, and a nearby university seeks to mitigate these problems with a bold project that brings together youth and seniors to characterize indoor and outdoor air quality and thermal conditions and to develop solutions. This project supports society?s efforts to develop effective responses to ongoing climate change.

Understanding the realistic extent of adaptive capacity in low-resource populations is an important question in environmental social science. The project develops a network of fixed outdoor and indoor air pollution and temperature sensors, provides youths living in public housing with environmental sensor-equipped backpacks to carry along routes used by seniors to measure personal exposures, establishes a publicly-accessible data hub, designed with input from an array of community stakeholders, and builds a modeling platform known as a dynamic digital twin to represent the public housing authority?s buildings and surrounding neighborhoods. This platform will support ?what-if? explorations of household solutions (e.g., DIY air filters), enterprise solutions (e.g., air conditioner retrofits), community solutions (e.g., shade trees), and policy solutions (e.g., establishing a right to air conditioning). A multi-faceted engagement and communication strategy will help community members use this information effectively. The digital twin will help the housing authority manage its facilities. Youths participating in the project will gain STEM skills in a highly replicable program. The project adds to empirical and conceptual knowledge of vulnerable residents? behavioral choices and adaptive capacity during heatwaves and air pollution episodes, highlighting the potential for constructive action at multiple levels spanning individuals, organizations, and institutions.

The CIVIC Innovation Challenge is a collaboration with Department of Energy, Department of Homeland Security, and the National Science Foundation.
RAPID/Collaborative Research: Integrated Sociotechnical Investigations of the Compounding Impacts of Maui Wildfires fueled by Hurricane Dora
This Grants for Rapid Response Research (RAPID) project collects ephemeral data to better understand the compounding impacts of Maui wildfires and Hurricane Dora and reveal the differences between residents and tourists in their behavioral responses as affected by infrastructure failures. It examines the sources of warning information, protective action decision-making, and evacuation logistics at the individual level. In the meantime, the project captures the operation states of disaster warning operations in Maui under the loss of cell and electric power services. Failures at each system are documented, as well as the cascading effect among inter-connected infrastructure systems. The research outcomes expand the existing body of scientific knowledge on warning and evacuation while advancing the understanding of informal networks and decision-making in the absence of official guidance.

The hurricane-fueled fast-moving Maui wildfire offers a unique research opportunity to explore the intricacies of decision-making in the absence of official warnings. This event has three unique characteristics. First, Maui has a large percentage of tourists who may exhibit different patterns in warning reception, protective action decision-making, and evacuation logistics. Second, none of the 80 warning sirens placed around the island were activated in response to the wildfire threat. Its absence, coupled with the loss of cell phone and power services, severely limited access to timely official warnings. Third, the cascading failures of critical infrastructure systems highlighted the interdependencies among them and the devastating consequences. This project collects and analyzes multi-dimensional data on heterogeneous behavioral responses by residents and tourists with varying degrees of warning information, as well as the ways in which these responses were affected by critical infrastructure failures (i.e., damages/disruptions to transportation, power, and communication network interoperability). The rich datasets not only bolster future digital twin-empowered applications but also contribute to the enhancement of emergency management in cyber and physical domains. In addition, the project engages and trains multiple Native Hawaiian students in data collection and analysis.

CISE-MSI: DP: CNS: AI-powered Diagnosis Augmented by Self-sustaining Sensing System for Intelligent Wastewater Infrastructure Management
Wastewater infrastructures are critical for modern cities, but aging sanitary sewer systems often suffer from defects like cracked pipes and damaged manholes, leading to the infiltration and inflow problem. This problem results in excessive surface runoff and groundwater flowing into the sewer system, causing sewer overflows and posing risks to public health and the environment. Current management approaches require significant time and expensive resources. This research project addresses this challenge by combining Graph Neural Networks and in-situ water pressure monitoring. The Graph Neural Networks surrogate model represents the urban wastewater system as a graph, allowing for efficient modeling of its temporal, spatial, and topological properties. Further by integrating a physical sensing system, this research enables accurate infiltration and inflow anomaly detection and predictions of cascading impacts with the Graph Neural Networks backbone allowing proactive and corrective actions to be taken in time.

This research revolutionizes the management of urban wastewater systems by leveraging the interdisciplinary knowledge and expertise from hydrological & hydraulic sciences, embedded systems, and artificial intelligence. The technical outcomes of this research can significantly enhance public safety and health in coastal regions by improving the resilience of urban wastewater systems to climate change effects and facilitating quick recovery after natural hazards. By exploring the development of a fully sensed digital twin of the targeted wastewater system in south Texas, the project advances towards increased understanding and management capabilities of wastewater systems. The research results are closely integrated into the education and training of students. Besides, this project promotes the participation of underrepresented groups and K-12 students to pursue STEM studies. All designs are made publicly available to ensure equitable access to Artificial Intelligence-powered decision-making tools for broad implications and future research advances.

Mid-scale RI-1 (M1:DP): National Testing Facility for Enhancing Wind Resiliency of Infrastructure in Tornado-Downburst-Gust Front Events (NEWRITE)
This Mid-scale Research Infrastructure-1 award will support the design phase for a university-based National Testing Facility for Enhancing Wind Resiliency of Infrastructure in Tornado-Downburst-Gust Front Events (NEWRITE). The design of NEWRITE will consider realistic wind fields (speeds up to 225 miles per hour (mph)) in non-synoptic windstorm (NSW) events (tornado/downburst/gust-front) to enable physical testing of their loading and damaging effects on civil infrastructure at mid-to-full model scales (1:10 to 1:1). These weather events cause considerable property damage and numerous fatalities annually in the United States. Damage to infrastructure will only increase with growing urbanization and increased intensification and frequency of such windstorms due to the changing climate. As evidenced in the past, major tornadoes with an intensity of Enhanced Fujita 3 (EF3) or greater (136 mph or greater, 3-second gust) have struck large population centers, causing multiple fatalities and significant damage to residential and engineered structures. Property damage from downbursts and gust fronts are equally significant. NEWRITE will be designed to be a state-of-the-art research and testing platform to mitigate the impacts of NSW hazards on the built environment and significantly reduce fatalities and economic losses. Iowa State University will lead this design project with participation from Clemson University, Missouri University of Science and Technology, Northeastern University, Texas Tech University, University of Arkansas at Fayetteville, University of Florida, University of Washington, and University of Wisconsin at Madison. This design project will be a component of the National Science Foundation (NSF)-supported Natural Hazards Engineering Research Infrastructure (NHERI) and will contribute to the NSF role in the National Windstorm Impact Reduction Program (NWIRP). Data produced during this design project will be archived and made publicly available in the NHERI Data Depot (https://www.DesignSafe-ci.org).

The design of NEWRITE will consider desired capabilities to generate wind fields associated with EF1 to EF5 tornadoes (86-225 mph), moderately intense downbursts (100-125 mph), and gust fronts (80-100 mph). NEWRITE will be designed to investigate NSW hazards on civil infrastructure at scales large enough to study near ground wind fields, wind loading, wind-borne debris impact, and structural damage mechanisms in NSW events. These design parameters would allow the wind engineering community to conduct research to understand the impact of wind and debris in NSW events on the built environment from individual structures to groups of structures at the community scale. The design of NEWRITE will consider capabilities to (a) advance the current knowledge of the transient characteristics of non-synoptic winds and the hazards they pose from the extreme wind and wind-borne debris, (b) advance the understanding of the mechanics of the wind structure and debris-structure interactions in NSW events, such as load distribution, load paths, and component stresses in structures, (c) enable better detection of NSW events on the ground through the integration of wind speed and ground pressure-sensors and radar data for improved warning, and (d) integrate experimental data with computational mechanics to improve structural modeling and damage prediction, enabling the development of resilient structures. The facility will be designed to physically simulate NSW hazards at mid-to-full model-length scale and velocity scale required to faithfully reproduce the damaging effects of these hazards on civil infrastructure and assess potential solutions to enhance their wind resiliency through robust design. During this design project, a smaller scale (1:20) physical prototype of NEWRITE will be designed and constructed at Iowa State University and a digital twin of NEWRITE will be developed to assist with the full-scale NEWRITE design. While the outcome of this project will be final design documents for construction of the full-scale NEWRITE, this award is not a commitment to support the construction phase of the full-scale NEWRITE. This project is jointly funded by the Mid-scale Research Infrastructure-1 Program and the Established Program to Stimulate Competitive Research (EPSCoR).


RAPID/Collaborative Research: Integrated Sociotechnical Investigations of the Compounding Impacts of Maui Wildfires fueled by Hurricane Dora
This Grants for Rapid Response Research (RAPID) project collects ephemeral data to better understand the compounding impacts of Maui wildfires and Hurricane Dora and reveal the differences between residents and tourists in their behavioral responses as affected by infrastructure failures. It examines the sources of warning information, protective action decision-making, and evacuation logistics at the individual level. In the meantime, the project captures the operation states of disaster warning operations in Maui under the loss of cell and electric power services. Failures at each system are documented, as well as the cascading effect among inter-connected infrastructure systems. The research outcomes expand the existing body of scientific knowledge on warning and evacuation while advancing the understanding of informal networks and decision-making in the absence of official guidance.

The hurricane-fueled fast-moving Maui wildfire offers a unique research opportunity to explore the intricacies of decision-making in the absence of official warnings. This event has three unique characteristics. First, Maui has a large percentage of tourists who may exhibit different patterns in warning reception, protective action decision-making, and evacuation logistics. Second, none of the 80 warning sirens placed around the island were activated in response to the wildfire threat. Its absence, coupled with the loss of cell phone and power services, severely limited access to timely official warnings. Third, the cascading failures of critical infrastructure systems highlighted the interdependencies among them and the devastating consequences. This project collects and analyzes multi-dimensional data on heterogeneous behavioral responses by residents and tourists with varying degrees of warning information, as well as the ways in which these responses were affected by critical infrastructure failures (i.e., damages/disruptions to transportation, power, and communication network interoperability). The rich datasets not only bolster future digital twin-empowered applications but also contribute to the enhancement of emergency management in cyber and physical domains. In addition, the project engages and trains multiple Native Hawaiian students in data collection and analysis.
Conference: NSF Workshop on Crosscutting Research Needs for Digital Twins
This two-day workshop will identify research needs for digital twins. A digital twin is a set of digital constructs that mimic the structure, context, and behavior of a physical system. Digital twins are coupled to their physical counterparts; they are characterized by a dynamic, continual, two-way flow of information between the digital representation and the physical counterpart. Data streaming from the physical system are integrated into the digital representation to reduce uncertainties and improve accuracy. The digital representation may in turn be used to control the physical system, optimize data acquisition, and prove decision support. Digital twins must execute rapidly enough to support decisions and control in time scales that are relevant to the physical system and must manage and quantify uncertainties. Of particular note is the bi-directional interaction between the virtual and the physical, which is central to distinguishing a digital twin from a conventional simulation. This bi-directional interaction brings many new challenges to modeling, data curation, and decision-making.

The workshop will bring together a diverse community of stakeholders to identify research gaps common across application domains and that may benefit from crosscutting interdisciplinary research efforts. The technical program is organized around four methodological themes: (1) Models; (2) Data; (3) Decisions; and (4) Verification, Validation & Uncertainty Quantification. To ensure crosscutting outcomes, these themes will be explored across four application domains: (1) Engineering, Materials & Manufacturing; (2) Smart Cities; (3) Biomedicine & Health; and (4) Climate, Natural Hazards & Environmental Sciences. Workshop speakers will be asked to identify barriers and enablers for achieving scalable digital twins in the four methodological themes, giving concrete examples drawn from the application domains. These barriers and enablers will be collected and organized during interactive workshop sessions. The objective is to identify, for each of the methodological themes, a top-five list of crosscutting barriers and a top-five list of potential crosscutting enablers, supported with concrete examples from the application domains.

Collaborative Research: FW-HTF-R: Future of Construction Workplace Health Monitoring
Given the disproportionate rate of fatalities and injuries in the construction industry and the potential of ambiguous health and hazardous situations with respect to the impending technological revolution and climate change, it is crucial to improve the health and safety of the future workforce. However, there is a lack of an effective, objective, and continuous approach for assessing construction workers' health status at jobsites. Although there have been important innovations in wearable physiological sensing technologies and artificial intelligence for objective assessment of construction workers' health parameters, there remain fundamental challenges for establishing a worker-centered holistic health monitoring approach with promising preventive potentials. These challenges stem from: a) lack of a scalable and feasible wearable sensor for continuous elicitation of workers' diverse bodily responses to stressors in the field; b) lack of a robust interpretive data-driven framework to process the elicited signals for automatic early detection of physical fatigue, mental stress, and exposure to heat stress; and c) lack of effective representation of health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. By establishing a real-time and context-aware holistic health monitoring approach, this project will play a fundamental role in improving the safety of close to 7 million workers in the U.S. construction sector. The developed intelligent health monitoring system is expected to produce changes in the quality of work and workforce policies, resulting in reduced conflicts and enhanced quality of life. It can also be used to address workplace health issues in other hazardous industries such as manufacturing, firefighting, and agriculture.

The overarching goal of this research is to improve construction workforce health and safety by integrating multi-disciplinary research in flexible, wearable sensor fabrication, artificial intelligence, and privacy-aware information visualization to provide near-real-time and projected future context-aware health and safety information to workers and managers for enabling improved task decisions by augmenting their situational awareness. The intellectual significance of this project lies in fulfilling the goal by generating and expanding new knowledge on three fronts. First, the project will design and fabricate a flexible wearable sensor for continuous and noninvasive measurement of workers' bioelectric signals and electrochemical responses at construction sites. The use of a single, flexible wearable sensing device instead of multiple off-the-shelf sensors will facilitate the scalability and feasibility of the proposed health sensing system in the construction workplace. Second, the project will develop robust machine learning algorithms and frameworks for continuous and objective assessment of workers' health conditions in the field based on physiological, contextual, and environmental data. For this purpose, this project will address fundamental challenges related to traditional machine learning algorithms by developing a novel interpretive data-driven approach robust to inter- and intra-individual variability while ensuring data security and privacy. Third, this research will generate a digital twin model (health and safety maps) of the construction sites through an array of collective health analyses and develop an automated feedback module for providing personal health-related information and corresponding mitigation strategies to field workers. The insights into the collective health and safety information can profoundly assist the workers and safety managers in making a sound, far-sighted decision about the execution of field-oriented construction operations in near real-time. This research effort will open new doors in improving proactive health and safety management in the field through collective visualization of workers' real-time health and safety information.
SBIR Phase II: Composing Digital-Twins from Disparate Data Sources
The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project relates to the creation of a digital twin, an interactive, 3-dimensional model of a real-world system, of complex industrial environments and assets. This digital twin provides infrastructure necessary for the application of virtual reality training and augmented reality live-guided procedures in industrial workplaces, at scale. By making the full-scale roll-out of these technologies possible, the technology seeks to impact human health and safety, operational efficiencies, and environmental risk reduction for process operations facilities, such as oil refineries and chemical plants. The long-term impacts of this technology may also enable automation and optimization, improving their efficiency, security, and safety. Such facilities are critical infrastructure and play a significant role in the national economy. The availability of this product may also enhance market opportunities for other businesses in the scanning, spatial computing, and training markets. The impact may be further broadened by adapting the process for digital twin production to new domains unrelated to the industrial market.

This Small Business Innovation Research (SBIR) Phase II project is advancing knowledge and understanding in both machine learning and spatial computing. This project focuses on a method for digitizing a complex, real-world system, in an efficient manner, sufficient to recreate the captured reality as an interactive digital twin. The primary technical hurdle is the combining of different data sources, that describe aspects of a particular real-world system, into a single, complete description. The initial physical systems being modelled are industrial process operations, but the core methods could apply to other types of systems, including natural systems, such as a rainforest. For industrial process operations, the goal is to encode the entire process operations facilities, at the component level, with sub-centimeter accuracy, at 10% of the current time and cost requirements. To achieve this, this project will combine physical scans with engineering documentation and relational probabilities. Once combined, the model will be used as the basis for a digital twin of the real-world system projected into spatial computed environments, such as virtual and augmented reality. These techniques replace a tedious and limited static scan and intensive human labor workflow with rapid scans, computer vision, and a combination of procedural and trained algorithms.
FW-HTF-RL/Collaborative Research: Future of Digital Facility Management (Future of DFM)
This Future of Work at the Human-Technology Frontier - Research: Large (FW-HTF-RL) award supports research to address the critical shift towards digital transformation in the facility management industry, a sector grappling with technological and workforce changes. The industry is on the verge of adopting advanced Information and Communication Technology, Internet of Things, and Big Data paradigms, aiming to make traditional operations more data-driven to meet organizational and national energy reduction goals. These advances target multiple improvements, including occupant comfort and health, system resilience, energy performance, and operational cost reductions. The transition necessitates facility managers to adapt to increased data volumes and new forms of human-machine interaction, thereby necessitating upskilling and reskilling. The project also recognizes the skills shortage in the industry, highlighting the need for efforts from industry leaders, educators, and policymakers to prepare a workforce for the future of facility management.

The technical goals of this project aim at addressing the digital transformation challenges within facility management. The first objective is to construct a digital twin ecosystem for a facility, enhancing the role of facility managers by providing them with physical and cognitive assistance. The second objective involves developing a multi-modal user interface to promote effective interaction within the digital ecosystem, allowing communication between occupants, FMs, and other stakeholders. Thirdly, the project will gauge facility managers' readiness to utilize the digital twin technology, intending to refine the technology continuously and identify any barriers to adoption. The final objective is to study the impacts of adopting this technology on facility managers, occupants, and facility owners, incorporating different perspectives to develop comprehensive solutions to the industry's challenges. Additionally, project work with industry leaders and facility managers will identify the necessary training and educational initiatives that can equip the current and future workforce with the skills required to keep pace with digital transformation. Such initiatives will involve curating curricula that are aligned with technological advancements, enhancing existing training programs, and designing new ones to fill the skills gap. Collaboration activities will also seek to inspire and attract diverse talents to the industry, thus fostering a work environment that is ready for the future of digital facility management.
CADMap: Creating Mapped Solid Models of Deformed As-Manufactured Geometries that Link to an Original Reference Design
The accurate, precise, and comprehensive representation of shape details of manufactured components is critical for the design, control, and optimization of next-generation, cyber-based manufacturing systems. However, in many cases, the design intent encoded in the Computer-Aided Design (CAD) model cannot be applied directly to the as-manufactured part because the physical part never perfectly matches the geometry of the designed part. Therefore, linking measurements back to the design is challenging. This project establishes a systematic and generalizable mapping methodology between as-designed models and as-built geometries in the presence of large deviations. The methods developed as part of the project define mappings that leverage the design information in the context of the as-built part, which is otherwise impossible with simple reverse engineering. This work will facilitate data mapping from different resources and modalities in manufacturing and across the product lifecycle. Hence this project will enable digital twin systems and Manufacturing 4.0 that rely on connectivity and information exchange between digital models and physical parts. In addition, the integration of education and outreach will broaden participation in manufacturing and train the next generation of engineers and researchers with state-of-the-art CAD modeling, manufacturing, and data analytics expertise.

The overarching goal of this research is to establish a systematic mapping framework between as-designed models and the as-built geometries to be used for applications in next-generation manufacturing systems. The specific research objectives of this project include: (1) creating a rough mesh fit of the as-built point cloud that represents the exterior of the as-built part to enable a mesh-based mapping between as-built and reference geometries. The approach of this project looks at the entire object as a unit and its natural deformations to accommodate large deformations. (2) Fitting the CAD model to the as-built geometry while preserving the design topology information embedded in the original CAD. The approach of this project makes conscious choices to retain the topology of the reconstructed shape. (3) Establishing a systematic mapping between as-designed models and the as-built geometries. The team introduces the idea of persistent mapping that uses a defined procedure based on identified features or landmarks to parameterize the object surface piece by piece. Finally (4), Evaluating the effectiveness and demonstrating the usefulness of these new methods for specialized case studies in additive manufacturing and nondestructive evaluation. The project will facilitate the digital integration of design and manufacturing within larger product lifecycle ecosystems, especially in sectors with requirements for process monitoring and quality assurance, e.g., additive manufacturing and nondestructive evaluation.
FW-HTF-RL/Collaborative Research: Future of Digital Facility Management (Future of DFM)
This Future of Work at the Human-Technology Frontier - Research: Large (FW-HTF-RL) award supports research to address the critical shift towards digital transformation in the facility management industry, a sector grappling with technological and workforce changes. The industry is on the verge of adopting advanced Information and Communication Technology, Internet of Things, and Big Data paradigms, aiming to make traditional operations more data-driven to meet organizational and national energy reduction goals. These advances target multiple improvements, including occupant comfort and health, system resilience, energy performance, and operational cost reductions. The transition necessitates facility managers to adapt to increased data volumes and new forms of human-machine interaction, thereby necessitating upskilling and reskilling. The project also recognizes the skills shortage in the industry, highlighting the need for efforts from industry leaders, educators, and policymakers to prepare a workforce for the future of facility management.

The technical goals of this project aim at addressing the digital transformation challenges within facility management. The first objective is to construct a digital twin ecosystem for a facility, enhancing the role of facility managers by providing them with physical and cognitive assistance. The second objective involves developing a multi-modal user interface to promote effective interaction within the digital ecosystem, allowing communication between occupants, FMs, and other stakeholders. Thirdly, the project will gauge facility managers' readiness to utilize the digital twin technology, intending to refine the technology continuously and identify any barriers to adoption. The final objective is to study the impacts of adopting this technology on facility managers, occupants, and facility owners, incorporating different perspectives to develop comprehensive solutions to the industry's challenges. Additionally, project work with industry leaders and facility managers will identify the necessary training and educational initiatives that can equip the current and future workforce with the skills required to keep pace with digital transformation. Such initiatives will involve curating curricula that are aligned with technological advancements, enhancing existing training programs, and designing new ones to fill the skills gap. Collaboration activities will also seek to inspire and attract diverse talents to the industry, thus fostering a work environment that is ready for the future of digital facility management.

NSF Convergence Accelerator Track J Phase 2: CropSmart - a digital twin for making wiser cropping decisions nationwide
Healthy crop production in the U.S. is critical for not only the food and nutrition security of the U.S. and the world but also the prosperity of the U.S. economy. The USDA Agricultural Innovation Agenda calls for increasing U.S. agricultural production by 40% while cutting its environmental footprint in half by 2050. Sound crop management decision-making is a key to achieving this ambitious goal. An example of such decision-making is ?should I irrigate my cornfield today? If so, by how many inches of water?? Traditionally, such decisions are made by individuals based on their empirical judgment, which is often subjective and less optimal. Science-based, data-driven approaches for cropping decision-making rely on timely and accurate information on current and predicted future conditions of crop and environment to make optimal decisions. However, it remains a challenge for stakeholders to adopt the data-driven approach because they do not have full and effective access to the timely and accurate information and lack facilities or knowledge to process the information. This project will meet the challenge by offering the data-driven optimal cropping decision-making services nationwide up to field scales through developing and operating the CropSmart digital twin. The services will be accessible to users through both web portals and smartphone Apps. This project will help USDA to archive its innovation goal, enhance food and nutrition security of the U.S. and the world, and bring hundred-million-dollar economic return and huge environmental benefits to U.S. economy and society annually.

CropSmart, to be built and operated by this project, is a digital replica of real-world cropping systems over the contiguous US up to 10-m spatial resolution. It will not only accurately represent the current crop and environment conditions, but also predict, with acceptable confidence levels, future conditions with hypothetical ?what if? scenarios, resulting in actionable predictions. CropSmart will provide three services to users: 1) user-specific decision ready information on which the user can make data-driven decision; 2) ?what if? tradeoff service which will generate consequences (e.g., yield, economic return, or environmental footprint) of different user decision options so that the user can find the optimal decision; and 3) decision advice service which will automatically generate optimal decision based on a user?s decision goal. CropSmart will be built by integrating the advanced remote sensing, crop and environmental modeling, AI/ML, agro-geoinformatics, and digital twin technologies through the multi-disciplinary convergence approach. The major project activities will include: 1) implementing CropSmart to support at least 6 types of top-priority decision-making use-cases specified by the user community; (2) deploying CropSmart operationally to cultivate its user community and show its gaming-change impacts; 3) broadening adoption, participation, and impact through a comprehensive education, extension, and outreach program; and (4) establishing a community-based CropSmart.org and implement the sustainability plan to sustain CropSmart activities after project expires and maximize the long-term project impacts. At the end of the performance period, this project will deliver the CropSmart software package, the operational CropSmart services, and a sustained community of at least 6,000 users.
SBIR Phase I: Testing computational feasibility and effectiveness of real time traffic nearcast for wildfire evacuation at the wildland urban interface
The broader/commercial impact of this Small Business Innovation Research (SBIR) Phase I project is to reduce the time for residents to evacuate to safe destinations by providing personalized evacuation guidance, resulting in lower risk to life and smoother government operations during a wildfire. Wildfires are an increasingly prevalent disaster; 50 million U.S. homes are currently in the Wildland-Urban Interface (WUI) areas. This project aims to empower residents in WUI communities by developing services that provide real-time information and personalized evacuation guidance during wildfires. Such services also supplement the actions of emergency response agencies that are often overloaded during wildfires due to resource and workforce constraints. The lessons learned through this project can be applied to other natural or man-made disasters, benefiting many more U.S. citizens. This technology will create highly skilled jobs and increase partnerships between academia, industry, government, and wildland-urban interface communities.

This technology innovation searches for the best evacuation strategies on digital replicates of the Wildland Urban Interface (WUI) using real-time traffic nearcast simulations that are dynamic and adaptive. These strategies are then be provided to evacuees as real-time, individualized routing guidance to safe destinations. The technology will also provide communications abilities, situational awareness, and an optimization platform for emergency response agencies. The project adopts the latest digital twin (DT) technology to revolutionize static planning methods and evacuation plan information distributed in leaflets. Specifically, for wildfire evacuation, the DT framework involves versatile simulations of the fire, communications, and traffic flow; Is designed to incorporate infrastructural parameters such as the road network, environmental parameters, as well as behavioral parameters of both the evacuees and the emergency managing agencies. The technical scope of this project focuses on the development and demonstration of a real-time solution based on DT simulation technology that improves a series of emergency evacuation metrics, including the total evacuation times, fire exposure times, and traffic congestion. Numerical validation against observed data and computational speed testing will be carried out to quantify the effectiveness and computational efficiency in order to build a baseline understanding of the performance of the solution, and subsequently establish confidence in real-life implementation.
Collaborative Research: Planning: FIRE-PLAN:High-Spatiotemporal-Resolution Sensing and Digital Twin to Advance Wildland Fire Science
The number of catastrophic wildfires in the United States has been steadily increasing in recent decades, which generate casualties, large loss of properties, and dramatic environmental changes. However, it is difficult to make accurate predictions of wildland fire spread in real time for firefighters and emergency response teams. Although many fire spread models have been developed, one of the biggest challenges in their operational use is the lack of ground truth fire data at high spatiotemporal resolutions, which are indispensable for model evaluation and improvements. The objective of this planning project is to bring together wildland fire science researchers, fire sensing and data science experts, and diverse stakeholders to develop standards and requirements for high-spatiotemporal-resolution wildland fire sensing and digital twin construction. An organizing committee will be formed from wildland fire science, engineering, and stake holder communities including fire ecology and behavior modeling, pollution monitoring, robotics, cyber physical systems (CPS), wildfire fighting, indigenous cultural burns, and prescribed fires. A series of physical and remote workshops will be held focusing on themes such as open fire data for wildland fire modeling validation, digital twins for prescribed fires, and safe and efficient wildland fire data collection.

Research tasks of this planning project include: 1) identification of key high-spatiotemporal-resolution fire metrics and data representations to support fire model validation and fire operations, 2) proposition of sensing strategies and algorithms for fire sensing and suppression robots and cyber physical systems that can support safe and efficient collection of desired high-resolution fire data, 3) development and evaluation of data assimilation and digital twin construction using high-resolution data to advance fire behavior modeling, coupled fire-atmosphere modeling, and smoke modeling, and 4) prototype and initial fire data ecosystem demonstration including collection of cultural burn data and establishment of GeoFireData, a benchmark fire data sharing and digital twin website, which can support different fire operation types such as fire spread model validation and controlled burn planning. The special attention will be devoted to interdisciplinary training of the next generation of scientists working with wildfire risks at the interface of computational sciences, engineering, ecology, and data sciences.

CLIMA: A Digital Twin Modeling Framework for Climate Adaptive Vertical Infrastructure
This award supports research focusing on developing a novel Digital Twin framework for the quantification of Greenhouse Gas (GHG) emissions associated with the operation of vertical infrastructure, and minimizing such environmental footprint by designing and deploying environmentally responsive building envelopes. Digital twin modeling refers to the creation of a high-fidelity three-dimensional representation of a physical asset, which is augmented by live data streaming from sensors, which is then fed into real-time predictive models. By adopting this paradigm, the physical and digital assets are continuously linked to each other (i.e., they age together, hence the term "twin"). This research project will leverage such capabilities to devise new strategies to design and operate buildings equipped with façades capable of modifying their geometry and behavior to maximize lighting and ventilation, while minimizing energy requirements and associated GHG emissions. Core to this effort is a heavily instrumented building within the University of Pittsburgh that will serve as the test bed to create and validate this new toolset. The research will also be complemented by delivering educational and outreach activities for graduate and undergraduate students, summer research internships, as well as middle schools and underrepresented minority outreach programs.

The specific goal of the research is to create digital tools to allow for a holistic assessment of the short- and long-term performance of the building, by analyzing all the structural and non-structural components at the level of the constituent materials and assessing how environmentally adaptive façade components can be leveraged to optimize such performance over time. The nonlinear, time-dependent nature of the response of the system in combination with its environment is, in fact, of crucial importance in view of the large degree of uncertainty on the demand resulting from climate change. General circulation models based on the Intergovernmental Panel on Climate Change?s Sixth Assessment Report will be downscaled for regional consideration and will be leveraged in the Digital Twin framework, in which mechanistic predictive models and Machine Learning algorithms will be embedded, with the ultimate goal of guiding the design and operation of climate-adaptive buildings to minimize life cycle GHG emissions. This project brings the potential to define a new generation of Digital Twin tools to perform comprehensive assessment of the performance of vertical infrastructure, paving the way for real-time quantification and visualization of GHG emissions associated with the construction and operation of complex civil systems, while at the same time shedding light on how climate adaptivity can be leveraged to minimize their carbon footprint.

This project is supported by the Engineering for Civil Infrastructure (ECI) Program and the Engineering Design and Systems Engineering (EDSE) Program of the Division of Civil, Mechanical and Manufacturing Innovation (CMMI) of the Directorate for Engineering (ENG).

CAREER: AI-enabled Integrated Nutrient, Streamflow, and Parcel sImulation for Resilient agroEcosystems (INSPIRE): a framework for climate-smart crop production and cleaner water
Climate-smart agricultural practices hold the promise of reducing carbon (C) emissions from farming, yet their implementation often presents complex trade-offs, particularly affecting nitrogen (N) and phosphorus (P) management. Integrated management of C, N, and P to ensure climate-smart crop production while preserving clean waters is hindered by several knowledge and technology gaps. To approach a solution for this grand challenge, this project aims to significantly advance the holistic understanding and modeling of the interconnected C, N, P, and water cycles in the Upper Mississippi River Basin. This goal will be pursued by developing an AI-based framework of integrated nutrient, streamflow, and parcel simulation for resilient agroecosystems (INSPIRE) that can easily ingest multi-source observations and provide an accurate and speedy quantification from the field to basin scale. The outcomes from this project are expected to provide valuable insights for policymakers and farming communities, particularly in optimizing management practices for improved carbon sequestration, soil health, and water quality in the America's heartland. Additionally, this project intertwines its research objectives with an educational agenda, which is featured by developing a computational tool to foster broad participations in large-scale computing among undergraduates. The project will also introduce a cyber-physical watershed mesocosm as an innovative trial of using the digital twin technology to enhance STEM education related to agricultural and environmental sustainability.

This project will develop under the overarching hypothesis that AI-assisted integrated simulation of C, N, P, and water fluxes, compared with existing process-based modeling approach, is better able to capture high resolution environmental variability and identify best practices for achieving climate-smart agriculture and water quality goals without sacrificing crop production. The scientific innovations will be achieved through four objectives. First, a Knowledge-Guided Machine Learning (KGML)-based INSPIRE-Field model will be developed to significantly improve the prediction accuracy of field-level C, N, P, and hydrological interactions. Second, INSPIRE-Field will be coupled with Graph Neural Network (GNN)-based hydrologic surrogate models that first aggregate field water and nutrient fluxes within small watersheds (i.e., INSPIRE-Watershed), and then routing watershed outputs throughout the Upper Mississippi River Basin (i.e., INSPIRE-Basin). To reduce the uncertainty of INSPIRE, a novel representation learning method to efficiently assimilate remote and in-situ sensing data via low-dimensional embeddings will be explored. Third, a user-friendly web interface will be developed that allows stakeholders to preview outcomes of different climate- smart management practices and identify field-specific preferred management strategies based on multiobjective optimizations for C, N, P, and hydrological goals. Finally, the education and practice of computing, sensing, and machine learning among the future workforce of agroecosystem engineers, educators, and decision-makers will be enhanced through project activities. The investigator aims to lead the frontier of data analytics for sustainable agriculture by integrating remote sensing, mechanistic modeling, and artificial intelligence, with the aspiration to enable monitoring and managing every cropland, track pollutants, forecast agricultural risks, provide farmers best solutions to minimize negative environmental impacts, and ultimately help the world to achieve a sustainable food future.

Collaborative Research: SWIFT-SAT: INtegrated Testbed Ensuring Resilient Active/Passive CoexisTence (INTERACT): End-to-End Learning-Based Interference Mitigation for Radiometers
As next-generation communication and satellite systems utilize more frequency bands, the potential interference risks to passive radiometer sensors used for environmental and atmospheric sensing are increasing. Thus, it is imperative to develop efficient methods to detect, characterize and mitigate anthropogenic sources of interference at passive radiometers. Radio frequency (RF) research domains, specifically those addressing the active/passive coexistence, are in critical need of datasets that enable learning-based detection, identification, and classification, as was observed in image processing domains. The goals of the project INTERACT (INtegrated Testbed Ensuring Resilient Active/Passive CoexisTence) are 1) to collect/to currate active/passive RF coexistence datasets with ground truth information 2) to develop data-driven learning-based RF interference (RFI) detection and mitigation approaches enabled by the generated data. The datasets will be collected by an airborne passive microwave radiometer system to be deployed on the NSF's AERPAW (Aerial Experimentation and Research Platform for Advanced Wireless) platform. The proposed research will further our undertanding on spectrum sharing through passive sensing methods, RF datasets, and learning based RFI mitigation approaches.

The project INTERACT proposes three key innovations: 1) A new Unmanned Aerial System (UAS) based passive radiometer system will be developed. This system together with the experimental development of various active transmission scenarios covering different geometries, transmitter parameters and waveforms at non-restricted bands will result in the first-ever large experimental RF dataset with ground truth information for passive/active RF coexistence. A digital twin for passive radiometry in the emulation environment of AERPAW will be developed to enable experimenters to facilitate extensive, yet realistic RF mitigation experiments in a Cloud environment. 2) Novel data-driven end-to-end learning-based RFI detection and mitigation approaches will be developed. The proposed solutions will focus on approaches that can achieve high-resolution RFI detection in the time-frequency domains, learning based radiometer calibration, and joint mitigation to estimate the scientific observation of radiometers under RFI. These solutions do not require centralized servers and are designed to work on passive radiometer systems in order to detect and mitigate RFI without any information exchange between coexisting systems. 3) The research will produce new deep reinforcement learning and subspace-based RFI mitigation approaches using the feedback from active and passive systems.
SBIR Phase I: CAS: DIGITAL TWIN FOR CLIMATE RESILIENCE ANALYTICS
This Small Business Innovation Research (SBIR) Phase I project augments community resilience to climate hazards by improving the situational awareness of public organizations, officials, and emergency managers. The project is focused on harnessing the data revolution in dealing with climate hazards. The team develops a digital twin technology for disaster preparedness, response, and recovery. Climate hazards (hurricanes and floods, in particular) are the most prominent stressors for communities in the United States and worldwide, causing dire physical, social, and economic hardships. The outcomes of this research have the potential for significant societal benefits that could enhance the public safety of millions of U.S. residents exposed to climate hazards and potentially lead to millions of dollars in avoided disaster management costs through proactive preparedness. The project could transform the ability of decision-makers, emergency managers, and responders to tailor their strategies and technologies to enhance situational awareness in dealing with climate hazards.

This Small Business Innovation Research (SBIR) Phase I project delves into the intricate challenges of creating and designing a state-of-the-art digital twin technology that harnesses the power of community-scale big data and machine intelligence, offering a proactive and predictive lens on community preparedness, evacuation measures, protective actions, and post-emergency event recovery. The research activities include: (1) creating and testing computational methods, algorithms and metrics for specifying the extent of a populations' preparedness, evacuation planning, and recovery at the block group scale in near-time; (2) prototyping and optimizing the architecture of a web-based digital twin platform with effective data fusion and computation workflows in order to implement the created methods and algorithms and visualize the output insights in an intuitive, timely, and decision-friendly manner; (3) evaluating the performance of the aforementioned computational methods embedded in the digital twin technology prototype in the context of recent climate hazard events; and (4) demonstrating the use case of the digital twin prototype for emergency response and management applications through existing and growing partnerships.
ART: Research to Solutions, Building Translational Capacity in the Central Florida Innovation Ecosystem
This project will accelerate the University of Central Florida?s (UCF?s) contributions to the regional and global innovation ecosystem centered in the Central Florida area. UCF is part of a strong and supportive innovation ecosystem primed for research translation. UCF has partnerships and collaborations with the sectors of space, aerospace, health, defense (optics and lasers, AI/ML/computer vision, digital twin, simulation, and modeling), energy, and many others. Accelerating UCF research translation will create an enormous economic development opportunity since UCF already produces high levels of fundamental research. This project will create the infrastructure needed to connect fundamental research produced by UCF and to train diverse talent to meet the needs of the Central Florida ecosystem and the nation?s ecosystem. Because of the industry sectors (space, aerospace, defense, and energy), and the global nature of the companies involved, this project will have national and global positive impact on economic growth, societal wellbeing, and national security.

This project will enhance the research translation capacity of UCF, a Hispanic Serving Institution (HSI), by building missing and by leveraging existing infrastructure with the assistance of mentoring institution, Georgia Tech, a high research translational institution. The project will undertake activities along the following key thrusts: (1) Enhance research translation capacity by creating a Venture Lab to directly support translation, and by reorganizing entrepreneurial support organizations to follow a more advanced model similar to our mentor institution. (2) Expand the educational and training opportunities for students across campus with reprograming and expansions of evidence-based entrepreneurship education at the colleges of engineering and computer science and business; and expand the pathways graduate students can pursue after graduation to include startup development and industrial research and development. (3) Recruit, mentor, and support six seed translational research projects aimed to be developed into successful technology startups. (4) Create, expand, and sustain a network of entrepreneurship and innovation ambassadors from industry and community partners in support of the project objectives 1-3.

The project is partially supported by NSF’s Louis Stokes Alliances for Minority Participation program. Partial funding for this project was provided by the Improving Undergraduate STEM Education (IUSE) program, which supports improvements in STEM teaching and learning for all undergraduate students.

STTR Phase I: Semantically-Enabled Augmented Reality for Manufacturing
This Small Business Technology Transfer (STTR) Phase I project facilitates safer and more efficient human-centered manufacturing tasks. The introduction of context-sensitive work guidance through immersive technologies will expedite workforce training, enhance users' spatial awareness, and outperform existing manufacturing work instruction systems, leading to heightened productivity across industries. This development embodies the emergence of cyber-human relationships and Digital Twin and Smart Factory applications, reinforcing U.S. manufacturing leadership, bolstering economic competitiveness, and fortifying national security. The anticipated commercial Platform-as-a-Service (PaaS) solution is poised to benefit approximately 10,000 U.S. manufacturing firms. Beyond its economic implications, the first-generation, open-specification Reality Modeling Language (RML) developed in this project is expected to gain widespread acceptance in the international standards community, improving spatial system automation across diverse industry verticals. Ultimately, this system will render the physical world more accessible, searchable, and comprehensively annotated with data, unlocking new frontiers in user support, safety, and efficiency.

This Small Business Technology Transfer (STTR) Phase I project addresses mission-critical challenges for fully leveraging Augmented Reality (AR) tools in manufacturing environments. It draws upon ontologically structured data and a proprietary Artificial Intelligence (AI)-driven knowledge system for automating the generation and display of context-specific AR content in 3D space, eliminating the need for individually designed AR interactions. The solution enables training and work instruction systems to become spatially- and contextually aware, in order to adapt to dynamic conditions impacting worker safety and efficiency. The objective of this project is to demonstrate and quantify how automatically generated, spatially- and semantically aware AR can provide work guidance, machine status data, and hazard warnings to increase worker capabilities versus conventional guidance tools. The RML will be derived and logically describe and computationally code the 3D spatial scene of a simulated factory floor, and later, RML will be released as an open code library to the developer community. The system will sense the real world and objects in real-time, learn as input is received, and prioritize and render AR content communicating context-specific suggestions and warnings. This project will demonstrate integration between workers, their environment, and the tools engaged to complete their tasks so production personnel can act confidently, safely and effectively.
CAREER: Digitize and Simulate the Large Physical World via Knowledge-Grounded Scene Representation
Numerous fields, such as agricultural robotics, necessitate that humans and intelligent agents have a shared understanding of the physical world to be able to make informed decisions. This requires that intelligent machines, to effectively assist humans, understand the physical world, simulate various possibilities, and predict and evaluate outcomes. Current computer vision systems, which largely focus on understanding and modeling observed phenomena, fall short of these capabilities. Similarly, domain-specific simulators fail to achieve this goal as they lack real-world grounding. This project aims to construct AI systems capable of creating a digital replica of the large 3D world and faithfully simulating various counterfactual scenarios, thereby enabling users to assess the outcomes of different decisions and actions. Central to this work is an actionable, knowledge-grounded scene representation, facilitating real-world modeling and simulation. The project has the potential to amplify advancements in various disciplines, such as robotics and agriculture.

This project aims to achieve this goal via four directions that advance digitizing and simulating the physical world. To achieve this goal, the project offers a framework that will (i) create an actionable scene representation from real-world videos; (ii) infer physical parameters of the digital world from visual observations; (iii) perform physics-grounded simulation over the digital twin that enables the creation of realistic and accurate counterfactuals; and (iv) integrate the digital twin with domain knowledge and modeling for various applications. Based on the research studies, the project will develop a digital twin education platform that can be applied in undergraduate and graduate education, mentoring, and K-12 outreach activities to engage the next generation of researchers in computing.
NSF Engines: Central Florida Semiconductor Innovation Engine
This NSF Engines award to the Central Florida Semiconductor Innovation Engine will play a critical role in bolstering our nation?s production capacity of semiconductors, particularly in the emerging advanced semiconductor packaging sector. Advanced semiconductor packaging is increasingly an essential process to achieving the performance demands of many of the emerging applications that are now at the forefront of technological innovation?for example, fifth-generation (?5G?) wireless communication, autonomous vehicles, artificial intelligence and machine learning, advanced sensors, and virtual/augmented reality. Currently, the United States does not have a meaningful onshore semiconductor advanced packaging manufacturing sector, as less than 3% of semiconductor advanced packaging manufacturing occurs in the United States; the nation is overwhelmingly reliant on offshore foundries, primarily in Asia. As the limits of Moore?s Law are reached and packaging becomes a more complex and important part of the semiconductor manufacturing process, this NSF Engine is positioned to capture the economic growth, drive key technological advances in a rapidly-changing sector, and secure our national defense by rooting a vital industry on American shores. In addition to the significant national impacts, the NSF Engine has the potential to create thousands of good-paying jobs in a highly dynamic and innovative technology space. The NSF Engine has already created flagship upskilling and retraining programs that have created ladders into economic opportunity for workers from all educational backgrounds. If successful, the concentration of semiconductor talent, existing physical infrastructure, and research capacity of the region could represent a key national security asset and a new force for economic prosperity in the region.

The broader region of service encompasses a seven-county region in Central Florida centered around Osceola County. This is a strategic location for an emerging semiconductor hub, and the NSF Engine is uniquely positioned for success in part by geography. NeoCity, the heart of the NSF Engine?s efforts, is located in one of the nation?s fastest growing metropolitan areas for STEM talent, with proximity to the Space Coast and one of the nation?s leading technical universities with a strong concentration of the nation?s semiconductor research talent, the University of Central Florida. Additionally, the NSF Engine has built a STEM-focused high school in the NeoCity Industrial Park with specific workforce tracks and internships that feed into various opportunities within the local semiconductor industry.

The region was also the recipient of a Build Back Better Regional Challenge award as well as significant investments from the state of Florida totaling nearly $300 million to position and scale the region?s capacity to serve as a semiconductor hub for the nation. These efforts have been led by ICAMR, Inc. (d.b.a. BRIDG), which is a non-profit public-private partnership with heavy involvement by Osceola County government, R1 academic and nonprofit research institutions (University of Central Florida, University of Florida, imec-USA), a Hispanic Serving Institution and technical community college (Valencia College), a workforce development provider (CareerSource Central Florida), economic and community development organizations (Orlando Economic Partnership, Florida High Tech Corridor), and various industry partners. Each play a significant and complementary role within the broader ecosystem.

Use-inspired research and translation activities by the NSF Engine, both occurring in academic labs and municipally-owned industrial fabrication facilities, stand to advance the current state of practice by advancing digital twin technology as it relates to the semiconductor industry, creating new technology for assured and secured semiconductor packaging, and developing new technologies for the implementation of vertical interconnects and interposers (e.g., through-silicon via, through-glass via, through-substrate via) that would position the NSF Engine at the leading edge of chip packaging design and manufacturing. Leading industry partners in the semiconductor sector have already initiated direct engagement with researchers to inform, test, and scale prototypes that result from research activities, further positioning the region as a central point for research talent and industry partners in the semiconductor sector to crossover and converge to ultimately produce significant advanced packaging innovation.

This NSF Engine ? due to a combination of intentional efforts to train a prepared workforce, unprecedented investments in physical infrastructure (including a county-owned fabrication lab), targeted economic development initiatives, and a regional concentration of experienced semiconductor research and engineering talent ? has a unique capacity and the necessary competitive advantage to succeed and thrive in a highly competitive sector and provide our nation with a key domestic supply chain resource for one of the most important technologies in the world today. It is also poised to create another vital national resource: a highly trained semiconductor workforce. Together with key partners such as Valencia College, the NSF Engine has already begun the essential work of creating pathways to high-paying jobs in the sector for residents from across the region that open opportunity and create economic stability for families up and down the economic ladder. These factors position the NSF Engine to be a powerful engine for U.S. competitiveness, economic opportunity, and advanced semiconductor packaging innovation.
NSF Engines: Central Florida Semiconductor Innovation Engine
This NSF Engines award to the Central Florida Semiconductor Innovation Engine will play a critical role in bolstering our nation?s production capacity of semiconductors, particularly in the emerging advanced semiconductor packaging sector. Advanced semiconductor packaging is increasingly an essential process to achieving the performance demands of many of the emerging applications that are now at the forefront of technological innovation?for example, fifth-generation (?5G?) wireless communication, autonomous vehicles, artificial intelligence and machine learning, advanced sensors, and virtual/augmented reality. Currently, the United States does not have a meaningful onshore semiconductor advanced packaging manufacturing sector, as less than 3% of semiconductor advanced packaging manufacturing occurs in the United States; the nation is overwhelmingly reliant on offshore foundries, primarily in Asia. As the limits of Moore?s Law are reached and packaging becomes a more complex and important part of the semiconductor manufacturing process, this NSF Engine is positioned to capture the economic growth, drive key technological advances in a rapidly-changing sector, and secure our national defense by rooting a vital industry on American shores. In addition to the significant national impacts, the NSF Engine has the potential to create thousands of good-paying jobs in a highly dynamic and innovative technology space. The NSF Engine has already created flagship upskilling and retraining programs that have created ladders into economic opportunity for workers from all educational backgrounds. If successful, the concentration of semiconductor talent, existing physical infrastructure, and research capacity of the region could represent a key national security asset and a new force for economic prosperity in the region.

The broader region of service encompasses a seven-county region in Central Florida centered around Osceola County. This is a strategic location for an emerging semiconductor hub, and the NSF Engine is uniquely positioned for success in part by geography. NeoCity, the heart of the NSF Engine?s efforts, is located in one of the nation?s fastest growing metropolitan areas for STEM talent, with proximity to the Space Coast and one of the nation?s leading technical universities with a strong concentration of the nation?s semiconductor research talent, the University of Central Florida. Additionally, the NSF Engine has built a STEM-focused high school in the NeoCity Industrial Park with specific workforce tracks and internships that feed into various opportunities within the local semiconductor industry.

The region was also the recipient of a Build Back Better Regional Challenge award as well as significant investments from the state of Florida totaling nearly $300 million to position and scale the region?s capacity to serve as a semiconductor hub for the nation. These efforts have been led by ICAMR, Inc. (d.b.a. BRIDG), which is a non-profit public-private partnership with heavy involvement by Osceola County government, R1 academic and nonprofit research institutions (University of Central Florida, University of Florida, imec-USA), a Hispanic Serving Institution and technical community college (Valencia College), a workforce development provider (CareerSource Central Florida), economic and community development organizations (Orlando Economic Partnership, Florida High Tech Corridor), and various industry partners. Each play a significant and complementary role within the broader ecosystem.

Use-inspired research and translation activities by the NSF Engine, both occurring in academic labs and municipally-owned industrial fabrication facilities, stand to advance the current state of practice by advancing digital twin technology as it relates to the semiconductor industry, creating new technology for assured and secured semiconductor packaging, and developing new technologies for the implementation of vertical interconnects and interposers (e.g., through-silicon via, through-glass via, through-substrate via) that would position the NSF Engine at the leading edge of chip packaging design and manufacturing. Leading industry partners in the semiconductor sector have already initiated direct engagement with researchers to inform, test, and scale prototypes that result from research activities, further positioning the region as a central point for research talent and industry partners in the semiconductor sector to crossover and converge to ultimately produce significant advanced packaging innovation.

This NSF Engine ? due to a combination of intentional efforts to train a prepared workforce, unprecedented investments in physical infrastructure (including a county-owned fabrication lab), targeted economic development initiatives, and a regional concentration of experienced semiconductor research and engineering talent ? has a unique capacity and the necessary competitive advantage to succeed and thrive in a highly competitive sector and provide our nation with a key domestic supply chain resource for one of the most important technologies in the world today. It is also poised to create another vital national resource: a highly trained semiconductor workforce. Together with key partners such as Valencia College, the NSF Engine has already begun the essential work of creating pathways to high-paying jobs in the sector for residents from across the region that open opportunity and create economic stability for families up and down the economic ladder. These factors position the NSF Engine to be a powerful engine for U.S. competitiveness, economic opportunity, and advanced semiconductor packaging innovation.

CAS-Climate: CAREER: A Unified Zero-Carbon-Driven Design Framework for Accelerating Power Grid Deep Decarbonization (ZERO-ACCELERATOR)
This NSF CAREER project aims to establish a transformative and convergent zero-carbon-driven framework to add the urgently needed carbon perspective into the design, operation, and control of electric power systems. The project will bring transformative changes to electric power systems, which are heavily dependent on fossil fuels and carbon-intensive processes. This will be achieved by developing and integrating new carbon-driven mechanisms, methodologies, and algorithms into the existing power grid operation paradigm to accelerate its decarbonization. The intellectual merits of the project include: 1) A mathematical framework to model carbon allowance allocation and forward emission trading, 2) the hypothesis of a carbon balance market as a spot market mechanism for maintaining continuous grid cleanliness, 3) load profile-based carbon accounting and forecasting methods, paired with an integrated electricity-carbon digital twin, 4) a game theoretic carbon response program to maximize demand-side carbon reduction. The broader impacts of the project include: 1) Providing an integrative blueprint for policymakers, electricity producers, grid operators, and consumers regarding their respective roles in expediting the decarbonization of the electric power sector, 2) addressing the current knowledge gap and raising awareness of energy transition within the U.S. workforce and classrooms, and 3) strengthening Houston's leadership in the global energy transition.

Outdated assets, well-established regulatory structures, and rigid operational requirements make power system decarbonization challenging. This project aims to establish an urgently needed carbon-driven framework to align with the U.S. government?s commitment to fully decarbonize the power sector by 2035, while maintaining its desired operational characteristics and societal responsibilities. The project will answer the following key research questions: 1) How to design an effective carbon allowance assignment strategy to incentivize emission reduction actions without imposing excessive costs on ratepayers? 2) How can new emission trading mechanisms be integrated into the existing electricity market to facilitate efficient carbon exchange? 3) How can the carbon impacts of consumers' electricity consumption activities be accurately measured and forecasted, and how to design demand-side mechanisms for demand-driven carbon reduction? The project will also train the next-generation energy workforce, and prepare them to understand, engage in, and ultimately, lead the energy transition.

Collaborative Research: CPS: NSF-JST: Enabling Human-Centered Digital Twins for Community Resilience
This US-Japan joint research project aims to apply and expand the concept of ?digital twins? into disaster science and build a ?Disaster Digital Twin? (DDT) which utilizes human-centered data to improve community resilience. The DDT will capture the evolution of a disaster and its impacts to humans, creating an approximate replica in the cyberworld through sensing, computing, and communication. Given that disasters have a disproportionate impact on older adults (across disasters in Japan and the US, nearly 75% of mortality and morbidity burdens are among those over the age of 65), this project will focus on older adults as the key population with personalized care needs in a disaster. The US-Japan team will design tools to create ?Virtual Disaster City? (VDC) with the aim to provide transformative improvements to disaster science by integrating model-driven methods with data driven techniques. As data are continuously updated, so are the models executing within the VDC ? as changes occur, new predictions and recommendations will help drive disaster management decisions.

Specific research tasks of this collaborative US-Japan project include: 1) stakeholder workshops to understand the older adult disaster resilience landscape; 2) integrate diverse data sources (geospatial and human-centric) into a novel data architecture, that supports enrichment and harmonization/alignment of multiresolution spatiotemporal data; 3) execute physics-driven hazard simulations developed by experts in the US/Japan team using the integrated data; and 4) simulate disaster processes and consequences in VDC with target older adult population and relevant hazards on the US West Coast and Tohoku region in Japan to explore optimality of response decisions/policies. The disaster digital twin framework and tools developed will build upon existing, tested technologies by PIs at both countries: multi-agent disaster simulation framework developed in Japan, and he CAREDEX data exchange platform for older adults in the US.

While the project focuses on disaster preparedness for senior populations during hazard events such as tsunamis and earthquakes, the proposed technology has the potential for widespread usage for vulnerable populations in multihazard settings. The project will offer opportunities for PhD and postdoctoral researchers to contribute, as well as involve undergraduates, graduates and minorities to develop and ruggedize systems related to disaster resilience for our vulnerable populations.

Collaborative Research: Sea-state-dependent drag parameterization through experiments and data-driven modeling
The ocean covers nearly 70% of the Earth?s surface and plays a dominant role in the global climate. At the ocean interface, surface waves and their resulting dynamics regulate the transfers of momentum and scalars between the atmosphere and ocean and are thus fundamental in shaping the sea states and weather patterns, exerting a direct impact on many aspects of human life. Although we know surface waves must be fully integrated into weather and climate forecast models, we do not yet fully understand the fundamental processes that couple the surface waves with turbulent flows above and below the ocean surface. A better understanding of wind stress modulations by surface waves is required to reduce uncertainties and develop accurate predictive models. This project aims at advancing the current understanding of wind stress over ocean waves using combined high-resolution imaging and numerical simulations. The outcome of this work will result in tangible broader impacts and societal benefits beyond the scientific community. It will incorporate findings into educational materials for a comprehensive three-day air-sea interaction workshop.

This collaborative project will integrate laboratory measurements of wind-wave interactions with a high-fidelity digital twin model of the laboratory system to develop a data-driven model for sea-surface drag. The specific objectives of the project are to (1) understand skin friction modulations induced by surface waves, (2) evaluate pressure drag through a high-fidelity digital twin model, and (3) develop a sea-state-dependent total surface drag parameterization. Laboratory measurements will provide an accurate description of surface skin friction drag but fall short when it comes to pressure forces. The digital twin model will augment the experimental setup by providing pressure forces. This integrated approach will provide unique insight into wave-induced modulations of the total wind stress (sum of tangential and pressure stresses at the air-water interface) under a range of wind-wave conditions. A data-driven sea-state-dependent surface flux parameterization will be developed by examining these modulations, leveraging recent advancements in machine learning technology. The model will be tailored for large-eddy simulations of wind over ocean wavefields in strongly forced conditions. This approach is expected to significantly advance the fundamental understanding of air-sea fluxes and lead to improved parameterizations of wind stress over the ocean.
CRII: CPS: Towards a Unified Framework for Enabling Live 3D Digital Twins
Digital representations that replicate 3D physical objects at low latency and high accuracy, known as ?live 3D digital twins,? are the next advance in cyber-physics systems. In the transportation domain where vehicles are highly instrumented with 3D sensors that, e.g., include LiDAR and stereo cameras, their data will be continuously streamed and fused into a digital representation that provides accurate real-time situational awareness for the driver and the transportation infrastructure itself, promoting greater safety and efficiencies. This project develops and implements techniques, known as pipelines, to help build and compose digital twin models that are adaptive with guarantees on performance, accuracy, and dependability.

This project develops a framework that enables live 3D digital twins? pipelines to adapt to underlying network and compute resources and ensure low latency and high accuracy across a range of operating conditions. This project develops common abstractions (operators) with which we can compactly build and represent these pipelines. Doing so not only enables the capability to holistically reason about them, but also speeds up the development of pipelines and makes them less error prone. In addition, this project develops a control plane that uses a distributed resource monitor to understand the underlying network and compute conditions.

This research on live 3D digital twins will lead to safer transportation systems as applications such as autonomous driving and drone delivery increasingly become part of future transportation infrastructures. The project prepares students to be part of the future transportation research and development workforce through curriculum that integrates education with this research.

EAGER: CET: Investigating Orbital Motion Dynamics in Offshore Wind Turbines During Installation: A Novel Digital Twin Approach Based on Generative Artificial Intelligence (AI)
This project is jointly funded by the Established Program to Stimulate Competitive Research (EPSCoR), and the NSF-wide Clean Energy Initiative supporting DCL 23-109. This EArly-concept Grants for Exploratory Research (EAGER) award is made in response to Dear Colleague Letter 23-109. The US plans to deploy 30 GW of offshore wind energy by 2030, increasing to 110 GW by 2050. Such deployment is projected to power 10 million homes, create 77,000 new jobs, and reduce carbon emissions by 78 million metric tons. However, significant knowledge gaps exist in understanding the unique dynamics of offshore wind turbines (OWTs) during their installation phases, posing major challenges to their safe and cost-efficient deployment. This high-risk, high-reward EAGER project, led by an interdisciplinary team from the University of Maine, will address a critical challenge related to installing OWT blades, which impedes the adoption and development of offshore wind energy. The team will conduct original research on the orbital motion dynamics in the nacelle of a partially installed bottom-fixed OWT configuration through wave tank experiments and numerical modeling. They will then use a digital twin framework based on generative artificial intelligence to predict the orbital motion dynamics in real-time. The project also aims to advance offshore wind education and workforce training through activities such as integrating new knowledge into existing undergraduate and graduate courses at the university, expanding collaboration with a local high school?s diverse STEM academy by offering K-12 research internships to students from underrepresented communities; preparing an open-source graduate textbook on OWT installation, and mentoring graduate students in partnership with the National Renewable Energy Lab (NREL), Equinor - a global leader in offshore wind farm development, and Kartorium - an Alaska-based digital twin company.

This project explores the root causes of the intricate oscillation orbits in OWTs during the blade installation phase. The current lack of understanding in this area including the absence of a predictive tool compromises the safe and cost-efficient deployment of OWT. The interdisciplinary team from the University of Maine will examine the orbital motion dynamics in the nacelle of a bottom-fixed, monopile-mounted OWT in its hammerhead configuration and identify correlations with variables such as varying sea states, wave headings, and hammerhead configurations. A Froude-scaled model experiment in a wave tank will be performed, developing and employing a groundbreaking methodology for designing a flexible scaled model of an OWT in hammerhead configurations, suitable for wave tank testing. The experiments will be complemented by a high-fidelity numerical model for analyzing these orbital motions and trajectories. The project will use the results from the model scale experiments and the high-fidelity numerical simulations to design, verify, and evaluate a digital twin of the dynamic system based on generative AI algorithms based on a novel physics-informed latent diffusion model to make real-time predictions of orbital motions. Here, domain-specific physics-based knowledge will be integrated into the architecture and training process of the modern diffusion model, enhancing the AI model?s learning capabilities, interpretability, and generalizability.
EAGER: IMPRESS-U Adaptive Infrastructure Recovery from Repeated Shocks through Resilience Stress Testing in Ukraine
This IMPRESS-U project is jointly funded by NSF, Estonian Research Council (ETAG), Research Council of Lithuania (LMT), National Science Center of Poland (NCN), US National Academy of Sciences, and Office of Naval Research Global (DoD). The research will be performed in a multilateral international partnership that unites the University of Florida (US), G.E. Pukhov Institute for Modelling in Energy Engineering of the NAS of Ukraine (PIMEE), Kiyv (Ukraine), National Technical University of Ukraine ?Igor Sikorsky Kyiv Polytechnic Institute? (Ukraine), Institute of Theoretical & Applied Informatics, Gliwice (Poland), Mykolas Romeris University, Vilnius (Lithuania), and Tallinn University of Technology, Tallinn (Estonia). US portion of the collaborative effort will be co-funded by NSF OISE/OD and CISE/CNS.

NON-TECHNICAL SUMMARY
This funding is awarded through an EAGER proposal, supporting the vital development of the Resilience-Recovery Under Attack (RRUA) framework. Originating in response to global disruptions like the COVID-19 pandemic, supply chain vulnerabilities, and geopolitical conflicts, the project holds significant importance. Focusing on Ukraine's digital services sector, which has faced persistent external shocks due to the ongoing conflict, this initiative is both timely and crucial. The primary objective is to establish connections between Ukrainian scientists and their counterparts in the West, mitigating the isolation caused by geopolitical tensions and fostering a collaborative research network dedicated to systemic resilience. Involving an international partnership comprising the USA, Ukraine, Poland, Estonia, and Lithuania, the project adopts an interdisciplinary approach, leveraging network science, resilience analytics, explainable AI, and digital twin technologies. Beyond enhancing Ukraine's infrastructural resilience, the project aspires to serve as a blueprint for global resilience strategies in analogous contexts.
To achieve broader impact, the Dallas-Fort Worth airport will function as the initial RRUA testbed for co-development and training. The RRUA concept and methods will undergo testing in Poland, Estonia, and Lithuania's energy and communications infrastructure, culminating in their application to Ukraine's digital infrastructure. In addition to advancing research goals, this project is steadfast in its commitment to promoting inclusivity in science and engineering. It actively involves junior Ukrainian researchers, facilitating their integration into the global scientific community. Educational opportunities will be offered through digital platforms, workshops, and simulation games, aligning with EU-Ukraine events. These initiatives aim to provide a distinctive learning experience, nurturing a new generation of scientists equipped with the skills to address complex resilience challenges and aligning with the NSF's mission to advance national health, prosperity, and welfare.

TECHNICAL SUMMARY
This NSF EAGER project award aims to advance resilience science by developing the Resilience-Recovery Under Attack (RRUA) framework, with a focus on the unique challenges faced by Ukraine's digital services sector. The RRUA framework introduces an innovative and comprehensive approach to resilience science, targeting the complex interdependencies of interconnected infrastructural systems subjected to dynamic threats and shocks. Hence, the project represents a significant leap from traditional resilience strategies that emphasize prevention, instead integrating recovery as a core component of the resilience paradigm.
We aim to validate the hypothesis that the recovery and resilience of systems under threats and system response stages to diverse shocks can be quantified via stress-testing of interconnected networks representing their systemic functions. We aim to provide novel insights into the lifecycle of resilience under external shocks during acute and persistent shocks, and in particular quantify the key controllers of each of the resilience-response phases to inform efficient recovery interventions. By employing a multi-faceted methodology combining network science, resilience analytics, explainable AI (xAI), and digital twin technologies, the project seeks to redefine systemic recovery modeling and adaptation of interconnected infrastructure across Ukraine, benefiting from the shared knowledge of our proposed international partnership with the USA, Ukraine, Poland, Estonia, and Lithuania. The project utilizes a three-pronged approach: refining RRUA using data-rich analyses at a US-based international airport, testing concepts and methods in Poland, Estonia and Lithuania testbeds, including human behavior components of vulnerability, and subsequently integrating RRUA within Ukraine's cyber and energy infrastructure systems in the presence of dynamic threats and variable data. Success could revolutionize Ukraine's prospects for recovery, positioning it as a global example for resilience strategies. Our research design will collaboratively explore how to operationalize our RRUA framework across varied settings.
EAGER: AI4OPT-AG: Advancing Quad Collaboration via Digital Agriculture and Optimization
The United Nations predicts a global population beyond 9 billion by 2050, requiring at least doubling current food production. However, half of our planet's habitable land is already used for agriculture with little left for new farms. As such, food productivity must increase in order to grow more food within the land we have. Controlled-environment agriculture (CEA) offers a great potential solution ? in this type of indoor farming, plants grow in oxygenated, fertilizer-rich hydroponics rather than soil, allowing growers to optimize plant nutrition. Paired with the precise control of environmental conditions enabled by indoor cultivation, plants can exhibit enhanced productivity in hydroponic systems and can be cultivated at higher areal densities in vertical configurations. Further, agricultural digitization can help improve efficiency, resilience, and sustainability of commercial vertical farming by opening the door to implementation of artificial intelligence (AI) for automation, real-time monitoring and control, and exploitation of massive amounts of data, promising deeper fundamental understanding of both plant physiology and performance within an engineering context. In this project, AI4OPT-AG, established by Georgia Tech, will lead an exploratory multi-national research network with targeted institutions in Japan, Australia, and India to collaborate and accelerate innovations in digital agriculture and optimization, and bolster food security for our future.

High-fidelity digital-twin-based approaches to digitization, and further, in-silico optimization, remain nascent in agriculture and the life sciences more broadly. This cultivation modality presents a useful platform for the study of living, dynamic, cyber-physical systems, with the implementation of hydroponic rather than soil-based configurations enabling direct, near-instantaneous control of the plant growth environment. Two specific next-generation techniques, hyperspectral imaging and gas chromatography fingerprinting of plant signaling molecules, could unlock valuable fundamental insights into plant physiology, but generate massive datasets requiring efficient processing to generate actionable insights. Innovations in digital agriculture, particularly in the engineering and integration of artificial intelligence (AI), biofeedback, and robotics could accelerate both knowledge generation and technology development. The primary challenges associated with this approach arise from the massive quantity of data, and the heterogeneity in data modality, quality, resolution, frequency, and complexity. There is also an opportunity to hybridize modeling approaches beyond purely data-driven or first- principles- based approaches. Such a hybrid approach may improve capabilities beyond those of its individual components, with first principles based physiological models providing fundamental constraints, transparency, and a tether to ground truths, while data-driven approaches provide data augmentation, unparalleled fitting abilities, and efficient management of high dimensional data. We are developing novel approaches to manage, integrate, exploit, and optimize these disparate, and often sparse data streams to enable rapid, computationally efficient techniques to support control and optimization applications. Although robotics and automation are valuable paths towards rapid feedback and high-throughput data collection in vertical farming operations, innovations in AI are crucial to enable rapid, accurate, high-throughput, non-destructive plant status determinations as a platform for performance optimization.

CC* Integration-Small: M2- NET: An Integrated Access and Backhaul Millimeter-wave Wireless Network for Campus Connectivity and Research
George Mason University (Mason) is a diverse public research institution with over 40,000 students. The university's computing portfolio encompasses a wide spectrum of projects, from immersive AR/VR and agro-genomics to vision, geolocation, public safety, digital twin technology, and drone initiatives. The focus of this project, referred to as M2-NET (Mason Millimeter-wave wireless NETwork), is to enable high-speed, robust connectivity to its west campus which is a potential epicenter for research and education. The project will design, deploy, and evaluate a multi-hop millimeter-wave (mmWave) wireless infrastructure that will serve not only as a research platform for a multitude of research and development projects but will also enable wireless connectivity in the west campus.

M2-NET will build a one-of-its-kind large-scale, outdoor mmWave networking and sensing infrastructure that will include unlicensed mmWave 60 GHz multi-hop backhaul, mmWave and sub-6 GHz WiFi access, open-source sub-6 GHz cellular RAN, software radios and mmWave radar sensors. With its capability to provide high-speed low-latency connectivity, ubiquitous sensing, and end-to-end protocol integration, M2-NET will support over a dozen interdisciplinary research projects (such as immersive content delivery, hardening cybersecurity for mmWave networks and drones as first responders) across different departments and centers at Mason. M2-NET will be deployed through novel machine learning based propagation models specifically developed for establishing reliable mmWave links with minimal measurement overhead, along with a high-throughput low-latency topology design that can provide wire-like connectivity with reconfigurability and agility. The infrastructure will result in numerous wireless datasets which will be openly shared with the global research community. M2-NET will serve as the exemplar platform for the evaluation and testing of mmWave backhaul networks for large-scale rural/sub-urban networking deployments, and the potential of addressing the digital divide.

The code repositories, documentation, and other material will be maintained by the M2-NET team. The project website (https://nextgwirelesslab.org/m2-net) will consolidate the M2-NET-related information, including the datasets, available code, documentation, and other related resources. The project repository/website maintenance is planned for several years beyond the initial project timeline through the community ecosystem and follow-up projects.
SBIR Phase I: Solving the 4,023-year-old logistics control problem using modern IoT standards and a novel combination of passive RFID, UWB, cellular technology & their application
The broader/commercial impact of this Small Business Innovation Research (SBIR) Phase I project aims to revolutionize logistics and transport management via real-time visibility at the unserved serial number level through integration of wireless technologies. This research will enhance scientific and technological understanding by providing unprecedented insights into the movement of goods, thereby optimizing transport methods and materials. The market opportunity addressed by the proposed technology is vast, with a value proposition centered around real-time tracking, digital control empowerment, and reduced tracking costs. By leveraging precise on- and off-premise technology with Radio Frequency Identification (RFID), the solution promises to provide a durable competitive advantage. The technology will be offered at a reasonable monthly pricing per unit, unlocking commercial opportunities both domestically and globally. This novel approach is projected to capture a significant portion of the market currently underserved by existing methods. Initially targeting pallets, containers, and commercial vehicles, potential annual revenues of $10M+ are projected in the third year of production, paving the way for expansion into wider domestic and global markets. Ultimately, the technology is poised to be a key factor in enabling commercial success while offering societal benefits such as minimized disruption of production and improved food quality.

This Small Business Innovation Research (SBIR) Phase I project seeks to provide technology for tracking reusable transports (e.g., pallets, containers, trailers, trucks) to reduce disruptions for manufacturers, movers, and package receivers by over 50%. Logistics operation inefficiencies in manufacturing, food distribution, and pharmaceutical delivery could be eliminated, reducing item journey monitoring costs and wasted time. The proposed devices will attach to pallets/containers, read passive RFIDs attached to serial numbered items, and communicate their location/state to a cloud based digital twin for each serial number. This will allow users to monitor products during shipping, updating product whereabouts, condition, and expected delivery timeline, boosting operational efficiency and product safety and creating the potential for circular logistics chains and goods that deliver themselves. This project will de-risk the device using evaluation hardware and demonstrate blink rate optimization feasibility, producing a prototype capable of transmitting and connecting to the monitoring software across various transportation types. The R&D will include reference design-based device testing focusing on battery longevity, chip power usage evaluation, and deployable antenna design for multi-item reads. The result will be an early prototype capable of being tested in an industrial pallet configuration for modular battery implementation in post-Phase I efforts.
I-Corps: Translation Potential of Digital Twin and Artificial Intelligence for Building Maintenance Management
The broader impact of this I-Corps project is the development of a scalable and adaptable technology utilizing digital twins and artificial intelligence (AI) to build maintenance management. The built environment accounts for approximately 45% of global carbon emissions and comprises almost 75% of building operating costs. Cutting-edge technologies such as digital twins and artificial intelligence (AI) are promising for synthesizing big data from sensors, automation systems, maintenance activities, and building occupancy. The proprietary technology of this I-Corps project will transform building operations and maintenance practices into a data-driven structure to predict possible failures and defects of critical building systems with simulations for risk scenarios. Together with the operational benefits, this technology will enhance occupant comfort, safety, and well-being, as well as improve the longevity, durability, and sustainability of the built environment. Overall, the technology will lead to buildings that are smarter and more responsive.

This I-Corps project utilizes experiential learning coupled with a first-hand investigation of the industry ecosystem to assess the translation potential of the technology. The solution is based on the development of a technology for building maintenance management that utilizes a digital twin that connects various data resources such as building automation, controls, and sensors to maintenance management systems with artificial intelligence (AI) to predict possible failures and defects with simulations for risk scenarios. The technology enables the interoperability of various data sets and provides simulations of potential failures with risk scenarios such as cost, safety, and impact on critical systems. Predictive models improve efficiency by utilizing big data to uncover trends and anomalies that are obscured in manual observation methods. The combination of digital twin technology and AI opens a wide range of possibilities to visualize building information based on operational data. Action plans support the decision-making processes with effective resource allocation and reduced equipment downtime.
Collaborative Research: CPS: Medium: Real-Time Crowd-Sourced Geospatial Digital Twin for Cyber-Physical Systems
This research project focuses on enhancing the way vital information is delivered to smart mobile devices?such as smartphones and tablets. With the advancement of technology, there is a growing necessity for these devices to receive various types of information (like images, videos, and texts) instantly and effectively. One promising approach to achieving this is through the use of Geospatial Digital Twins (GDT), which are digital models of physical environments. GDTs are becoming increasingly important as they allow for real-time updates and interactions, making them invaluable for various applications such as monitoring, maintenance, and emergency response. Traditionally, data for GDTs has been collected through automated systems like distributed sensor devices, satellites and drones. However, these methods have limitations, especially when it comes to updating data quickly and covering hard-to-reach areas. To overcome these challenges, this project will develop a novel approach that involves the community through ?human-in-the-loop? strategies. This means using crowd-sourced data, where people provide real-time updates to digital models. This method not only promises to enhance the accuracy and timeliness of the information but also to allow discovery of new information. The project has the potential to revolutionize how we interact with and understand our physical world, potentially making this work a cornerstone for further scientific and educational advancements. The project will also play an important role in education, integrating research findings into university curricula and offering unique learning opportunities for students, including students from underrepresented groups.

The goal of this project is to establish an intellectual foundation for building a real-time crowd-sourced GDT. To achieve this goal, we will work toward a fundamental understanding of crowd-sourced multi-modal information collection and processing to account for the underlying human incentives and human-machine integration, which underpin the foundation of crowd-sourced GDT. In this project, we will investigate the design of crowd-sourced GDT to ensure timely, truthful, and unbiased imagery data collection from the crowd. Our efforts will be organized around four tightly integrated research thrusts: 1) ensuring crowd-sourced data freshness for a GDT; 2) integrating crowd-sourced data for real-time GDT updates; 3) guaranteeing truthful reporting in crowd-sourced data collection; and 4) mitigating self-reinforcing bias in crowd-sourced GDT updates. Collectively, this project will result in new tools for optimization and control that directly contribute to real-time crowd-sourced GDTs.
Collaborative Research: SaTC: CORE: Small: Towards Secure, Resilient, Privacy-enhancing Digital World Experiences
The metaverse ecosystem, an integrated fusion of physical, digital, and virtual realities, presents a plethora of advantages for both research and industry, including accelerated system development, enhanced productivity, and global real-time interactions. Despite extensive research efforts toward implementing metaverse systems across various domains, a notable deficiency exists in addressing the critical security and privacy issues within this immersive mixed-reality landscape. The intersection of virtual and physical realms gives rise to distinctive security concerns, encompassing inter-realm adversarial attacks creating counterfeit representations, the risk of information leakage across distributed virtual reality devices, and the exposure of user privacy at both the network and application levels. These less-recognized threats arise from the intricate interplay and added complexity of the digital realm, coupled with the reliance on cutting-edge technologies like digital mapping, machine learning, and data analysis in mixed environments. In response to these challenges, the project?s novelties are pioneering strategies to fortify the metaverse ecosystem, with the goal of creating secure, resilient, and privacy-enhanced digital world experiences. The project's broader significance and importance span digital twin networks, manufacturing, and automation testing, where the developed security and privacy prevention techniques can be employed in various data analysis tasks, including medical data processing, road traffic prediction, user mobility, and trajectory predictions. The project integrates the research insights into new modules for computer security and privacy courses and hosts outreach activities like Data Privacy Week, with the vision of advancing the participation of underrepresented minorities in STEM fields and improving STEM education.

This project addresses security and privacy challenges in emerging metaverse ecosystems, drawing from interdisciplinary knowledge in cyber-physical systems, machine learning, traffic analysis, and usable privacy. The project places emphasis on advancing three interconnected facets: 1) Developing innovative defense mechanisms to protect digital mapping and synchronization against adversarial manipulation, including a resilient digital representation approach and a mask-and-trim strategy to replace adversarial inputs during the synchronization process. 2) Creating novel privacy-preserving distributed training methods that enable collaborative ML model training for virtual reality users. This involves quantization-based federated learning methods to optimize learning accuracy, data leakage, bit quantization, and energy consumption. 3) Introducing novel privacy controls for virtual reality end users. This entails developing techniques to thwart behavioral inference both at the network layer (through traffic) as well as the application layer (through embedded sensors) using obfuscation techniques such as differential privacy. The overall design aims to establish a safe and trustworthy digital environment for users worldwide.
CIRC: Dev: Development towards a Community Research Platform for sub-THz Satellite Communication Networks
Non-terrestrial networks (NTNs) can provide ubiquitous coverage and resilient connectivity, but currently only with achievable data rates far from the expectations of 5G networks and 6G forecasts. Terahertz (THz) band technology (0.1-10THz) has recently been envisioned as a potential enabler of high-rate space-based NTNs. This project is aimed at exploring the development of the world?s first community research platform for sub-terahertz satellite communication networks. The platform plans to have two twin small satellites with a software-defined sub-terahertz radio platform that will enable numerous inter-satellite communication experiments, as well as a ground station to investigate the feasibility of ground-to-satellite access links.

This collaborative project brings together interdisciplinary investigators from Northeastern University (NU) and Morehead State University (MSU), including accomplished experts in THz communications, wireless networking, mechanical and aerospace engineering, systems engineering, and satellite development. This two-year effort will focus on developing and testing space-qualified sub-terahertz radios, which will integrate world-record transmit power front-ends at 225 GHz with an ultrabroadband programmable digital signal processing engine. In addition, the satellite bus requirements, including the dimensions and weight of the small satellites, the electrical power system capacity, mechanical interfaces, and deployment procedures, will be analyzed. Moreover, a digital twin simulator will be designed to accurately replicate the entire infrastructure operation and provide extensive inputs to the mission planners.

This project's developmental work could lead to better global connectivity. High-throughput sub-THz NTNs will bridge the Digital Divide by providing reliable high-speed internet to remote and underserved communities. Additionally, the inherent resilience of satellite-based systems offers a reliable communication backbone for critical operations during both natural and human-driven instabilities and catastrophes. Beyond societal impact, the project will contribute to international NTN standardization efforts and spectrum policy development. Furthermore, project findings will be integrated into interdisciplinary courses at both universities, fostering future generations of researchers.

The project website, http://www.thz-sat.com, will be a one-stop shop for everyone interested in broadband non-terrestrial networks at terahertz frequencies. The website will provide an overview of the field, the latest publications by the team, and the up-to-date status of the infrastructure development, including all the major milestones towards the eventual launch and operation of this unique infrastructure.

Research Infrastructure: CIRC: New: Full-stack Codesign Tools for Quantum Hardware
Quantum information science (QIS), at the intersection of physics, mathematics, and computer science, is pioneering new frontiers in computing and communication, promising unprecedented capabilities. However, the multidisciplinary complexity of QIS demands expert knowledge in diverse areas, from cutting-edge experimental physics to theoretical computer science. This project develops an open-source ecosystem of modeling tools for quantum hardware that bridges the gap between these extremes. The project novelties are founded upon creating user-friendly tools that allow researchers to model quantum systems without mastering a vast array of underlying techniques. Among the project impacts are enabling a network scientist to test the performance of a networking protocol on realistic hardware models without having intricate knowledge of quantum simulation algorithms or, conversely, a hardware engineer to optimize their hardware sitting at the bottom of a full-stack network application modeled for them. As a whole, these tools will democratize access to quantum technology, foster innovation, and accelerate the growth of QIS.


The project revolves around the development of a symbolic algebra system for backend-agnostic modeling of quantum systems, capable of marshaling multiple backend simulators for different formalisms, seamlessly translating symbolic representations into numerics. The most appropriate simulation methods is automatically selected for each scenario. The investigators employ recent advances in scientific machine learning and auto-differentiation (even over discrete random functions) to provide the necessary foundation for constructing simulator systems of unprecedented sophistication. Emphasizing the importance of co-design, the project incorporates cutting-edge optimization and digital twin tooling, allowing for holistic optimization of quantum hardware and network dynamics.

SCC-IRG Track 1: Community-Responsive Electrified and Adaptive Transit Ecosystem (CREATE): Planning, Operations, and Management
This Smart and Connected Community (S&CC) project supports research that aims to foster a Community-Responsive Electrified and Adaptive Transit Ecosystem (CREATE) to tackle interrelated challenges that arise in the planning, operations, and management of public bus fleet electrification. Public bus fleets, including transit and school buses, represent a prime opportunity for transportation electrification, and associated improvements in environmental quality and health benefits in impacted communities. Widespread adoption of electric buses has been hindered by an array of complex and interrelated planning, operational, and managerial challenges. Some major ones are range limits, long charging time, high capital expenses, low bus utilization ratios, equipment downtime, underdeveloped workforce, and diverse stakeholder interests and priorities. To overcome these hurdles, this project adopts a holistic approach by integrating intelligent technology development with community needs, while prioritizing environmental justice (EJ) and transportation justice (TJ) in research and solution design. A suite of intelligent decision support tools will be developed and deployed to support a scalable, transferable, and sustainable path for electric bus transition. This project will also assess collaborative governance in public bus fleet electrification planning and policymaking. An advisory council consisting of key stakeholders in public bus fleet electrification will be established to guide all stages of the project. In collaboration with industry and community partners, this project will further contribute to the development of the workforce to facilitate a sustainable future for electrified public bus transportation. Outputs from this project will include a report and clearinghouse on lessons learned, resources and best practices to guide other public transit agencies and school systems in their fleet electrification efforts, thereby accelerating the nationwide transition to electric buses. The project outcomes will advance EJ and TJ, benefiting marginalized communities that have long been harmed by diesel-exhaust pollution.

The overarching research goal of this award is to tackle interrelated challenges that arise in the planning, operations, and management of public bus fleet electrification. The project will be conducted in close collaboration with local transit agencies, public schools, local municipalities, utility companies, electric bus distributor and maintenance service providers, and national laboratories. The multidisciplinary team will devise innovative technologies to support analytical and practical needs of community partners. Strategic planning for electric bus fleets differs fundamentally from conventional fleet planning because decisions such as charging capacity and charger locations profoundly influence fleet performance due to the much-reduced range and longer refueling time. A holistic approach is thus devised to integrate operations into the strategic planning for bus fleet electrification. It will develop an integrated strategic and operational planning framework guided by EJ and TJ. Innovations in graph machine learning and learning-enabled stochastic optimization will enable stakeholders to address the complexity, nonlinearity, uncertainty, and data scarcity challenges associated with bus electrification. Daily operations of electric bus fleets present hurdles to achieving high utilization of electric buses and chargers while maintaining optimal state of charge ranges, all without compromising the reliability of bus services. Adaptive operations decision support capabilities will be developed to address those operational challenges. Those include new smart predict-then-optimize, digital twin-based real-time decision making, and transfer learning algorithms to dynamically optimize fleet charging, incident response, and maintenance, thereby achieving the overarching goal of high vehicle utilization and competitive total ownership cost. The project will culminate in a pilot program to deploy CREATE Suite, a set of intelligent decision support tools, to facilitate public bus fleet electrification efforts at the project?s public transit systems and school districts partners. At the conclusion of this project, an electric bus clearinghouse will be established as a centralized platform to support bus electrification.
Remote Experimentation and Digital Twinning for Accessible and Innovative Learning (REDTAIL)
This project aims to serve the national interest by enhancing learning and promoting understanding of lab concepts through the innovative use of digital twin technology - the integration of traditional simulation-based engineering models with real-world remote 3D hardware models. Students spend a significant amount of time using programmable devices such as microcontrollers and microprocessors. A common challenge for them is the visualization of those devices as part of a real-world system rather than just circuit boards with switches and blinking LED lights (Light Emitting Diodes). This 36-month IUSE Level 2 Engaged Student Learning project, titled ?Remote Experimentation and Digital Twinning for Accessible and Innovative Learning (REDTAIL)", promotes the democratization of immersive access to advanced STEM devices and equipment to students located in different cities, states and even countries. This is particularly significant for students from underrepresented and underserved communities, where the project is expected to have a substantial impact. REDTAIL, led by the Remote Hub Lab research group at the University of Washington, is to be leveraged to design, test and evaluate an innovative digital twin infrastructure for students of electrical engineering, computer engineering, and computer science. The project offers an ecosystem that is expected to overcome the limitations of traditional lab sessions by enabling students to interact with real devices in simulated real-world scenarios, thereby enhancing their grasp of fundamental engineering concepts. REDTAIL makes immersive learning globally accessible through an established network by LabsLand, a global network of remote laboratories that connects institutions to equipment located worldwide. The project's diverse team includes Navajo Technical University, LabsLand, and the University of Wuppertal and Hochschule Bonn-Rhein-Sieg University in Germany.

Researchers will develop a REDTAIL Simulation Repository (RSR), which will host the simulations from within the Cloud. In order to ensure that communication between the simulation and the actual device has no latency, each simulation comprises two parts: one running on the web (visualization of simulation) and another in a controller physically connected to the target device. Additionally, researchers will develop an open source REDTAIL Development Kit (RDK) to be used by third party developers for creation, test and upload of simulations to the RSR repository. REDTAIL is centered around innovative pedagogy, blending remote experimentation with 3D simulations to enrich curricula, making them more engaging and closely aligned with real-world scenarios. Beyond its immediate scope, REDTAIL offers scalability and an open-source framework, making immersive learning globally accessible through the LabsLand network. The research planned will contribute to fundamental knowledge in three areas: 1) the integration of remote experimentation into engineering curricula; 2) the effective combination of remote experimentation with 3D simulation representation; and 3) the impact of these innovative technologies and pedagogies on faculty teaching and student learning. Project evaluation will involve collection and analysis of data at the participating institutions for measurement of student performance, engagement, satisfaction, and the overall educational impact of the REDTAIL infrastructure and teaching approach. Insights derived from evaluation will be used to further improve and enhance the ecosystem. The university's Office of Educational Assessment researchers will work with the principal investigator (PI) and project team to develop formative and summative evaluation instruments to monitor and assess the impact of the intervention. The NSF IUSE: EDU Program supports research and development projects to improve the effectiveness of STEM education for all students. Through the Engaged Student Learning track, the program supports the creation, exploration, and implementation of promising practices and tools.
RAISE: CET: Building the Fusion Future - Identifying and Bridging the Workforce Gap for the Emergent Fusion Energy Industry
This Research Advanced by Interdisciplinary Science and Engineering (RAISE) award is made in response to Dear Colleague Letter 23-109, as part of the NSF-wide Clean Energy Technology initiative. The transition to a sustainable and clean-energy future requires both rapid expansion of existing renewable technologies and expedited development of emerging energy sources. Fusion energy, a promising and potentially scalable solution, could become a vital part of overcoming this energy challenge. With the emerging fusion private sector, successful fusion energy commercialization could play a critical role in driving towards net-zero emissions targets and strengthening energy security; however, achieving this goal requires a diverse and skilled workforce. This project aims to address the anticipated technical and engineering skill gap in the emerging fusion energy industry. It does so through a comprehensive education and workforce development plan composed of digital and online education resources as well as hybrid digital-through-hands-on experiential education and training methods that can scale to support an informed citizenry and diverse STEM workforce nationwide.

The first objective of this project is to identify the educational requirements precipitated by the energy sector's inclusion of fusion technology. Subsequently, this project will develop an innovative fusion technology curriculum, enriched with virtual reality simulators including a digital twin of the Columbia University Tokamak for Education device. The project will develop and disseminate a hybrid curriculum that aims to enhance the proficiency of students and incumbent workers, fostering the cultivation of a competent, diverse fusion technician and engineering workforce. A cross-disciplinary hybrid curriculum will be employed, integrating classroom, virtual, and hands-on learning experiences to expand and diversify the pool of fusion technicians and engineers. To support these goals, this project seeks to foster strong collaborations by forging new connections between educators, researchers, national laboratories, national and regional workforce development entities and the wider fusion industry. This consortium will facilitate the dissemination of curriculum to a broader audience, ensuring extensive adoption and spread. The project will directly support training of undergraduates, PhD students, and postdocs in the development of educational tools and curricula, as well as engage a broad range of students at the collegiate and K-12 levels through testing and application of the developed resources.
NSF/FDA SiR: Development of a Regulatory Science Tool for Virtual Design of Novel Ventricular Pump Strategies
This NSF/FDA Scholars-in-Residence project is focused on ventricular assist devices (VADs), which are heart pumps used to support patients with severe heart failure. Many patients die each day while waiting for a heart transplant, and VADs can be a life-saving alternative. However, evaluating their safety and effectiveness is complex due to the dynamic interaction with each patient?s unique heart condition, posing significant challenges for the United States Food and Drug Administration (FDA). The goal of this project is to create a new computational model to predict the complex interactions of the VAD with the beating native heart. This model will simulate real-life conditions by integrating data from clinical studies, benchtop experiments, and computer simulations, offering a more precise method for predicting VAD performance. This model will enable the FDA to make better-informed regulatory decisions and support the development of safer, more effective VADs tailored to individual patient needs. This tool will drive improvements in VAD technology, ultimately enhancing patient outcomes and reducing deaths among those waiting for transplants. As part of the project, a graduate course on medical devices will be updated to include the use of computational models in regulatory science, and an FDA internship for an engineering student will be offered. These activities ensure that future engineers and regulators are equipped to advance VAD technology. This research has the potential to transform heart failure treatment by improving VAD performance and tailoring their performance to individual patient needs. It could reduce the number of deaths among those waiting for a heart transplant and lead to better patient outcomes.

This research will result in a lumped parameter model (LPM) for predicting dynamic pressure and flow for a ventricular assist device (VAD) when coupled with the native human heart and circulation. The LPM will be a digital twin of an experimental mock circulatory loop (MCL) used for testing VAD safety and efficacy. A system of differential equations derived from an electrical circuit representation of the MCL are solved for a range of conditions representing heart failure patients and combined with the VAD performance curves to generate dynamic pressure and flow waveforms. These waveforms can be used to identify safety concerns such as pressure spikes and turbulent flow early in the design or review process. Verification, validation and uncertainty quantification of the LPM will be performed following FDA guidelines and published standards in preparation for use in VAD regulatory review and design. The LPM resulting from this project will aid industry and FDA via the science-based and transparent results and contribute to the integration of computational models into the regulatory review process.

PFI-TT: Developing a Multi-purpose Container for Large-Scale Production of 2D Materials
The broader impact of this Partnerships for Innovation - Technology Translation (PFI-TT) project is to develop a multi-purpose container and associated digital twin software for the large-scale production of two-dimensional (2D) materials, a critical component in advancing next-generation electronic materials. By enabling more efficient and cost-effective growth of 2D materials, this innovation has the potential to have commercial impact, as the semiconductor industry and research centers can greatly benefit from reduced experimental costs, time savings, and lower emissions of harmful byproducts. The technology will allow a wide range of users to design experiments that evaluate the feasibility of growing specific 2D materials and thin films and optimize their synthesis set-ups, thereby fostering innovation and facilitating the development of new technologies. The multi-purpose container and software allows new materials to be explored. Additionally, the software will be used in advanced courses, and a web-based version will enhance education for K-12 students through hands-on research experiences.

The project focuses on developing an advanced, configurable, multi-purpose container and its digital twin software based on an experimentally validated multi-physics model of atomically thin materials. The software will enable the design and adjustment of parameters for the container, which will be used to build prototypes for wafer-size growth of 2D materials at commercial scales. The digital twin software will simulate the effects of different control parameters on the vaporization of materials and their deposition in atomically thin layers, optimizing the synthesis setup with fewer experiments. This project addresses the challenge of reliably and reproducibly synthesizing 2D materials over large areas, which is essential for their industrial applications. Current methods often suffer from obstruction of the growth and reduced reproducibility upon subtle variations in growth conditions and synthesis setup. By developing a software package that integrates computational materials science and engineering, this project aims to create a digital twin for the synthesis of advanced materials and overcome these challenges, enabling the large-scale formation of 2D materials. This project will enable the wafer-scale realization of atomically-thin materials and air-stable, crystalline, 2D superconductors, laying the foundation for advanced electronics and future quantum technologies.

This project is jointly funded by the Partnerships for Innovation (PFI) Program, and the Established Program to Stimulate Competitive Research (EPSCoR).

E-RISE RII: Research Center for Distributed Resilient and Emergent-Intelligence-Based Additive Manufacturing
This initiative is a collaboration among four minority-serving institutions: New Mexico State University, the University of New Mexico, New Mexico Institute of Mining and Technology, and Navajo Technical University. The project will build the underpinnings for an advanced distributed intelligent additive manufacturing (DIAM) infrastructure. The broad vision of this project is to make foundational innovations in distributed networking, cybersecurity, and digital-twin design in additive manufacturing (AM) leading to the creation of robust research programs in AM in New Mexico. This, in turn, will spur the creation of a diverse, well-trained workforce that will make New Mexico competitive in AM. The project will establish the Center for Distributed Resilient and Emergent intelligence-based Additive Manufacturing (DREAM) to make distributed AM a reality. This initiative aligns with New Mexico?s Economic Development and Science & Technology plans. The resulting democratization of manufacturing will spur inclusive economic growth in the state and contribute to national efforts to onshore manufacturing. The project capitalizes on several strengths in New Mexico. The state has a diverse higher education system, collaborative federal laboratories, an engaged private sector, and state funding interests aligned with the DREAM project.

The proposed research will establish the groundwork for DIAM by developing the networking and security framework essential to AM and Industry 4.0 contexts. DREAM aims to develop a successful research center, utilizing expertise from various sectors, to cultivate an advanced AM infrastructure. The Center will become an epicenter of expertise in intelligent AM driven by the following intellectual merit: (1) assessing architectural challenges in DIAM networking and security, and proposing a scalable, cloud-edge continuum blueprint through software virtualization and containerization; (2) addressing security and trust needs in DIAM, and proposing frameworks to overcome challenges; (3) ensuring verifiability and auditability of DIAM; and (4) building a novel distributed testbed infrastructure with a digital twin to refine networking, security, and communication processes for DIAM support. Collaborations with research universities across the state, national laboratories, and industry will contribute to the project?s long-term success. While AM educational initiatives are currently found in college-level engineering courses, DREAM's efforts will be unique. The project will provide an integrated pathway, starting with middle school and continuing to the doctoral and post-doctoral levels. Additionally, DREAM's educational models will be intertwined with research experiences and infused with pedagogical models that promote diversity, inclusion, and belonging. This project is poised to make significant contributions to the fields of AM, cybersecurity, and education, ultimately strengthening New Mexico's economic and technological landscape. This project is funded by the NSF EPSCoR Research Incubators for STEM Excellence (E-RISE) RII Program. The E-RISE RII Program supports the development and implementation of sustainable broad networks of individuals, institutions, and organizations that will transform the science, technology, engineering and mathematics (STEM) research capacity and competitiveness in a jurisdiction within a field of research aligned with the jurisdiction's science and technology priorities.
RII FEC: Accelerating Community-Centric Energy Transformation through AI-driven Digital Twinning for Climate-Aware Resilience
This project addresses the urgent challenge of climate change intertwined with aging energy infrastructure in the United States. This is a critical challenge most acute in underserved communities. The project aims to enhance local research infrastructure across four EPSCoR jurisdictions?New Mexico, Montana, Oklahoma, and Alabama. In turn, these regions will benefit from advancements in AI, digital twin technology, and renewable energy. By developing AI-driven digital twins tailored to the energy infrastructures and socio-economic needs of three representative underserved communities (Kit Carson, Mora-San Miguel, and the Navajo Nation), the DigiCARES project will optimize energy utilization, integrate renewable sources more effectively, and improve overall climate resilience. This project not only advances the field of energy system planning but also supports education and workforce development, promoting diversity in STEM fields through active collaboration with minority-serving institutions and initiatives such as New Mexico State University?s Pre-freshman Engineering Program, The University of Alabama in Huntsville Research and Engineering Apprenticeship Program, and Oklahoma State University?s NSF-sponsored Oklahoma Louis Stokes Alliance for Minority Participation. These efforts align with NSF?s mission to promote the progress of science, advance national health, prosperity, and welfare, and secure national defense by addressing energy equity and sustainability challenges. The project will include extensive outreach activities to engage underserved communities in co-developing these technologies, ensuring their specific needs and perspectives shape the outcomes. The approaches and outcomes of DigiCARES in AI and digital twins, especially in addressing climate resilience, will be applicable to various communities, scalable to other regions in the US, and potentially globally.

DigiCARES embarks on a transformative approach in energy systems planning and operation, leveraging AI-driven digital twins to address the nexus of climate, energy, and community. The overarching goals of this project are to enhance energy efficiency, reduce energy burden, integrate renewable sources, and fortify climate resilience in underserved communities. The project integrates diverse scientific disciplines including data science, climate science, AI, sociology, and energy policy, to create interoperable datasets, interactive tools, and community-centric platforms. These digital twins will provide in-depth analysis and strategy testing for energy systems under various future scenarios. The research encompasses six key activities: multi-scale climate dynamics, sociodemographic energy mapping, community-centric planning and operation, AI-driven digital twin development, pilot studies in three communities, and translating energy insights into interactive visual narratives. The project will actively engage the communities in Kit Carson, Mora-San Miguel, and the Navajo Nation in the co-development process, ensuring that the technologies meet their specific needs and are effectively implemented. Through these activities, DigiCARES aims to generate advanced models, open datasets, and strategic decision frameworks, providing scalable solutions and setting a new benchmark for interdisciplinary research in climate resilience and sustainable energy systems. These efforts will significantly contribute to building a resilient and equitable energy future, supporting the nation?s net-zero emission goals. This project is funded by the EPSCoR Research Infrastructure Improvement-Focused EPSCoR Collaborations (RII-FEC) program. The RII-FEC program builds inter-jurisdictional collaborative teams of EPSCoR investigators in focus areas consistent with the NSF Strategic Plan. RII-FEC projects include researchers from at least two EPSCoR eligible jurisdictions with complementary expertise and resources necessary to address challenges, which neither party could address as well or as rapidly independently.
Collaborative Research: FDT-BioTech: Aspects of Digital Twin Studies for Neuroimages
Neurodegenerative diseases (for example, Alzheimer's disease, Parkinson's disease, multiple sclerosis) impact millions of people in the United States and result in hundreds of thousands of deaths. These disorders can affect people of all ages, although they are more common in older adults. Digital twin models, leveraging the exponential growth of biomedical data and artificial intelligence and data science techniques, are opening exciting avenues to obtain new insights into these diseases and revolutionize their treatment and prevention. The investigators will address multiple problems on this interface, and develop data science-driven theoretical foundations, methodological tools and algorithmic principles for several aspects of digital twin models towards better understanding of digital twins as a whole, and in particular in the context of their use in neuroscience and in prevention, treatment and better understanding of neurodegenerative diseases. They will also address the ethical, legal, and social implications of using digital twin models in the context of healthcare in general, and in studying neurodegenerative diseases using magnetic resonance-technology driven images (MRI) in particular. This research will greatly aid in the deployment of digital twins in medical and healthcare practice, and will significantly advance neuroscience and the study of neurodegenerative diseases.

The investigators will address open problems in low-dimensional manifold learning, causal pathway searches and feature discoveries and selections, and develop multiple techniques for verification, validation and uncertainty quantification of digital twins using Bayesian techniques, data assimilation, resampling, empirical likelihood methods and topological data analysis. They will also develop dynamical system models, incorporating observational image data, for computational efficiency and synthetic data generation for ethical use of artificial intelligence and digital twin technology in studying neurodegenerative diseases. Additionally, they will develop knowledge graph driven systems for use by regulatory and other healthcare monitoring agencies for de-risking and easy implementation of data-driven modern technologies. The investigators will work in conjunction with regulatory and other healthcare governing agencies towards better understanding of neurodegenerative diseases and successful deployment of data-driven technologies to mitigate suffering from such diseases. The investigators will mentor, train and teach students on various aspects of digital twins, data science and neuroscience and their interconnections, and will help build a highly skilled workforce on these topics.

Collaborative Research: TRTech-PGR: Digital Ideotype for Optimal Canopy Architecture
Maize breeders have significantly increased crop yields by optimizing plants for higher planting densities. Further improvements in crop productivity per unit of land can be achieved by modifying the structure of individual plants and their arrangement in fields. Our current technologies allow for scanning large quantities of plants, but the acquired data is often underused. The goal of this project is to use the scanned data to create a virtual model of maize (its digital twin), which will capture the maize geometry, reflectivity, and function. The digital twin will be able to simulate real plant growth and response to the environment by careful verification against the measured data. The digital twin will be used in hypothetical scenarios of changing climatic conditions to answer "what-if" questions, providing answers for better plant architecture and planting distributions. By using AI and automatic optimization, this project will attempt to identify genetic markers and candidate genes governing variation in the same traits, enabling efforts to breed or engineer plants with optimal canopy architectures. This innovative approach will advance our understanding of plant biology and contribute to meeting global food demands.

This project takes an important step towards in silico optimization of maize canopy architecture. We propose to develop innovative data processing and advanced visualization tools to generate fundamental knowledge applicable to agriculture to advance food needs. Our tools will reconstruct maize into its digital twins (plant ideotypes), simulate configurations of individual plants and plant populations differing in leaf canopy-related traits, and evaluate how plant traits perform in varying environments. We will use the vast amount of gathered data from phenotyping facilities and gantry to reconstruct 3D plants into their simulation-ready digital twins, fine-tune computer simulations to visualize and optimize the plant structure and function and identify optimal canopy architectures for given sets of conditions. This work will be combined with genome-wide association study for leaf canopy architecture traits derived from 3D reconstructions of real populations to identify markers and candidate genes, enabling efforts to breed or engineer plants producing optimal canopy architectures. The results of this work will strongly impact agronomic and plant genetic research in both the public and private sectors. There is a critical need for models to predict how plant varieties will respond to different environments. The 3D interactive application will allow experimenting with complex situations at interactive frame rates on a standard desktop computer, something never achieved before. It will be connected to existing data pipelines that provide vast amounts of (often unused) data. We will develop a set of novel algorithms that reconstruct 3D maize plant shapes and functions from input data from varying sources (RGB, depth, point clouds). The developed system will also generate synthetic data suitable for AI training (labeled sets of plants and 3D geometries with proper lighting). The project will partner with The National Data Mine Network, an NSF-funded initiative and the Computer Science department at Purdue University to engage and recruit students in phenotyping, data analysis, algorithmic design, and deployment.
MATH-DT: Advancing Digital Twins for Jet Engines Through Mathematical and Computational Innovation
This project advances predictive digital twins of jet engines. The digital twin is a virtual object representing the jet engine and will be used to inform decisions such as design, optimization, operation and maintenance. This is crucial to enhancing aircraft safety whereby the digital twin enables a proactive approach to identifying and resolving potential issues, and to scheduling preventive maintenance. The digital twin allows a better understanding of the physical behaviors that an engine would exhibit under many operational scenarios, some too dangerous for physical experimentation. The research directly engages undergraduate students through teaching laboratories and a more in-depth engagement program targeted toward students from underrepresented and under-served groups in engineering. The project also involves training of doctoral students in Computer Science, Mathematics, and Engineering.

The overarching goal of this project is to expand the mathematical foundations of digital twins with application to jet engines, and to increase their predictive simulation capabilities by fusing information from advanced modeling and state-of-the-art measurements. It develops high-fidelity multiphysics models for jet engine combustion and flow, as well as scalable reduced-order models, both with quantified uncertainties. Particle-surface interaction models are constructed to quantify erosion and deposition effects on engine performance. An array of novel hierarchical data assimilation algorithms are developed using variational approaches, ensemble Kalman filters, and transport map particle filters, all in the context of a hierarchy of models. An innovative experimental setting at the Virginia Tech Advanced Propulsion and Power Laboratory with a JetCatP100-RX engine allows testing with the physical twin.

Collaborative Research: TRTech-PGR: Digital Ideotype for Optimal Canopy Architecture
Maize breeders have significantly increased crop yields by optimizing plants for higher planting densities. Further improvements in crop productivity per unit of land can be achieved by modifying the structure of individual plants and their arrangement in fields. Our current technologies allow for scanning large quantities of plants, but the acquired data is often underused. The goal of this project is to use the scanned data to create a virtual model of maize (its digital twin), which will capture the maize geometry, reflectivity, and function. The digital twin will be able to simulate real plant growth and response to the environment by careful verification against the measured data. The digital twin will be used in hypothetical scenarios of changing climatic conditions to answer "what-if" questions, providing answers for better plant architecture and planting distributions. By using AI and automatic optimization, this project will attempt to identify genetic markers and candidate genes governing variation in the same traits, enabling efforts to breed or engineer plants with optimal canopy architectures. This innovative approach will advance our understanding of plant biology and contribute to meeting global food demands.

This project takes an important step towards in silico optimization of maize canopy architecture. We propose to develop innovative data processing and advanced visualization tools to generate fundamental knowledge applicable to agriculture to advance food needs. Our tools will reconstruct maize into its digital twins (plant ideotypes), simulate configurations of individual plants and plant populations differing in leaf canopy-related traits, and evaluate how plant traits perform in varying environments. We will use the vast amount of gathered data from phenotyping facilities and gantry to reconstruct 3D plants into their simulation-ready digital twins, fine-tune computer simulations to visualize and optimize the plant structure and function and identify optimal canopy architectures for given sets of conditions. This work will be combined with genome-wide association study for leaf canopy architecture traits derived from 3D reconstructions of real populations to identify markers and candidate genes, enabling efforts to breed or engineer plants producing optimal canopy architectures. The results of this work will strongly impact agronomic and plant genetic research in both the public and private sectors. There is a critical need for models to predict how plant varieties will respond to different environments. The 3D interactive application will allow experimenting with complex situations at interactive frame rates on a standard desktop computer, something never achieved before. It will be connected to existing data pipelines that provide vast amounts of (often unused) data. We will develop a set of novel algorithms that reconstruct 3D maize plant shapes and functions from input data from varying sources (RGB, depth, point clouds). The developed system will also generate synthetic data suitable for AI training (labeled sets of plants and 3D geometries with proper lighting). The project will partner with The National Data Mine Network, an NSF-funded initiative and the Computer Science department at Purdue University to engage and recruit students in phenotyping, data analysis, algorithmic design, and deployment.
ENG-NETZERO/Collaborative Research: Digital Twin Solutions for Energy Provision Logistics in Three-Sided Electric Mobility Markets
This award will fund research that attempts to establish a new framework for digital twin modeling of urban energy delivery strategies. This framework is grounded in logistics and platform economics and leverages both model-based and data-driven approaches, along with a living lab and industry collaboration. The research project aims to address fundamental challenges in integrating electric mobility into urban infrastructure, ensuring efficient energy delivery while promoting sustainability and economic viability. The project's significance lies in its potential to enhance urban energy efficiency, reduce greenhouse gas emissions, and support the integration of renewable energy sources in a manner that inclusively serves all communities. By fostering collaboration between academia and industry, this research will contribute to the advancement of science and technology in urban energy systems. Additionally, it will support education by providing students with hands-on experience and exposure to interdisciplinary topics in energy and mobility with real-world data. The project also aims to promote diversity in STEM fields by engaging a broad range of students and researchers.

The research consists of three thrusts designed to address the complexities of urban energy delivery through digital twin modeling. The first thrust will design a three-sided market modeling framework that captures the interactions between travelers, mobility providers, and energy providers. This framework will incorporate mechanisms from logistics and platform economics to understand and optimize the dynamics within electric mobility markets. The second thrust focuses on developing a scalable digital twin environment to support comprehensive analysis and decision-making for these ecosystems. This environment will integrate both model-based and data-driven approaches, enabling the simulation and evaluation of various strategies and their impacts on urban energy delivery. The third thrust aims to create an "energy delivery playbook" by identifying diverse strategies through industry collaboration and testing them in theoretical and living lab environments. This playbook will provide guidelines and best practices for implementing effective energy delivery solutions in urban settings. The research will establish fundamental bounds on the achievable performance of three-sided electric mobility markets and develop tools and algorithms with guaranteed performance metrics. By combining theoretical analysis with practical application, the project will offer robust solutions to enhance the efficiency and sustainability of urban energy systems. The integration of this research into education through a dedicated software platform will further extend its impact, offering students valuable experience and fostering the next generation of infrastructure experts.
CPS: SMALL: NSF-MeitY: 5G Enabled Real-Time Digital Twins of Dynamic Construction Sites
This Cyber-Physical Systems (CPS) grant, a collaboration between the US National Science Foundation and the Ministry of Electronics and Information Technology of the Government of India (NSF-MeitY), supports research to develop a digital twin of dynamic environments such as construction sites using multiple unmanned aerial and ground robots equipped with cameras and 5G radios. This research can (a) enhance worker safety by identifying potential hazards and (b) improve construction efficiency by monitoring and optimizing resources devoted to different tasks. Technology emerging from this project can bring increased productivity in the construction industry, which has lagged behind other sectors such as manufacturing or agriculture. It can lead to affordable and green housing without compromising labor safety. To achieve these goals, the researchers will collaborate closely with multiple industry partners, and ensure public sharing of robotic data. This project will also work with the Philadelphia School District, which serves a primarily low-income and underrepresented minority population, to develop school curricula involving robotics and computer vision.

This project will build photometric, geometric, semantic, and radiometric representations of dynamic scenes using techniques from neural radiance fields and multi-modal visual-language features. Information-theoretic formulations of active perception and path planning will be used for monitoring construction progress and safety of human workers, using multi-robot teams. These algorithms will be run on aerial and ground robots along with edge servers that communicate periodically with these size, weight and power (SWaP)-constrained platforms using the latest standards in 5G. Using these algorithms, human workers carrying 5G radios can be localized using their radiometric signatures. Effectively, this project envisions a real-time perception and control stack executed on a heterogeneous multi-robot team. This research will be demonstrated using real-world experiments in Philadelphia and Bangalore (India).

MATH-DT: Advancing Digital Twins for Jet Engines Through Mathematical and Computational Innovation
This project advances predictive digital twins of jet engines. The digital twin is a virtual object representing the jet engine and will be used to inform decisions such as design, optimization, operation and maintenance. This is crucial to enhancing aircraft safety whereby the digital twin enables a proactive approach to identifying and resolving potential issues, and to scheduling preventive maintenance. The digital twin allows a better understanding of the physical behaviors that an engine would exhibit under many operational scenarios, some too dangerous for physical experimentation. The research directly engages undergraduate students through teaching laboratories and a more in-depth engagement program targeted toward students from underrepresented and under-served groups in engineering. The project also involves training of doctoral students in Computer Science, Mathematics, and Engineering.

The overarching goal of this project is to expand the mathematical foundations of digital twins with application to jet engines, and to increase their predictive simulation capabilities by fusing information from advanced modeling and state-of-the-art measurements. It develops high-fidelity multiphysics models for jet engine combustion and flow, as well as scalable reduced-order models, both with quantified uncertainties. Particle-surface interaction models are constructed to quantify erosion and deposition effects on engine performance. An array of novel hierarchical data assimilation algorithms are developed using variational approaches, ensemble Kalman filters, and transport map particle filters, all in the context of a hierarchy of models. An innovative experimental setting at the Virginia Tech Advanced Propulsion and Power Laboratory with a JetCatP100-RX engine allows testing with the physical twin.
ENG-NETZERO/Collaborative Research: Digital Twin Solutions for Energy Provision Logistics in Three-Sided Electric Mobility Markets
This award will fund research that attempts to establish a new framework for digital twin modeling of urban energy delivery strategies. This framework is grounded in logistics and platform economics and leverages both model-based and data-driven approaches, along with a living lab and industry collaboration. The research project aims to address fundamental challenges in integrating electric mobility into urban infrastructure, ensuring efficient energy delivery while promoting sustainability and economic viability. The project's significance lies in its potential to enhance urban energy efficiency, reduce greenhouse gas emissions, and support the integration of renewable energy sources in a manner that inclusively serves all communities. By fostering collaboration between academia and industry, this research will contribute to the advancement of science and technology in urban energy systems. Additionally, it will support education by providing students with hands-on experience and exposure to interdisciplinary topics in energy and mobility with real-world data. The project also aims to promote diversity in STEM fields by engaging a broad range of students and researchers.

The research consists of three thrusts designed to address the complexities of urban energy delivery through digital twin modeling. The first thrust will design a three-sided market modeling framework that captures the interactions between travelers, mobility providers, and energy providers. This framework will incorporate mechanisms from logistics and platform economics to understand and optimize the dynamics within electric mobility markets. The second thrust focuses on developing a scalable digital twin environment to support comprehensive analysis and decision-making for these ecosystems. This environment will integrate both model-based and data-driven approaches, enabling the simulation and evaluation of various strategies and their impacts on urban energy delivery. The third thrust aims to create an "energy delivery playbook" by identifying diverse strategies through industry collaboration and testing them in theoretical and living lab environments. This playbook will provide guidelines and best practices for implementing effective energy delivery solutions in urban settings. The research will establish fundamental bounds on the achievable performance of three-sided electric mobility markets and develop tools and algorithms with guaranteed performance metrics. By combining theoretical analysis with practical application, the project will offer robust solutions to enhance the efficiency and sustainability of urban energy systems. The integration of this research into education through a dedicated software platform will further extend its impact, offering students valuable experience and fostering the next generation of infrastructure experts.
CPS: SMALL: NSF-MeitY: 5G Enabled Real-Time Digital Twins of Dynamic Construction Sites
This Cyber-Physical Systems (CPS) grant, a collaboration between the US National Science Foundation and the Ministry of Electronics and Information Technology of the Government of India (NSF-MeitY), supports research to develop a digital twin of dynamic environments such as construction sites using multiple unmanned aerial and ground robots equipped with cameras and 5G radios. This research can (a) enhance worker safety by identifying potential hazards and (b) improve construction efficiency by monitoring and optimizing resources devoted to different tasks. Technology emerging from this project can bring increased productivity in the construction industry, which has lagged behind other sectors such as manufacturing or agriculture. It can lead to affordable and green housing without compromising labor safety. To achieve these goals, the researchers will collaborate closely with multiple industry partners, and ensure public sharing of robotic data. This project will also work with the Philadelphia School District, which serves a primarily low-income and underrepresented minority population, to develop school curricula involving robotics and computer vision.

This project will build photometric, geometric, semantic, and radiometric representations of dynamic scenes using techniques from neural radiance fields and multi-modal visual-language features. Information-theoretic formulations of active perception and path planning will be used for monitoring construction progress and safety of human workers, using multi-robot teams. These algorithms will be run on aerial and ground robots along with edge servers that communicate periodically with these size, weight and power (SWaP)-constrained platforms using the latest standards in 5G. Using these algorithms, human workers carrying 5G radios can be localized using their radiometric signatures. Effectively, this project envisions a real-time perception and control stack executed on a heterogeneous multi-robot team. This research will be demonstrated using real-world experiments in Philadelphia and Bangalore (India).
CMMI-EPSRC: Tackling New Simulation and Optimization Challenges Towards Self-Organizing Manufacturing Digital Twins
This research supported by this NSF Civil, Mechanical and Manufacturing Innovation/UKRI Engineering and Physical Sciences Research Council (CMMI-EPSRC) award aims to advance simulation-based manufacturing process digital twin (DT) technologies by developing a self-organizing DT framework that continuously validates and calibrates the simulator and controls the manufacturing system with minimal human intervention. Despite the recognized importance of simulation-based DT for decision-making under uncertainty, significant barriers exist in its application, particularly in the continuous alignment of simulation models utilizing several key performance indicators observed from the manufacturing system and the need for quick, optimal control responses during contingencies. This research will directly address these challenges by making significant contributions to the mathematical foundations of DTs and enhance the competitiveness of US and UK manufacturers through industry collaborations. Real-time manufacturing DT technology can significantly promote national welfare by enhancing the efficiency, reliability, and resilience of manufacturing processes. This leads to more consistent and higher quality products at a lower cost, benefiting consumers and increasing the competitiveness of domestic industries. The simulation DT technology is applicable to other critical sectors such as healthcare and defense. Therefore, the methodological advancements achieved from this research can also benefit public health and national security.

The specific goals of this research are establishing and verifying mathematical and algorithmic frameworks for 1) online validation of the DT with multidimensional multi-epoch data, 2) self-calibration of the DT simulator, 3) optimal control for contingency scenarios, and 4) parallel computing for rapid optimization. For the online validation, a hypothesis test that incorporates multi-dimensional multi-epoch data to detect statistically significant discrepancies between the model-generated and the system KPIs will be created. If the DT simulator fails the hypothesis test, then the online calibrator is automatically triggered. The calibration will be formulated as a simulation optimization problem that minimizes a statistical distance between the distributions of the simulated and system KPIs. To solve this problem efficiently, a new ?batch-then-project? Bayesian optimization (BO) algorithm will be established that can efficiently tackle high-dimensional problems. To utilize a calibrated simulator in online contingency responses, the DT requires a simulation optimization algorithm that finds the optimal set of categorical actions efficiently without enumerating all possible combinations. This research will explore embedding actions on a graph to measure the similarity between two sets of actions and exploiting it to make statistical inference on the optimality. To provide a practical solution in real time, all algorithms will be designed to utilize parallel computing.
SII-NRDZ: MITRE NRDZ-as-a-Service Field Deployment
In this project, MITRE collaborates with Northeastern University, University of Colorado Boulder, University of Utah, and the SETI Institute to conduct long-duration dynamic spectrum sharing experimentation as the Engineering and Execution Lead (EEL) for the NSF National Radio Dynamic Zones (SII-NRDZ) program. Dynamic spectrum sharing is adaptive coexistence using techniques that enable multiple electromagnetic spectrum users to operate on the same frequencies in the same geographic area without causing harmful interference to other users. Dynamic spectrum sharing is essential to sustain the benefits of spectrum access for many sectors of society, including faster communications, new astronomical and scientific discoveries, more energy-efficient cities, increased highway capacity and safety, and more accurate weather predictions. The MITRE-led team, called SPARKIE, advances the use of spectrum sharing through maturing software solutions and conducting field experiment campaigns that generate scientific data and build stakeholder trust, and through releasing reusable spectrum sharing software. SPARKIE team activities also provide hands-on training, curriculum development, and recorded online content to expand the cadre of future spectrum managers, engineers and scientists in the spectrum field.

Initial experimental campaigns use the Open Zone Management System (OpenZMS) software package from the University of Utah to manage dynamic spectrum sharing. The initial experimental campaigns include experiments at a radio astronomy facility, the Hat Creek Radio Observatory (HCRO) in California, for spectrum sharing with nearby 900 MHz ISM band smart utility meters; and at a facility for experimentation on advanced communications systems, the Platform for Open Wireless Data-driven Experimental Research (POWDER) in Salt Lake City, Utah, for spectrum sharing with nearby 5G cellular systems operating in the 3.55-3.70 GHz Citizens Broadband Radio Service band. Risk of the initial experimental campaigns is reduced by pre-deployment experiments in a digital twin that utilizes the Colosseum testbed of Northeastern University. Data from the initial campaigns informs selection of a zone management system solution and sites and applications for subsequent capstone experiments. The capstone experiments are planned to last at least four months of continuous spectrum sharing, demonstrating prevention of harmful interference while enhancing spectrum access for facility operations through sharing spectrum between the facilities and nearby users. Other outcomes from the project include a toolkit for building zone management systems, a cloud-based service for safe field tests of zone management systems, contributions to the Dynamic Sharing System Testbed specified in the 2023 National Spectrum Strategy, and initial definition of an envisioned future National Radio Dynamic Zone.

Collaborative Research: MATH-DT: Mathematical Foundations of Quantum Digital Twins
This project develops, analyzes, and deploys Quantum Digital Twins (QDTs), which are digital clones of existing quantum computers. Built within a comprehensive mathematical and statistical framework, these QDTs will enable bidirectional interactions between quantum computers and virtual models on classical systems, optimizing quantum performance and marking a significant step toward achieving the proverbial Quantum Leap in computational abilities. This advancement will help maintain the United States' leadership in quantum information science and technology, supporting the National Quantum Initiative Act and producing next-generation quantum-enabled technologies for sensing, information processing, communication, security, and computing. Additionally, the project establishes foundations that can enhance other Digital Twin technologies across various fields, from energy to health. It will also facilitate the interdisciplinary training of young scientists in modern data-driven computational methods and the experimental and theoretical aspects of quantum devices and digital twins, with outreach efforts to local communities and Native American tertiary colleges.

The QDTs developed in this project aim to overcome the limitations of traditional quantum simulations, which use a linear component-by-component approach, by introducing four key advancements: (i) the first-ever mathematical formulation of QDTs grounded in a Bayesian probabilistic framework, addressing the inherently probabilistic nature of quantum devices, (ii) new randomized Bayesian experimental design techniques tailored for QDTs, capable of handling the complex dynamics and uncertainties in quantum systems, (iii) a robust generalized Bayesian framework using optimal transportation theory with adaptive prior and model enrichment mechanisms, enabling QDTs to detect and correct their flaws while minimizing system downtime, and (iv) advanced risk-neutral techniques for quantum optimal control and validation, improving QDTs' ability to generate high-fidelity quantum gates. The project also integrates these algorithms and methods into existing open-source software products, demonstrating and disseminating the developed QDTs.
EAGER: CHIRRP: Responding to Groundwater Depletion and Building Resilience by Examining Emerging Social-Environmental-Technical Structures
In recent decades, groundwater resources in major U.S. agricultural regions have reached critical levels of depletion. Such significant groundwater depletion causes a multitude of hazards, including agricultural and drinking water shortages, land subsidence, and reduced ecosystem health. Groundwater sustainability is essential for improving the viability of the agricultural sector and for supporting the resilience of farming communities. This project is focused on enhancing the resilience of groundwater resources through a participatory planning approach. Conventional water resources management practices based on Earth system models alone are often insufficient to address these sustainability challenges, partially due to the lack of appropriate tools for engaging stakeholders. To cope with these challenges, the project team will develop an artificial intelligence (AI) based modeling system that empowers farming communities to address groundwater depletion and assess resilience outcomes under different climate and management scenarios. Key research activities include community engagement and AI-based integrated model development to replicate social, hydrological, and technical dynamics in an adaptive groundwater management system. It will directly benefit farming community members by providing new collaborative planning tools for developing groundwater sustainability strategies.

The overarching goal of this project is to create a new paradigm of integrated social-Earth science models that place the community at the center of the design process. The project will develop a semi-structured digital twin (DT) that empowers farming communities to collaboratively tackle groundwater depletion and examine resilience strategies and outcomes under existing and emerging social-environmental-technical structures. The team will develop a graph network to represent the complex topological structures of multifaceted social, hydrological, and technical subsystems, and employ machine learning and deep learning models to simulate intricate hydrological processes and support community planning initiatives. The project will address critical research gaps, including (1) how changes in dynamic interactions among hydrological, climatic, technology use, and irrigation processes affect the equilibrium of groundwater storage, (2) how farming communities effectively participate in collaborative planning and co-design groundwater management strategies, and (3) how research endeavors and community knowledge and priorities can be bridged for actionable solutions to confront groundwater depletion. The project will advance the understanding of groundwater susceptibility and irrigation system resilience to various exogenous and endogenous challenges through the lens of social-environmental-technical systems. By equipping communities with participatory planning tools, the project will facilitate effective groundwater sustainability planning and solution development, enhance scientific communication among stakeholders, and promote responsible groundwater usage for a sustainable and productive future.
Collaborative Research: Frameworks: Differentiable Dynamic Simulation on Complex Geometries for Parameter Inference, Design Optimization and Control
Many physical simulation applications can be viewed as building ?digital twins? of real systems, i.e., computer models that enable studying physical phenomena computationally, avoiding the costs and risks associated with physical experiments. Differentiable simulation allows automation of two critical aspects of digital twin creation and use, improving the quality of the result and democratizing digital twin use: integration of real-world data, and in the case of engineering systems, optimization of system parameters to achieve a particular goal. Examples include identifying realistic material parameters of a patient-specific biomechanical digital twin or discovering the optimal shape of a shoe sole for uniform load distribution. This project will develop open-source software for differentiable simulation for systems involving elastic deformations with contact. These tools will be evaluated in three major application areas: (computational fabrication, biomechanics, and robotics). The deliverables of this project will be open-source software packages accessible to a broad user base.

The project plans to utilize dPolyFEM, a modular software framework for design, control, system parameter inference, and learning problems for physical phenomena in material design, biomechanics, and robotics, based on differentiable simulation. The focus is on developing robust, efficient, and scalable software blocks for differentiable simulation that can handle input data satisfying only weak assumptions (e.g., on mesh quality, shape, or boundary conditions) and require no parameter tuning while providing users sufficient control over performance-accuracy trade-offs. The project will support the most common class of physical problems in the target domains: elastodynamic problems involving complex geometry, large deformations, contact, and friction. For scalability, dPolyFEM will provide shared-memory parallelization. This system will consist of several modules that can be used independently or in an integrated way, enabling easy integration of its components into existing general-purpose and domain-specific software. From a technical standpoint, this system will build on three innovations: (1) considering differentiable simulation as a single end-to-end problem including meshing, FE solution, and adjoint formulation, (2) casting the time-integration of physical systems as an energy minimization, for which robust solvers can be developed, and (3) systematically testing the system on large-scale benchmarks

The resulting open-source differentiable simulation framework will enable applications in many fields of interest to NSF. The project team includes computer scientists (CISE), applied mathematicians (DMS), and engineers (ENG), and it is expected that the contributions will have an impact on all three communities. Individual modules can and will be integrated into major open-source projects, likely benefitting tens of thousands of users.
FuSe2: Topic 3: AI-Enhanced Material-Device Codesign of Boron Arsenide as the Next-Generation Semiconductor
Nontechnical description
High-quality semiconducting materials have been the driving force for the information and computing revolution in the past century. The rapidly increasing integration density, operational speed and power of electronic devices require highly efficient dissipation of heat from operating devices in order to reduce the risk of overheating and thermal failure. This project aims to develop a next-generation semiconducting material ? cubic boron arsenide (BAs) ? with intrinsically high electrical and thermal conduction properties. Preliminary studies have suggested that BAs can conduct heat at least 10-times better than silicon, in addition to having superior electrical conduction and optoelectronic properties. Despite its promising properties, currently, high-quality BAs can only be made in very small crystals using an inefficient growth method. The research team plans to develop new methods to grow BAs in form factors that are relevant for practical applications, such as thin films and large single crystals, and understand how defects can modify its properties. In addition, the research team focuses on developing a co-design platform to optimize material property and device design simultaneously using artificial intelligence. This study aims to demonstrate prototypal BAs devices building upon these fundamental advancements. This project also supports educational and research activities to train the next-generation semiconductor industry workforce with combined skills in theory and experiment. The team plans to achieve this goal by directly training graduate student researchers, incorporating research progress into new hands-on courses, hosting undergraduate researchers with a diverse background, and engaging industrial partners.

Technical description
The overarching goal of this collaborative project is to develop BAs as the next-generation semiconductor that combines an ultrahigh thermal conductivity, high bipolar charge mobilities, and a long hot photocarrier lifetime for microelectronic and optoelectronic applications. The project aims to bridge the knowledge gap between fundamental electron and phonon interaction properties in BAs and their impact on coupled electrical and thermal transport and practical device applications. The core strategy to achieve this goal is rooted in the principle of material/device codesign enabled by a "digital twin" of BAs-based devices that is powered by physics-integrated deep learning. To complement this platform, the research team plans to develop a mesoscopic modeling framework to establish theoretical understanding of electrical and thermal transport properties in BAs, synthesize and characterize high-quality bulk crystals and thin films of BAs with state-of-the-art techniques including high-pressure flux growth and molecular beam epitaxy, and obtain experimental data and knowledge that can feed back and refine the digital twin. Furthermore, the research team aims to develop novel experimental methods capable of directly probing electron and phonon interaction with point and extended defects at the atomic level and systematically examine doping strategies and heterostructures to enable practical device applications of BAs. The project paves the way for BAs to become a practical new semiconductor material and provides fundamental insights into transport and defect physics in emerging semiconductors with unusual electron and phonon structures.

Collaborative Research: FDT-BioTech: Promoting Healthy Cardiovascular Aging through Personalized Multiscale Digital Twins
Cardiovascular aging, which involves various structural and functional changes in the vascular, valvular, and ventricular systems, is a significant risk factor for heart diseases and associated morbidities. These conditions typically progress silently and become noticeable only in advanced stages, making early detection and intervention crucial. Traditional diagnostic methods, which rely on symptoms to guide further testing, are increasingly challenged by a growing patient population and a short-staffed clinical workforce. This project aims to transform aging care by developing personalized digital twin technology that will integrate data from wearable devices, echocardiographic measurements, and advanced cardiovascular modeling. This initiative will enhance the monitoring and understanding of cardiovascular aging through noninvasive methods, allowing for better therapeutic interventions. The digital twin technology will have broad applications in healthy aging care and disease monitoring. Additionally, the project will provide educational opportunities in mathematics, scientific computing, and biomedical science, promoting diversity and inclusion in STEM fields. Outreach efforts will emphasize the importance of healthy lifestyles and scientific literacy to the broader community

The central theme of this project is to utilize computational and animal models to develop a physics-based personalized digital twin for monitoring and understanding cardiovascular aging. By integrating subject-specific simulations, artificial intelligence, and multiscale noninvasive data, the digital twin will enhance insights into cardiovascular health. The project will focus on developing a data-driven, physics-informed digital twin for real-time monitoring and prediction of aging-related cardiovascular diseases, using mechanics-based markers. Leveraging advanced modeling techniques, scientific machine learning, and noninvasive measurements, this project aims to fill significant knowledge gaps and create a transformative tool for personalized healthcare. The development of novel algorithms for handling multiscale, multimodal data is anticipated to enhance the understanding of mechanical changes associated with cardiovascular aging. Expected outcomes include a physics-based digital tool with predictive capabilities, validated against animal models, offering the potential for early detection and intervention in aging-related cardiovascular diseases.

CIVIC-PG Track A: Create an Ethical Urban Digital Twin to Co-design Heat Mitigations for Integrated Indoor and Outdoor Environments
The objective of this Civic Innovation Challenge (CIVIC) project is to support research focused on designing and piloting an Ethical Urban Digital Twin (EUDT) tailored for San Antonio's Westside neighborhood ? a historically disadvantaged Hispanic community. Collaboration between the University of Texas at San Antonio, Texas A&M University, the Historic Westside Residents Association, and the City of San Antonio seeks to provide holistic solutions to mitigate heat-related problems. With rising temperatures, measures like enhanced insulation, air conditioning, and natural ventilation are crucial to individuals? health. However, past studies often investigate indoor or outdoor spaces separately, overlooking the inherent connection between the two and limiting the offering of integrated solutions. While digital twin holds promise to bridge the gap between indoor and outdoor spaces for thermal comfort evaluation, they generates ethical concerns related to privacy, transparency, and fairness that might lead to undue burden on disadvantaged communities. The broader significance of this research project lies in its potential to serve as a scalable model for aiding other communities facing similar socioeconomic and climate challenges.

In Stage 1, the research project centers on building an Ethical Urban Digital Twin for disadvantaged communities to mitigate heat risk. Academic partners will use privacy-preserving sensors and algorithms to prototype digital twin models of indoor and outdoor environments for homes, offices, and businesses. Low-cost PurpleAir sensors will be installed to gather real-time microclimate data, leveraging past work and existing assets in the Westside neighborhood. Community engagement will involve workshops and interactive sessions with residents and stakeholders to co-design the EUDT and address ethical concerns. The integrated models intend to enable simulation of various environmental scenarios to assess thermal comfort and identify effective mitigation strategies. The anticipated outcomes of this research include: (1) an integrated indoor and outdoor digital twin prototype powered by real-time environmental sensors; (2) community feedback on the ethics of digital twin prototype and the feasibility of risk mitigation solutions; (3) a Stage 2 work plan ready for immediate implementation.

Collaborative Research: Enhancing Managed Spectrum Sharing with FR3-Cognizant Digital Twins
The unprecedented growth of wireless connectivity and increasing demand for high data rates necessitate a transition to the FR3 frequency band (7-24 GHz), which offers the combined benefits of extensive coverage, high capacity, and ultra-fast data rates. This project addresses the critical need for enhanced Spectrum Access Systems (SAS) within the FR3 band to ensure efficient coexistence among its diverse applications, such as mobile satellite services, radio astronomy, and various federal and commercial operations. The project undertakes the development of a precise wireless digital twin to accurately model signal propagation and enable spectrum access and management systems to improve spectrum sharing requests for general users, as well as identify optimal bands or channels at any given time and location, allowing more effective channel requests from the access systems. Additionally, it facilitates more efficient spectrum allocation in densely populated areas, minimizing interference and maximizing spectrum efficiency. The PIs will develop both individual and group-based research projects related to this proposal, actively recruit female and underrepresented students for research, and provide research opportunities for K-12 students through their institutions' outreach programs.

This research project is organized into three thrusts. The first thrust focuses on building a wireless digital twin to capture the physically-largest, static features of the environment using 3D scene representation and ray tracing tools to simulate signal propagation. This validates the core capability of accurately identifying signal transmission paths and channel quality. The second thrust aims to develop a dynamic wireless digital twin that accounts for environmental dynamics, such as human movement and changes in object positions, by optimizing real-time channel updates. The third thrust ensures the digital twin adheres to spectrum sharing regulations, prioritizing incumbent users while integrating real-time spectrum usage data to enhance predictive capabilities. This research provides a comprehensive framework for managed spectrum sharing in the FR3 band, fostering the deployment and performance of next-generation wireless networks.
Integrating Vision-Language Models towards Autonomous Manufacturing in Extreme Environments
In the realm of fully autonomous data-driven manufacturing, generalizability of solutions facilitated by Artificial Intelligence (AI) is critical for scalable solutions. While significant progress has been made in industrial automation, transitioning from engineering requirements that begin with the manufacturing process plan specification to specific robotic control for manufacturing operations still requires significant manually configured input parameters. These manually configured steps limit advances in data driven autonomous manufacturing, particularly within extreme environments, such as in outerspace, underwater, biological and radioactive environments. This project establishes a systematic and generalizable methodology to integrate vision language models and robotics to fully automate manufacturing assembly operations. The project will provide solutions for the automated extraction of process task descriptions from engineering documentation, an integrated multi-agent task planning algorithm which assigns tasks and commands to robots with a digital twin guided real-time feedback evaluation system. The project will facilitate the integration of engineering design specifications, manufacturing process requirements, and robotic systems to improve productivity, especially in extreme environments, aligning well with the US National Strategy on the development and use of Artificial Intelligence in Manufacturing. The integration of education and research will broaden participation in manufacturing by training the next generation of engineers and researchers at the intersection of manufacturing processes and robotics systems.

This project investigates an end-to-end generalizable task planning framework for robots in manufacturing environments by filling the knowledge gap between abstract process engineering design instructions and low-level robot control, leveraging the capability of vision-language models (VLMs) for high-level multimodal reasoning, and developing a customized task planner paired with a digital twin (DT) for iterative evaluation. The project consists of three highly integrated thrusts. First, it involves interpreting Product Manufacturing Information and process instructions to derive high-level task sequences from various input forms such as text prompts, technical engineering drawings, 3D layouts, and structured data. This information is processed using a vision-based language model, which integrates computer vision and natural language processing to generate task sequences. The second thrust addresses automated sub-goal planning in single or multi-robot manufacturing scenarios through a language-integrated task planner. This planner selects optimal sub-goals and assigns task primitives to robots. Finally, the project focuses on validating and correcting plans using the digital twin, which provides sensor feedback and evaluates trajectories before physical realization. Advanced machine learning methods will be employed to ensure the validity of instructions sent to the manufacturing environment's physical asset endpoints.
CIVIC-PG Track A: Incorporating large-area imaging and analytics into community-driven underwater restoration and management
As the climate changes, coastal communities are putting greater value on the ability of nearshore marine ecosystems to dissipate storm surges and protect human infrastructure and livelihoods. Thus, coastal ecosystem stewardship and restoration is increasingly viewed as a smart ecologic and economic investment in ?green? infrastructure. This Civic Innovation Challenge (CIVIC) planning process brings together university scientists, community representatives and businesses, and civic entities to co-design a science/research-based, implementable, scalable, and sustainable solution that tackles a critical challenge: Coral reef degradation and destruction due to climate change. This is a challenge that needs to be addressed to increase island and tropical community resilience and protection from sea level rise and erosion caused by increased storm frequency and intensity. This project, and its planning team, work to address the coastal protection problem by employing advanced technology for the monitoring and restoration of coral reefs using the U.S. Virgin Islands as a pilot. For island communities, like those in the Virgin islands, coral reefs are the natural and premier frontline barrier that mitigates onshore destruction of harbor installations, homes, businesses, and other installed infrastructure from damaging wave action and storm surges. For this CIVIC planning process the science team from the Scripps Institution of Oceanography brings new technology (large-area imaging) to the table, working with the community-based U.S. Virgin Island Restoration of Coral Squad, to co-create an implementation plan for deploying an innovative coral reef monitoring and technology training regimen to keep track of local coral reef degradation and monitor the effectiveness of community reef restoration efforts. Civic partners engaged in this effort include local businesses, non-profits, and government agencies who want to work with academic partners who have expertise in techniques that can help them in their fight to maintain health coral reefs around their islands. Together, the scientists and community are co-designing with what could be an effective model for community-university exchange, scalability, and sustainability for community use of new technology (i.e., large-area imaging) and utilizing its benefits for improving coastal community resilience. Broader impacts of the work include deployment of a novel technology to enable more informed means of monitoring and checking the health of coral reefs and efforts to maintain them in a warming ocean world and training of local residents and civic partners in the use and utility of the technology and working closely with university scientists. The work will also engage students from the U.S. Virgin Islands.

The project involves the use of a technology new to ocean science: Large-area imaging technology (a.k.a. photogrammetry). It can enhance and accelerate coastal restoration by creating a visual ?digital twin? that can be used to gauge the relative success of reef degradation and the effectiveness of different restoration approaches. This CIVIC planning process brings together a network of scientists, a novel technology, and accessible online tools and image processing techniques to help island communities, like those in the US Virgin Islands, better monitor fringing coral reef health. The value of large-area imaging is that it provides a visual understanding of the results of outcomes of the interaction between rising ocean temperature and coral reef ecosystem health. While this technology offers immense potential, financial and technical limitations have prevent widespread uptake outside of academic circles. Surveys of individuals working in marine ecosystem stewardship and restoration have documented systemic inequities in access to emerging technologies such as large-area imaging. These challenges are linked, in some cases, to financial constraints; but many are linked to uneven investment in user training and in access to the technology and technology-enabled workflows. The goal of this collaboration and planning process is to have scientists and end-user stakeholders identify impediments to access in the context of coral restoration in the US Virgin Islands. A multi-stakeholder meeting in the US Virgin Islands will provide community-wide perspectives that will help in the co-design of potential solutions. This will result in access to advanced technology and training in its use to accelerate the learning and technology implementation needed to build more climate-ready marine ecosystems. This planning process will improve the understanding how community-based efforts can be designed to provide improved nature-based solutions to climate change. It will also foster and strengthen collaboration between researchers and community stakeholders, develop new collaborations and partnerships, refine the research vision to enable submission of a successful follow-on proposal that will implement the community vision, and provide data to address research questions and develop evaluation methods and measures for the follow-on project.

Planning: SCC-CIVIC-PG Track B: Manufacturing Automation Toolkit (MAT): Enhancing Equity through Collaborative Visioning in Manufacturing Future
Robotics and AI technologies have rapidly reconfigured workplaces. The manufacturing industry is at the forefront of these changes. Although these technologies have the potential to significantly reduce labor intensity and improve work efficiency, workers often lack the resources to learn about, envision, and prepare for their integration into daily work, leading to concerns and missed opportunities for process innovation and empowerment. This project offers a novel approach to engage the workforce through technology innovations, learn about these technologies, and simulate workplace change. This stage 1 planning grant is conducted in cooperation with United Auto Workers (UAW) Local 602, the State of Michigan, Rockwell Automation, manufacturing industry partners, and manufacturing community.

The stage 1 planning grant focuses on understanding production workers? concerns through interviews, observation, and building multidisciplinary teams as the project stakeholders develop the technical requirements for (1) a ?digital twin? model for visualizing how new automation technologies would change the workflow and that allows workers to experience the change in a virtual environment, (2) a generative AI that answers workers? questions about automation technologies, and (3) a catalog of emerging automation technologies in manufacturing, made accessible to workers for exploring and learning about new technologies. The stage 2 project would build a prototype system and pilot it at automative manufacturing facility.

This project is in response to the Civic Innovation Challenge program?s Track B. Bridging the gap between essential resources and services & community needs and is a collaboration between NSF, the Department of Homeland Security, and the Department of Energy.
SCC-CIVIC-PG Track B: Data-Driven Monitoring and Optimizing of Right-of-Way Permits
Right-of-way closures, i.e., the closing of streets, bike lanes, sidewalks, and roads, frequently occur in urban areas due to construction projects, cargo delivery, and special events. Dynamic construction schedules, lack of compliance, and potentially outdated technological systems hinder the ability of city transportation authorities to effectively issue, monitor, and inspect permits, leading to compliance violations and traffic disruptions. The rate of permit violations is exceptionally high in urban areas, and the cost of these violations and the resulting inspections is staggering: Nashville Department of Transportation (NDOT) currently loses an estimated $2 million from the unpaid permit fees alone and pays external contractors an estimated $5 million annually for inspection. Most importantly, road closures (particularly illegal closures and violations) harm traffic, commuter safety, and local businesses. This problem is not restricted to Nashville alone; urban areas across the USA face challenges with enforcing and monitoring right-of-way closures. This project is a collaboration between Vanderbilt University, NDOT, the Metropolitan Government of Nashville, and Nashville Metropolitan Information Technology Services (ITS) to tackle these challenges through fundamental and civic-engaged research. Specifically, this project will design and develop data-driven artificial intelligence models that will 1) automate the detection of permit violations and 2) estimate the effects of right-of-way closures to optimize future permit issuance.

Our goal of automating the detection of right-of-way closures and estimating the impact of future closures requires fundamental advances in data science, machine learning, and software engineering. Specifically, the intellectual merit of our project lies in 1) the design and implementation of novel neural network architectures to automatically detect road closures; 2) the design and implementation of a data-driven approach to infer locations of road closures from heterogeneous and noisy crowdsourced data; 3) the development of an urban digital twin to estimate the effect of road closures in Nashville; 4) the development of an optimization engine for the issuance of right-of-way permits; and 5) system and data integration to deploy the proposed technology in Nashville. The resulting technical framework will be available for other urban areas, and it will apply to problems beyond right-of-way monitoring, e.g., emergency response and traffic management, and generally to the broader problem of monitoring societal-scale cyber-physical systems.

This project is in response to the Civic Innovation Challenge program?s Track B. Bridging the gap between essential resources and services & community needs and is a collaboration between NSF, the Department of Homeland Security, and the Department of Energy.
SCC-CIVIC-PG Track A: An AI-Assisted Digital Twin Platform for Advanced Energy Auditing and Retrofitting Analysis in Low-Income Homes at Multiple Scales
With escalating extreme weather events, aging homes especially those in lower income groups and rural regions face severe energy burdens. Advancements in the field of building energy auditing have been propelled by the integration of sophisticated technologies, such as Unmanned Aerial Vehicles (UAVs), Infrared Thermography (IRT), Computer Vision (CV), Artificial Intelligence (AI), and data-driven Building Energy Modeling (BEM). However, barriers remain in the integrative and effective management and analysis of the data collected by these innovative technologies in real-world practices, particularly in those underserved communities that often lack access to expert technicians, financial resources, and essential technological infrastructure. This project studies the community-engaged, participatory design of an innovative Digital Twin (DT) platform to synthesize advanced aerial imaging, AI analytics, energy modeling, and expert knowledge to pinpoint exactly where and how homes can be improved for energy efficiency. By improving accessibility and efficiency in energy auditing, the project seeks to enhance building sustainability and resiliency in underserved communities. Partnering with civic agencies, technical experts, and local communities in the southeastern United States, this research will explore and overcome barriers to adopting these technologies, contributing to better living conditions and environmental sustainability.

This project aims to engage civic agencies and technical experts to participatory design and develop an AI-assisted DT platform that seamlessly integrates cutting-edge auditing technologies, providing effortless access to multi-scale analytics for the efficient auditing and retrofitting decision-making. To achieve this, the planning project seeks to build partnerships within the U.S. southeastern low-income communities, to study three major questions: 1) What are the critical needs and challenges of weatherization for underserved communities. 2) How to overcome the barriers and enhance the adoption of advanced auditing technologies in underserved communities without overwhelming local resources? 3) How can stakeholders conduct appropriate analytics to support their needs in energy performance evaluation and retrofitting decision making? The objectives of the research are: 1) identifying challenges and needs for weatherization in low-income houses, 2) designing DT platform to support advanced technology implementation, and 3) empowering the DT with advanced AI to assist with multi-scale retrofit decision-making. By facilitating the DT platform with user-friendly access to functional tools, alongside exploring the socio-technical dynamics of their implementation, this research seeks to develop actionable strategies that ensure equitable improvements in building energy efficiency and resiliency across all communities.
FDT-BioTech:CARDIT: Cardiac Digital Twins; but better!
Sudden cardiac death (SCD) remains a leading cause of mortality worldwide, with an incidence ranging from 50 to 100 per 100,000 people in the general population of Europe and North America. SCD accounts for 15-20% of all deaths. Despite advancements in medical science, accurately assessing the individualized risk of SCD remains a significant unmet clinical need. Digital twins of hearts offer a powerful tool for personalized medical care, enabling data-informed decision-making under uncertainty, that is crucial for early identification and prevention of heart conditions. However, even the most advanced state-of-the-art digital twin models contain systematic errors due to our incomplete understanding of human biology, the limited availability and quality of patient data, and the inherent uncertainties present in the models. The primary goal of this research project is not to build the perfect cardiac digital twin, but rather to make the best currently achievable digital twin of a patient?s heart better by reliably transforming it to the truth (data). Beyond the scientific findings, this research will be incorporated into educational and outreach programs designed to attract underrepresented groups to engineering and enhance undergraduate and graduate education in biomedical engineering and applied mathematics.

The project employs novel, scalable, and data-driven mathematical/algorithmic calibration tools, that combine mathematics, scientific computation, and machine learning to account for the biological variability of individual patients. The approach will not involve adding more physics or biology to the cardiac digital twin model, or increasing the resolution of the digital twins? spatial discretization; what is sought, rather, is identifying and mathematically correcting any deviations (model errors) in the current cardiac digital twin using available data and numerical analysis-inspired machine learning algorithms. These tools will reduce the simulation time required to inform clinical intervention (ablation) in patients with cardiomyopathy, and will hopefully broadly enhance the predictive capabilities and utility of digital twins across various application domains beyond our cardiac-focused effort. The outcomes of this study are expected to fundamentally advance the use of cardiac digital twins in clinical practice and lay a foundation for studying the risk of sudden death for patients with ischemic and nonischemic cardiomyopathy.

ENG-ADVWIRE:CCSS: RF Interference Mitigation in High-Density Heterogeneous Semiconductor Device Packaging Through Digital Twin Emulations
Future electronics manufacturing will not only encompass devices like chips and chiplets but also integrate passive components, sensors, and Radio Frequency (RF) front ends with power amplifiers into high-density heterogeneous packages. Currently, package components are tested individually and reassembled on high-density interposer substrates. However, future trends indicate that the interconnect pitch between devices will scale down to less than a micron. This shift towards high-density integration necessitates more sophisticated methods to mitigate electromagnetic interference (EMI) and ensure electromagnetic compatibility (EMC) within the overall package. With this in mind, this research focuses on thin film magnetodielectric materials and electronic package topologies, coupled with Multiphysics digital twin models, aimed at suppressing: 1) jittering and degradation of bit error rates, 2) switching noise in digital integrated circuits due to emissions or crosstalk, 3) close proximity coupling and radiation due to high-speed digital or analog interconnects, and 4) transmission line crosstalk due to conduction or ground bounces of electromagnetic fields. The primary goal of this project is to develop new techniques for predicting and suppressing signal interference in future high-density electronic packages. Concurrently, the project will develop a comprehensive multiphysics toolset to integrate circuit extraction methods and materials for EMI suppression with popular chip and electronic/RF circuit toolsets seamlessly. This research is expected to impact the rapidly growing U.S. semiconductor industry and enable the packaging of reliable, vertically integrated electronics. Additionally, it will substantially enhance the training of a diverse group of students by leveraging Florida International University's unique position as the only Majority-Minority Carnegie R1 Research University in the continental U.S. This research will further advance educational efforts to broaden participation of women and other underrepresented groups in STEM through curriculum development and REU programs.

The aim of this research is to pioneer disruptive EMI/EMC mitigation techniques in high-density heterogeneous semiconductor device packages through digital twin emulations. The goal is to advance knowledge and enable optimal embedded packaging with heterogeneous active and passive components to ensure future electronic package reliability and performance. To address these challenges, the project will investigate: a) new EMI/EMC ferromagnetic composite materials, b) novel packaging topologies, and c) advanced computational and circuits extraction methods. A number of innovations are expected: 1) multilayered films formed of cobalt nickel-iron alloy (CoNiFe) and copper (Cu) layers, but still only 5 micrometers thick, to achieve maximum shielding with minimal film thickness. As much as 30 dB more shielding can be attained using these composite films; 2) New shielding topologies for reliable interference suppression using prefabricated forms to enable rapid electronic package formations. These are aimed at suppressing high-power device interference and high-speed circuit radiation at the chip interconnects (as much as 40 dB or more); 3) New class of multiphysics toolsets that cut across electromagnetic, thermomechanical and thermal designs to provide system performance and reliability. Such multiphysics models are of critical importance as interactions among various physics domains imply design trade-offs needing quantification; 4) AI multiphysics models that combine model-agnostic meta-learning and physics-informed learning approaches using measured data as well as analytical and semi-analytical models for accurate and robust modeling; 5) Novel S-parameter extraction methods that uniquely account for coupling and external fields using new port excitations within the circuit network of the multiphysics model.
Collaborative Research: FDT-BioTech:physics-informed and machine learning-accelerated digital twin simulations for cardiovascular medical device evaluation
A digital twin is a virtual model that mirrors and updates in real-time based on data from its physical counterpart. In biomedical and healthcare fields, digital twins, representing virtual models of patients, medical devices, and more, can open up new avenues for developing and evaluating innovative biomedical technologies, particularly enabling virtual clinical trials for evaluating cardiovascular medical devices and advancing regulatory sciences. However, current digital twin technologies lack sufficient computational fidelity and efficiency to effectively support these biomedical and healthcare applications. To resolve these challenges, this project aims to develop advanced computational methods for creating high-fidelity, fast-running digital twins of patient hearts and cardiovascular medical devices. Additionally, the methods will be made publicly available through a software/cyberinfrastructure platform. This will facilitate virtual clinical trials that can evaluate the efficacy and safety of medical devices, as well as improve device designs before initiating real clinical trials in a safe, cost-effective, and precisely controlled manner. In addition to advancing digital twin technologies, the project?s cyberinfrastructure will serve as an educational resource for students, researchers, and industrial engineers to enhance their understanding of advanced digital twin techniques for medical device evaluation.

This project will develop novel machine learning (ML)-based image analysis algorithms and physics solvers for performing near-realtime virtual clinical trials with high-fidelity digital twins of patient hearts and cardiovascular medical devices. Patient-specific geometries and tissue mechanical properties will be incorporated into the digital twin construction for near-realtime physics simulations. Consequently, virtual clinical trials can be performed at significantly reduced time and financial costs. This project will deliver (1) novel ML algorithms for accurate digital twin geometry reconstruction from 3D+t medical images, enabling point-to-point mesh correspondence for high-fidelity dynamic motion tracking; (2) a robust and computationally efficient inverse method to identify in vivo material properties from medical images, which is essential for creating material-realistic digital twins; (3) a new ML-based fluid-structure interaction (ML-FSI) solver for biomechanics and hemodynamic analyses, thereby enabling dynamic digital twin simulations throughout a cardiac cycle. While the primary focus will be on digital twins of the left heart and aorta, the computational methods can be generally applied to create digital twins of the entire heart. The computational methods will be demonstrated through concrete examples involving Transcatheter Aortic Valve Replacement (TAVR) and Thoracic Endovascular Aortic Repair (TEVAR) devices. The algorithms and methods developed in this project will be generic and readily applicable to devices for treating various cardiovascular diseases.

Collaborative Research: FDT-BioTech: First-Principles Informed Data-Enabled Predictive Digital Twin of Human Physiology
Digital twins mimic actions and processes of physical assets in real-life executions. This research project concerns with development of a framework for learning digital twins of physical systems capable of incorporating real-world data into first-principles based mathematical representations. Learning digital twin from real data is a novel capability which can help enable effective strategic planning in various domains such as space exploration, autonomous transportation, sustainable water future, smart manufacturing, critical mineral mining, alternative power generation, and healthcare to name a few. This project focuses on applications to important problems in healthcare sciences related to data-informed decision-making exploiting virtual representations of human physiology and has implications for the development and evaluation of new therapies and treatments. One compelling example application is glucose metabolism in people with Type 1 diabetes (T1D). Patients with T1D must replace insulin exogenously as determined by multiple daily measurements of the blood glucose concentration, to maintain glucose homeostasis and avoid hypo / hyper-glycemia and life-threatening diabetic ketoacidosis. As a result, the person with diabetes has to make multiple, complex decisions each day based on food composition, exercise, hormonal cycles and other behavioral factors. Personalized glucose metabolism digital twins developed through this award will be used to devise new ethical treatment modalities and evaluate safety and effectiveness of automated insulin delivery systems without risk factors. Digital twins as such can also feed essential knowledge about system safety and effectiveness to regulatory agencies through assurance cases and advance regulatory science in profound ways. Both study sites University of Houston and Arizona State University are Hispanic serving institutions and the research is integrated with educational and outreach activities to create awareness, especially among youths, and understanding of diabetes and its management, broaden participation of groups traditionally underrepresented in STEM and contribute positively to engineering education.

The first-principles informed data-enabled framework seeks to advance foundational techniques underpinning the development and use of digital twins and synthetic data in biomedical and healthcare domains, by combining advances across mathematical modeling, machine learning (ML) and systems? theory with human physiology. This research will (1) develop advanced structures based on neural networks (NNs) for the recovery of an underlying physics-based model that are capable of operating in real-world conditions characterized by limited data availability, low and non-uniform sample rate and spatial and temporal noise, (2) develop novel parametrizations of black-box dynamics using NNs from a class of models with "built-in" properties of stability and robustness to perturbations, (3) integrate real-world physical twin generated data which are heterogeneous, scarce and noisy, into its virtual first-principles based mathematical representation, (4) develop a novel framework for learning unmodeled dynamics due to e.g., unaccounted for inputs, inter- and intra-individual variability. Extensive evaluation of this methodology will be conducted using publicly available datasets specially for T1D patients.

FDT-BioTech: A digital twin framework for developing and analyzing virtual patient cohorts to enable virtual clinical trials
The current clinical trial system is notoriously inefficient and resource intensive with only 10% of drugs that enter the system eventually being approved for patient use. This is due, at least partially, to how difficult it is to test all the possible interventions (for example, drugs or surgery) in large groups of patients to determine that they are safe and effective. Furthermore, it is not possible to vary the order, dose, and timing of any promising intervention as there are simply too many combinations and not enough resources (or patients) available to test all the options. Thus, the clinical trial system currently attempts to determining the utility of one intervention delivered in a very limited number of schedules. This project will use mathematical models built on the key characteristics of cancer to build virtual patient populations upon which a large range of interventions can be ?tested? via simulation. The results of this project could be used to, for example, select the intervention strategy with the highest likelihood of significantly reducing patient mortality, enriching the patient population to those most likely to benefit from the intervention, or eliminate interventions that are unlikely to achieve FDA approval. Success in this project will result in a fundamental shift in the way clinical trials are currently designed and executed.

There are three main technical objectives for this project. The first one is to the improve computational efficacy of organ-scale, biology-based mathematical models to enable high throughput screening of novel interventional strategies. Through the use of surrogate models, we will ensure that the timescales of computing the effect of a set of interventions on a virtual patient population are within acceptable timescales?all while maintaining the interpretability of the results. The second one is to develop rigorous mathematical techniques to generate a cohort of stochastic virtual patients with unique patient anatomies and physiologies with uncertainty (i.e., distributions of model parameters to capture inter- and intra-patient heterogeneity) by combining parameter distributions obtained from model calibration to the historical patient data. We will then validate this approach by reproducing the results of historical clinical trials. The third and final technical objective is to perform a virtual clinical trial that systematically tests an array of practical therapeutic interventions that vary the dose and timing of standard-of-care chemotherapies on a virtual breast cancer patient population to determine the safety and efficacy of novel therapeutic interventions. This will provide the method to perform computationally efficient virtual clinical trials at scale. By completing these technical objectives, we will provide the community with a methodology to dramatically improve the efficiency of in-person clinical trials or, even, eliminate them entirely by evaluating in silico a large range of interventionsin parallel on representative virtual patient cohorts of the target disease.

Math-DT: Advancing the Mathematical Foundations for Dynamic Digital Twinning of Next-Generation Mobile Wireless Networks
Digital Twins?virtual models of physical systems?have garnered growing attention in recent years, driven by rapid advances in sensing, communications, computing, machine learning and artificial intelligence, and hold the potential to vastly accelerate scientific discovery and revolutionize many industries. In particular, Digital Twins play a fundamental role in designing, managing and optimizing 5G wireless networks and will be essential in enabling next-generation 6G wireless networks. Despite recent advances in realistic channel modeling, there are still many fundamental challenges in developing fit-for-purpose Digital Twins for next generation wireless networks that can seamlessly integrate data for informed decision making, and can be dynamically updated as the physical environment varies and network operational objectives change. Motivated by these challenges, the team of investigators develops novel mathematical theories, and new data-driven, AI-guided models and algorithms that will lay the mathematical foundations for digital twinning of next generation wireless networks. The award also supports undergraduate and graduate students from underrepresented groups in research and educational activities as well as organization of K-12 outreach programs.

This proposal aims to advance the mathematical foundations of next generation wireless network Digital Twins. The investigators will place the ray tracing problem?essential to such digital twins?in the more general framework of first order Hamilton-Jacobi equations, and will make theoretical and algorithmic advances in data-driven learning of Hamilton-Jacobi equations. The research team will prove optimal sample size complexity bounds for learning Hamilton-Jacobi equations and their solutions from data, and develop algorithms for achieving these bounds, in both the static and active learning settings. They will develop a temporal surface reconstruction algorithm that combines temporal LiDAR and video camera information by leveraging neural kernels and transport equations. In order to quantify the uncertainty in their results, the investigators will establish posterior contraction rates for learning Hamilton-Jacobi equations, and develop methods to construct and analyze Bayesian credible sets and perform scalable posterior sampling. Finally, the investigators will integrate their theoretical and algorithmic advances into a next generation wireless network Digital Twin platform that will be evaluated in both controlled and dynamic real-world environments.
Collaborative Research: FDT-BioTech: Advancing Mathematical and Statistical Foundations to Enhance Human Digital Twin of Neurophysiological Modeling and Uncertainty Quantification

This project aims to develop the mathematical foundations for a digital twin (DT) system for individuals with autism spectrum disorder (ASD), focusing on dynamic modeling, prediction, uncertainty quantification, and treatment or intervention recommendation through DT-based optimization. ASD is characterized by challenges in social interaction, communication, and behavior, such as difficulties in forming relationships, understanding nonverbal cues, speech development, repetitive behaviors, and sensory sensitivities. The project will create a unified system integrating clinical and neuro-developmental data, analyzed using a DT healthcare paradigm. The DT technology will enable individualized models, and its predictive capabilities will allow healthcare providers to anticipate progression and adjust treatment or intervention proactively. Additionally, the continuous feedback loop from real-time data will enhance therapeutic outcomes. The developed methods and theories will have broader applicability to other medical areas, improving healthcare efficiency, reducing system burdens, and informing public health strategies. This will ultimately enhance care and promote community well-being. The project will also develop quality cyberinfrastructure to share algorithms, data, and open-source software with the community. Furthermore, the investigators plan to expand scientific impacts through collaborating with medical experts and industry scientists, training undergraduate and graduate students, and integrating research findings into course development.

The project will develop a DT framework by modeling brain activities with a unified data structure, linked to behavioral characteristics and interventions aligned with individuals' neuro-developmental processes. This system will integrate multimodal and multi-source data related to human health and development. It will establish foundational models for training and generating synthetic data from DT models, enabling personalized predictions of progression and uncertainty quantification through novel interdisciplinary approaches. The DT system consists of four research modules: (1) Develop computational models based on conditional variational auto-encoders (CVAE) and longitudinal CVAE to analyze brain activities, integrate diverse imaging data, and model neurodevelopmental processes. (2) Create a novel bilevel formulation for multi-distribution fine-tuning techniques on pretrained foundational models and a fast algorithm to learn from heterogeneous data sources to predict ASD outcomes. (3) Develop a model-free conformal prediction procedure to ensemble predictions from multiple models obtained with different modalities and progression simulations, integrating various types of uncertainties into one framework. (4) Develop a DT-based reinforcement learning framework to recommend personalized treatment/intervention plans that significantly improve online learning efficiency and clinical outcomes. The project will address challenges such as multimodality and multi-source data, high-dimensional features, dynamic progression of ASD symptoms, brain functional connectivity, and the need for personalized intervention or treatment recommendations and uncertainty quantification.

MATH-DT: Gradient-enhanced Deep Gaussian Processes for Optimization of Diffusive High-Speed Unsteady Mixers
Rotating detonation combustors (RDCs) coupled to highly diffusive mixers enable compact, green, and efficient energy production. RDCs operate through the injection of an air-fuel mixture which is detonated through a reactive shock wave rotating at supersonic speeds and fed through the mixer to cool and slow the flow before it reaches a turbine which ultimately harnesses the energy. RDC-mixers hold great promise to revolutionize power and propulsion systems, but they are difficult to model/optimize due to unsteady mixing, extreme temperatures, and high-speed diffusion. This collaborative project aims to develop models and methodologies that enable optimization of the RDC-mixer for maximal fuel efficiency. The investigators will leverage a three-pronged meta-modeling framework featuring an innovative digital twin, a novel statistical surrogate model, and a physical experiment involving a high speed wind tunnel in which the mixer will be assessed through high-frequency optical and probe-based measurement techniques. RDC-mixer-turbine systems are directly impactful to clean energy and heat production, but their potential impact is even broader. Diffusing elements and mixers are used in a variety of applications, ranging from aviation, aerospace, agriculture, refrigeration cycles and heat exchangers. The mathematical modeling foundations developed in this project will be widely applicable to computer simulation experiments and digital twins.

This project is organized into three aims. First, motivated by the complexities of the digital twin, a gradient-enhanced Bayesian deep Gaussian process surrogate will be developed to provide non-stationary flexibility, uncertainty quantification, gradient-enhancement for improved accuracy, and gradient predictions to facilitate Bayesian optimization. Second, the digital twin of the RDC-mixer will be developed at reduced computational costs as existing simulations of RDC-mixers require weeks of compute time. Tailored unsteady boundary conditions are proposed to separate the computational fluid dynamic simulations for the combustor and mixer, which will enable faster computation. The digital twin will incorporate steady and unsteady flows, meshing, and adjoint solvers to provide gradient information at minimal cost. Third, a novel calibrated Bayesian optimization framework will be developed to first optimize calibration parameters of the digital twin, then use these with a bias-correction model to sequentially optimize the physical experiment. The physical model will be used in the calibration feedback loop to train the bias-correction model and to test and validate the best designs. Collectively, the surrogate model, digital twin, and physical experiment will enable effective optimization of the RDC-mixer design for optimal fuel efficiency.
Collaborative Research: MATH-DT Closing the generalization gap of digital twins
From the weather to human health to fighter jets, there are many complex systems whose outcomes we would like to predict and control. To achieve these goals, scientists and engineers often build digital twins---computer models that emulate and interact with the underlying physical systems. The current project describes fundamental research into the generalization ability of digital twins: to what degree can digital twins predict outcomes under conditions they have not previously encountered? For example, if the digital twin for an airplane has only seen data collected under normal operating conditions, can it accurately predict the plane's response to turbulence? By combining mathematical tools from nonlinear dynamics and computational tools from machine learning, this project aims to develop fundamental theories on generalization and build robust digital twins that can perform well in extreme or unexpected conditions. While the proposed framework applies to a broad class of complex systems, it is first being applied to circadian rhythms, which are the internal timekeeping mechanisms of the human body. Human biological clocks are increasingly subject to disturbances introduced by modern lifestyles such as long-haul air travel and nighttime computer use. Predictive digital twins can give personalized recommendations on effective interventions, such as optimal strategies to speed up recovery from jet lags. The project will also provide opportunities to teach modern mathematical concepts to a diverse population of undergraduate and graduate students. Through this project, students learn valuable skills in mathematical modeling, data analysis, science communication, and gain first-hand experience in building and managing state-of-the-art machine learning pipelines.

Current domain-agnostic digital twins based on deep neural networks are very expressive but can struggle when generalizing beyond their training conditions. Physics-based digital twins, on the other hand, generalize better to unseen conditions thanks to the strong inductive bias built into the model. On the other hand, they are often not sufficiently flexible to fully capture the rich dynamics in data. This project develops a new class of hybrid digital twins with tunable physics-based and domain-agnostic components, allowing practitioners to balance expressivity versus generalization, depending on the available data and the nature of the task. Utilizing concepts such as basins of attraction in multistable dynamical systems, a key objective of the project is to quantify how the generalization ability of the digital twin changes as the weights assigned to the two components are adjusted. In particular, the project explores the possibility that a properly weighted domain-agnostic component in the hybrid digital twin can sometimes improve out-of-distribution generalization, especially when the inductive bias provided by the physics-based component is imperfect. Digital twins that generalize to unseen conditions are crucial to applications such as finding optimal interventions for restoring disrupted circadian rhythms. For example, to find optimal strategies to speed up recovery from jet lags, a digital twin needs to predict the dynamics of a severely perturbed circadian clock based on data gathered mostly from normally operating clocks. These investigations will guide the creation of more robust digital twins and help inform critical decisions under new or uncertain conditions.








